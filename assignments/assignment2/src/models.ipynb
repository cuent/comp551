{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nr_pRkcIco0T"
   },
   "outputs": [],
   "source": [
    "from data_loader import load_data\n",
    "from naive_bayes import BernoulliNaiveBayes\n",
    "from nlp_processing import LemmaCountVectorizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegression \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from scipy.stats import randint, norm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from nbsvm import NBSVM\n",
    "\n",
    "pd.options.display.max_columns = None # show all columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 320,
     "status": "ok",
     "timestamp": 1549494432579,
     "user": {
      "displayName": "FRANCISCO XAVIER SUMBA TORAL",
      "photoUrl": "",
      "userId": "11128736706802637809"
     },
     "user_tz": 300
    },
    "id": "rKbl5m-gLme3",
    "outputId": "fa417ed1-ac49-41c6-9c1c-801dae469916"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Train(X-(25000,), y-(25000,)), Test(X-(25000,))'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "train, test = load_data()\n",
    "# full_text = list(train.iloc[:, 1].values) + list(test.iloc[:, 1].values)\n",
    "# raw training and test data\n",
    "X_train = train.iloc[:,1].values\n",
    "X_test = test.iloc[:,1].values\n",
    "y = train.iloc[:,2].values.astype(int)\n",
    "\n",
    "\"Train(X-%s, y-%s), Test(X-%s)\"%(X_train.shape, y.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "Are most ocurrent words important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zzzz:73286',\n",
       " 'zzzzz:75997',\n",
       " 'zzzzzzzz:93976',\n",
       " 'zzzzzzzzzzzz:96467',\n",
       " 'zzzzzzzzzzzzz:101871',\n",
       " 'zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz:107331',\n",
       " 'zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz:135724',\n",
       " 'zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz:145864',\n",
       " 'æsthetic:164141',\n",
       " 'østbye:336748']"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = LemmaCountVectorizer(strip_accents='unicode', stop_words=None, \n",
    "                                stem=True, ngram_range=(1, 1))\n",
    "data = vect.fit_transform(X_train)\n",
    "counts = np.ravel(data.sum(axis=0))\n",
    "idx_sort = np.argsort(counts)\n",
    "words = vect.get_feature_names()\n",
    "top = 10\n",
    "[\"{}:{}\".format(w,c) for c,w in zip(counts[idx_sort][-top:], words[-top:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"['001']:[3]\",\n",
       " \"['10pm']:[3]\",\n",
       " \"['10x']:[3]\",\n",
       " \"['123']:[3]\",\n",
       " \"['125']:[3]\",\n",
       " \"['128']:[3]\",\n",
       " \"['12a']:[3]\",\n",
       " \"['131']:[3]\",\n",
       " \"['134']:[3]\",\n",
       " \"['136']:[3]\",\n",
       " \"['147']:[3]\",\n",
       " \"['1500']:[3]\",\n",
       " \"['15s']:[3]\",\n",
       " \"['1780s']:[3]\",\n",
       " \"['1798']:[3]\",\n",
       " \"['1820']:[3]\",\n",
       " \"['1824']:[3]\",\n",
       " \"['183']:[3]\",\n",
       " \"['1836']:[3]\",\n",
       " \"['1837']:[3]\",\n",
       " \"['1839']:[3]\",\n",
       " \"['1846']:[3]\",\n",
       " \"['1847']:[3]\",\n",
       " \"['1850']:[3]\",\n",
       " \"['1850s']:[3]\",\n",
       " \"['1853']:[3]\",\n",
       " \"['1863']:[3]\",\n",
       " \"['1865']:[3]\",\n",
       " \"['1881']:[3]\",\n",
       " \"['1888']:[3]\",\n",
       " \"['1892']:[3]\",\n",
       " \"['18a']:[3]\",\n",
       " \"['1903']:[3]\",\n",
       " \"['1911']:[3]\",\n",
       " \"['1h']:[3]\",\n",
       " \"['2012']:[3]\",\n",
       " \"['2200']:[3]\",\n",
       " \"['232']:[3]\",\n",
       " \"['24th']:[3]\",\n",
       " \"['2600']:[3]\",\n",
       " \"['26th']:[3]\",\n",
       " \"['270']:[3]\",\n",
       " \"['275']:[3]\",\n",
       " \"['27th']:[3]\",\n",
       " \"['28th']:[3]\",\n",
       " \"['2hrs']:[3]\",\n",
       " \"['30min']:[3]\",\n",
       " \"['30pm']:[3]\",\n",
       " \"['38th']:[3]\",\n",
       " \"['3p']:[3]\",\n",
       " \"['3rds']:[3]\",\n",
       " \"['451']:[3]\",\n",
       " \"['47s']:[3]\",\n",
       " \"['49th']:[3]\",\n",
       " \"['571']:[3]\",\n",
       " \"['6000']:[3]\",\n",
       " \"['607']:[3]\",\n",
       " \"['70mm']:[3]\",\n",
       " \"['70th']:[3]\",\n",
       " \"['90mins']:[3]\",\n",
       " \"['99p']:[3]\",\n",
       " \"['9mm']:[3]\",\n",
       " \"['____']:[3]\",\n",
       " \"['_atlantis_']:[3]\",\n",
       " \"['_is_']:[3]\",\n",
       " \"['aage']:[3]\",\n",
       " \"['abducting']:[3]\",\n",
       " \"['abdul']:[3]\",\n",
       " \"['abercrombie']:[3]\",\n",
       " \"['aberration']:[3]\",\n",
       " \"['abide']:[3]\",\n",
       " \"['ablaze']:[3]\",\n",
       " \"['abm']:[3]\",\n",
       " \"['abolition']:[3]\",\n",
       " \"['abominably']:[3]\",\n",
       " \"['abortive']:[3]\",\n",
       " \"['aborts']:[3]\",\n",
       " \"['abruptness']:[3]\",\n",
       " \"['absolve']:[3]\",\n",
       " \"['absolved']:[3]\",\n",
       " \"['abstinence']:[3]\",\n",
       " \"['accelerating']:[3]\",\n",
       " \"['accelerator']:[3]\",\n",
       " \"['accession']:[3]\",\n",
       " \"['accessories']:[3]\",\n",
       " \"['accommodating']:[3]\",\n",
       " \"['accommodation']:[3]\",\n",
       " \"['accommodations']:[3]\",\n",
       " \"['accordian']:[3]\",\n",
       " \"['accorsi']:[3]\",\n",
       " \"['accosts']:[3]\",\n",
       " \"['accountability']:[3]\",\n",
       " \"['accountants']:[3]\",\n",
       " \"['accumulates']:[3]\",\n",
       " \"['accuser']:[3]\",\n",
       " \"['ackerman']:[3]\",\n",
       " \"['acknowledgment']:[3]\",\n",
       " \"['acolyte']:[3]\",\n",
       " \"['acquittal']:[3]\",\n",
       " \"['acumen']:[3]\",\n",
       " \"['adamantium']:[3]\",\n",
       " \"['adaptable']:[3]\",\n",
       " \"['adaptions']:[3]\",\n",
       " \"['addams']:[3]\",\n",
       " \"['addicting']:[3]\",\n",
       " \"['adhd']:[3]\",\n",
       " \"['adherents']:[3]\",\n",
       " \"['adhering']:[3]\",\n",
       " \"['adios']:[3]\",\n",
       " \"['adjusts']:[3]\",\n",
       " \"['admonishing']:[3]\",\n",
       " \"['adobe']:[3]\",\n",
       " \"['adorably']:[3]\",\n",
       " \"['adoree']:[3]\",\n",
       " \"['adriano']:[3]\",\n",
       " \"['adroitly']:[3]\",\n",
       " \"['advices']:[3]\",\n",
       " \"['aerobicide']:[3]\",\n",
       " \"['aerosol']:[3]\",\n",
       " \"['aetheist']:[3]\",\n",
       " \"['affective']:[3]\",\n",
       " \"['afghans']:[3]\",\n",
       " \"['afield']:[3]\",\n",
       " \"['afrika']:[3]\",\n",
       " \"['agey']:[3]\",\n",
       " \"['aggravates']:[3]\",\n",
       " \"['aggressiveness']:[3]\",\n",
       " \"['agonies']:[3]\",\n",
       " \"['agriculture']:[3]\",\n",
       " \"['aguirre']:[3]\",\n",
       " \"['ahhhh']:[3]\",\n",
       " \"['aicha']:[3]\",\n",
       " \"['airball']:[3]\",\n",
       " \"['airbrushed']:[3]\",\n",
       " \"['airless']:[3]\",\n",
       " \"['airman']:[3]\",\n",
       " \"['airmen']:[3]\",\n",
       " \"['airports']:[3]\",\n",
       " \"['airship']:[3]\",\n",
       " \"['airtime']:[3]\",\n",
       " \"['aish']:[3]\",\n",
       " \"['aissa']:[3]\",\n",
       " \"['ajeeb']:[3]\",\n",
       " \"['ajnabi']:[3]\",\n",
       " \"['akosua']:[3]\",\n",
       " \"['akshey']:[3]\",\n",
       " \"['alahani']:[3]\",\n",
       " \"['alanis']:[3]\",\n",
       " \"['albans']:[3]\",\n",
       " \"['albertson']:[3]\",\n",
       " \"['aldolpho']:[3]\",\n",
       " \"['aleisa']:[3]\",\n",
       " \"['alejandra']:[3]\",\n",
       " \"['algerian']:[3]\",\n",
       " \"['algy']:[3]\",\n",
       " \"['aliases']:[3]\",\n",
       " \"['alisha']:[3]\",\n",
       " \"['alittle']:[3]\",\n",
       " \"['alix']:[3]\",\n",
       " \"['aloha']:[3]\",\n",
       " \"['alongwith']:[3]\",\n",
       " \"['alphaville']:[3]\",\n",
       " \"['als']:[3]\",\n",
       " \"['alsanjak']:[3]\",\n",
       " \"['altagracia']:[3]\",\n",
       " \"['alternated']:[3]\",\n",
       " \"['alvarez']:[3]\",\n",
       " \"['alwina']:[3]\",\n",
       " \"['alyce']:[3]\",\n",
       " \"['amassed']:[3]\",\n",
       " \"['amatuer']:[3]\",\n",
       " \"['amazonian']:[3]\",\n",
       " \"['ambience']:[3]\",\n",
       " \"['ambiguously']:[3]\",\n",
       " \"['ambles']:[3]\",\n",
       " \"['ambrose']:[3]\",\n",
       " \"['amenities']:[3]\",\n",
       " \"['americanised']:[3]\",\n",
       " \"['ames']:[3]\",\n",
       " \"['amiss']:[3]\",\n",
       " \"['amontillado']:[3]\",\n",
       " \"['amorphous']:[3]\",\n",
       " \"['amours']:[3]\",\n",
       " \"['amputation']:[3]\",\n",
       " \"['amuck']:[3]\",\n",
       " \"['anaesthetic']:[3]\",\n",
       " \"['analysing']:[3]\",\n",
       " \"['anarene']:[3]\",\n",
       " \"['anathema']:[3]\",\n",
       " \"['anchorwoman']:[3]\",\n",
       " \"['andalou']:[3]\",\n",
       " \"['andoheb']:[3]\",\n",
       " \"['andreeff']:[3]\",\n",
       " \"['anecdotal']:[3]\",\n",
       " \"['aneurysm']:[3]\",\n",
       " \"['angora']:[3]\",\n",
       " \"['angrier']:[3]\",\n",
       " \"['anguishing']:[3]\",\n",
       " \"['anhalt']:[3]\",\n",
       " \"['anjelica']:[3]\",\n",
       " \"['anjos']:[3]\",\n",
       " \"['annamarie']:[3]\",\n",
       " \"['annex']:[3]\",\n",
       " \"['annihilation']:[3]\",\n",
       " \"['anno']:[3]\",\n",
       " \"['annually']:[3]\",\n",
       " \"['antagonism']:[3]\",\n",
       " \"['antarctic']:[3]\",\n",
       " \"['antebellum']:[3]\",\n",
       " \"['antheil']:[3]\",\n",
       " \"['anthems']:[3]\",\n",
       " \"['anthrax']:[3]\",\n",
       " \"['anthropomorphic']:[3]\",\n",
       " \"['antiheroes']:[3]\",\n",
       " \"['antiseptic']:[3]\",\n",
       " \"['antonin']:[3]\",\n",
       " \"['anubis']:[3]\",\n",
       " \"['anvil']:[3]\",\n",
       " \"['anya']:[3]\",\n",
       " \"['anyday']:[3]\",\n",
       " \"['anywho']:[3]\",\n",
       " \"['aod']:[3]\",\n",
       " \"['ap3']:[3]\",\n",
       " \"['apiece']:[3]\",\n",
       " \"['apocryphal']:[3]\",\n",
       " \"['apologetic']:[3]\",\n",
       " \"['apotheosis']:[3]\",\n",
       " \"['app']:[3]\",\n",
       " \"['apparantly']:[3]\",\n",
       " \"['apparatchik']:[3]\",\n",
       " \"['appearence']:[3]\",\n",
       " \"['appendages']:[3]\",\n",
       " \"['appetizer']:[3]\",\n",
       " \"['appetizers']:[3]\",\n",
       " \"['applications']:[3]\",\n",
       " \"['appraisal']:[3]\",\n",
       " \"['approachable']:[3]\",\n",
       " \"['appropriation']:[3]\",\n",
       " \"['approving']:[3]\",\n",
       " \"['approximates']:[3]\",\n",
       " \"['aquafresh']:[3]\",\n",
       " \"['aquarius']:[3]\",\n",
       " \"['aramaic']:[3]\",\n",
       " \"['aranoa']:[3]\",\n",
       " \"['arbaaz']:[3]\",\n",
       " \"['arby']:[3]\",\n",
       " \"['archangel']:[3]\",\n",
       " \"['arched']:[3]\",\n",
       " \"['archenemy']:[3]\",\n",
       " \"['architects']:[3]\",\n",
       " \"['archived']:[3]\",\n",
       " \"['ardour']:[3]\",\n",
       " \"['arent']:[3]\",\n",
       " \"['aribert']:[3]\",\n",
       " \"['aristide']:[3]\",\n",
       " \"['armada']:[3]\",\n",
       " \"['armando']:[3]\",\n",
       " \"['arming']:[3]\",\n",
       " \"['armourae']:[3]\",\n",
       " \"['armoury']:[3]\",\n",
       " \"['arnett']:[3]\",\n",
       " \"['arrondissements']:[3]\",\n",
       " \"['arsenic']:[3]\",\n",
       " \"['arses']:[3]\",\n",
       " \"['artefact']:[3]\",\n",
       " \"['artemesia']:[3]\",\n",
       " \"['arthritis']:[3]\",\n",
       " \"['arthurian']:[3]\",\n",
       " \"['artimisia']:[3]\",\n",
       " \"['artiness']:[3]\",\n",
       " \"['arvanitis']:[3]\",\n",
       " \"['arwen']:[3]\",\n",
       " \"['aryana']:[3]\",\n",
       " \"['aryans']:[3]\",\n",
       " \"['asbestos']:[3]\",\n",
       " \"['ascend']:[3]\",\n",
       " \"['ascendancy']:[3]\",\n",
       " \"['ascends']:[3]\",\n",
       " \"['ascent']:[3]\",\n",
       " \"['ashame']:[3]\",\n",
       " \"['ashkenazi']:[3]\",\n",
       " \"['ashok']:[3]\",\n",
       " \"['ashram']:[3]\",\n",
       " \"['aspirant']:[3]\",\n",
       " \"['assante']:[3]\",\n",
       " \"['assassinations']:[3]\",\n",
       " \"['assembling']:[3]\",\n",
       " \"['assery']:[3]\",\n",
       " \"['assholes']:[3]\",\n",
       " \"['assignation']:[3]\",\n",
       " \"['assignations']:[3]\",\n",
       " \"['assigning']:[3]\",\n",
       " \"['assuredness']:[3]\",\n",
       " \"['assuring']:[3]\",\n",
       " \"['aster']:[3]\",\n",
       " \"['asteroid']:[3]\",\n",
       " \"['astree']:[3]\",\n",
       " \"['astrid']:[3]\",\n",
       " \"['astrological']:[3]\",\n",
       " \"['astronomically']:[3]\",\n",
       " \"['asuka']:[3]\",\n",
       " \"['aswell']:[3]\",\n",
       " \"['atenborough']:[3]\",\n",
       " \"['atheism']:[3]\",\n",
       " \"['athon']:[3]\",\n",
       " \"['atleast']:[3]\",\n",
       " \"['atoll']:[3]\",\n",
       " \"['atoms']:[3]\",\n",
       " \"['attendees']:[3]\",\n",
       " \"['attested']:[3]\",\n",
       " \"['attests']:[3]\",\n",
       " \"['attractiveness']:[3]\",\n",
       " \"['attributable']:[3]\",\n",
       " \"['auburn']:[3]\",\n",
       " \"['auctioned']:[3]\",\n",
       " \"['audley']:[3]\",\n",
       " \"['aug']:[3]\",\n",
       " \"['auggie']:[3]\",\n",
       " \"['augusten']:[3]\",\n",
       " \"['aumont']:[3]\",\n",
       " \"['auscrit']:[3]\",\n",
       " \"['auspices']:[3]\",\n",
       " \"['austerity']:[3]\",\n",
       " \"['autant']:[3]\",\n",
       " \"['authoress']:[3]\",\n",
       " \"['authorial']:[3]\",\n",
       " \"['authoritarianism']:[3]\",\n",
       " \"['authoritatively']:[3]\",\n",
       " \"['authorship']:[3]\",\n",
       " \"['autographs']:[3]\",\n",
       " \"['automated']:[3]\",\n",
       " \"['aux']:[3]\",\n",
       " \"['av']:[3]\",\n",
       " \"['avenet']:[3]\",\n",
       " \"['avenged']:[3]\",\n",
       " \"['averag']:[3]\",\n",
       " \"['averages']:[3]\",\n",
       " \"['averted']:[3]\",\n",
       " \"['aviator']:[3]\",\n",
       " \"['avon']:[3]\",\n",
       " \"['avonlea']:[3]\",\n",
       " \"['awa']:[3]\",\n",
       " \"['axes']:[3]\",\n",
       " \"['axton']:[3]\",\n",
       " \"['ayats']:[3]\",\n",
       " \"['aylmer']:[3]\",\n",
       " \"['ayurvedic']:[3]\",\n",
       " \"['az']:[3]\",\n",
       " \"['azkaban']:[3]\",\n",
       " \"['baaaaad']:[3]\",\n",
       " \"['baaad']:[3]\",\n",
       " \"['babyface']:[3]\",\n",
       " \"['backbiting']:[3]\",\n",
       " \"['backfired']:[3]\",\n",
       " \"['backlighting']:[3]\",\n",
       " \"['backpacking']:[3]\",\n",
       " \"['bade']:[3]\",\n",
       " \"['badguys']:[3]\",\n",
       " \"['bafflement']:[3]\",\n",
       " \"['bafflingly']:[3]\",\n",
       " \"['bagging']:[3]\",\n",
       " \"['baichwal']:[3]\",\n",
       " \"['bails']:[3]\",\n",
       " \"['baise']:[3]\",\n",
       " \"['baited']:[3]\",\n",
       " \"['baldwins']:[3]\",\n",
       " \"['bales']:[3]\",\n",
       " \"['ballets']:[3]\",\n",
       " \"['ballsy']:[3]\",\n",
       " \"['bally']:[3]\",\n",
       " \"['ballyhooed']:[3]\",\n",
       " \"['balm']:[3]\",\n",
       " \"['bandage']:[3]\",\n",
       " \"['banded']:[3]\",\n",
       " \"['bangers']:[3]\",\n",
       " \"['bankers']:[3]\",\n",
       " \"['bankole']:[3]\",\n",
       " \"['bankrolled']:[3]\",\n",
       " \"['banners']:[3]\",\n",
       " \"['bantam']:[3]\",\n",
       " \"['barabar']:[3]\",\n",
       " \"['barack']:[3]\",\n",
       " \"['barbarous']:[3]\",\n",
       " \"['barbour']:[3]\",\n",
       " \"['barfing']:[3]\",\n",
       " \"['barfly']:[3]\",\n",
       " \"['barging']:[3]\",\n",
       " \"['barmaid']:[3]\",\n",
       " \"['barmy']:[3]\",\n",
       " \"['barnum']:[3]\",\n",
       " \"['barometer']:[3]\",\n",
       " \"['barracuda']:[3]\",\n",
       " \"['barraged']:[3]\",\n",
       " \"['barris']:[3]\",\n",
       " \"['barrow']:[3]\",\n",
       " \"['barrows']:[3]\",\n",
       " \"['baryshnikov']:[3]\",\n",
       " \"['bas']:[3]\",\n",
       " \"['baser']:[3]\",\n",
       " \"['bashful']:[3]\",\n",
       " \"['basin']:[3]\",\n",
       " \"['bastedo']:[3]\",\n",
       " \"['bataan']:[3]\",\n",
       " \"['batali']:[3]\",\n",
       " \"['batchelor']:[3]\",\n",
       " \"['bathrooms']:[3]\",\n",
       " \"['battering']:[3]\",\n",
       " \"['battled']:[3]\",\n",
       " \"['baudelaire']:[3]\",\n",
       " \"['bawl']:[3]\",\n",
       " \"['baying']:[3]\",\n",
       " \"['bayldon']:[3]\",\n",
       " \"['bayonne']:[3]\",\n",
       " \"['bayreuth']:[3]\",\n",
       " \"['bazookas']:[3]\",\n",
       " \"['bbfc']:[3]\",\n",
       " \"['bdsm']:[3]\",\n",
       " \"['beached']:[3]\",\n",
       " \"['beamont']:[3]\",\n",
       " \"['beane']:[3]\",\n",
       " \"['beastie']:[3]\",\n",
       " \"['beasties']:[3]\",\n",
       " \"['beater']:[3]\",\n",
       " \"['beaty']:[3]\",\n",
       " \"['beaut']:[3]\",\n",
       " \"['beaux']:[3]\",\n",
       " \"['bechstein']:[3]\",\n",
       " \"['bedeviled']:[3]\",\n",
       " \"['bedfellows']:[3]\",\n",
       " \"['bedridden']:[3]\",\n",
       " \"['beehives']:[3]\",\n",
       " \"['beeps']:[3]\",\n",
       " \"['beesley']:[3]\",\n",
       " \"['befell']:[3]\",\n",
       " \"['befits']:[3]\",\n",
       " \"['beginner']:[3]\",\n",
       " \"['begrudge']:[3]\",\n",
       " \"['beguiles']:[3]\",\n",
       " \"['beheads']:[3]\",\n",
       " \"['beholden']:[3]\",\n",
       " \"['belabor']:[3]\",\n",
       " \"['beleive']:[3]\",\n",
       " \"['belgians']:[3]\",\n",
       " \"['belittled']:[3]\",\n",
       " \"['bellerophon']:[3]\",\n",
       " \"['bellhop']:[3]\",\n",
       " \"['bellucci']:[3]\",\n",
       " \"['belted']:[3]\",\n",
       " \"['bemoans']:[3]\",\n",
       " \"['benches']:[3]\",\n",
       " \"['bended']:[3]\",\n",
       " \"['bengal']:[3]\",\n",
       " \"['benita']:[3]\",\n",
       " \"['berardinelli']:[3]\",\n",
       " \"['beresford']:[3]\",\n",
       " \"['berg']:[3]\",\n",
       " \"['beringer']:[3]\",\n",
       " \"['berle']:[3]\",\n",
       " \"['berlinale']:[3]\",\n",
       " \"['bermuda']:[3]\",\n",
       " \"['bernson']:[3]\",\n",
       " \"['bertinelli']:[3]\",\n",
       " \"['berton']:[3]\",\n",
       " \"['bertram']:[3]\",\n",
       " \"['besser']:[3]\",\n",
       " \"['bested']:[3]\",\n",
       " \"['bestselling']:[3]\",\n",
       " \"['betamax']:[3]\",\n",
       " \"['bett']:[3]\",\n",
       " \"['betterment']:[3]\",\n",
       " \"['beurk']:[3]\",\n",
       " \"['bffs']:[3]\",\n",
       " \"['bharat']:[3]\",\n",
       " \"['bhatti']:[3]\",\n",
       " \"['bhave']:[3]\",\n",
       " \"['bhodi']:[3]\",\n",
       " \"['bhumika']:[3]\",\n",
       " \"['bi1']:[3]\",\n",
       " \"['bide']:[3]\",\n",
       " \"['biel']:[3]\",\n",
       " \"['bigg']:[3]\",\n",
       " \"['bigots']:[3]\",\n",
       " \"['bigtime']:[3]\",\n",
       " \"['bii']:[3]\",\n",
       " \"['biking']:[3]\",\n",
       " \"['bilcock']:[3]\",\n",
       " \"['biographer']:[3]\",\n",
       " \"['bionic']:[3]\",\n",
       " \"['biplane']:[3]\",\n",
       " \"['birdy']:[3]\",\n",
       " \"['biro']:[3]\",\n",
       " \"['birthdays']:[3]\",\n",
       " \"['births']:[3]\",\n",
       " \"['biscuits']:[3]\",\n",
       " \"['bishops']:[3]\",\n",
       " \"['bisset']:[3]\",\n",
       " \"['biswas']:[3]\",\n",
       " \"['bittersweetness']:[3]\",\n",
       " \"['bizzare']:[3]\",\n",
       " \"['blackballed']:[3]\",\n",
       " \"['blackbuster']:[3]\",\n",
       " \"['blackest']:[3]\",\n",
       " \"['blackjack']:[3]\",\n",
       " \"['blacklist']:[3]\",\n",
       " \"['blackly']:[3]\",\n",
       " \"['blacula']:[3]\",\n",
       " \"['bladed']:[3]\",\n",
       " \"['blaisdell']:[3]\",\n",
       " \"['blakely']:[3]\",\n",
       " \"['blameless']:[3]\",\n",
       " \"['blander']:[3]\",\n",
       " \"['blandick']:[3]\",\n",
       " \"['blankets']:[3]\",\n",
       " \"['blankfield']:[3]\",\n",
       " \"['blankly']:[3]\",\n",
       " \"['blasco']:[3]\",\n",
       " \"['blat']:[3]\",\n",
       " \"['bleakest']:[3]\",\n",
       " \"['bleh']:[3]\",\n",
       " \"['blemish']:[3]\",\n",
       " \"['blemishes']:[3]\",\n",
       " \"['blenheim']:[3]\",\n",
       " \"['blighted']:[3]\",\n",
       " \"['blighty']:[3]\",\n",
       " \"['blindfold']:[3]\",\n",
       " \"['blithering']:[3]\",\n",
       " \"['blitzstein']:[3]\",\n",
       " \"['blizzard']:[3]\",\n",
       " \"['bloodbaths']:[3]\",\n",
       " \"['bloodiest']:[3]\",\n",
       " \"['bloodline']:[3]\",\n",
       " \"['bloodsurfing']:[3]\",\n",
       " \"['bloomed']:[3]\",\n",
       " \"['bludgeon']:[3]\",\n",
       " \"['bludgeoned']:[3]\",\n",
       " \"['bluffs']:[3]\",\n",
       " \"['bluish']:[3]\",\n",
       " \"['bluntschli']:[3]\",\n",
       " \"['blurted']:[3]\",\n",
       " \"['bluto']:[3]\",\n",
       " \"['boardwalk']:[3]\",\n",
       " \"['boating']:[3]\",\n",
       " \"['bobbi']:[3]\",\n",
       " \"['bobbie']:[3]\",\n",
       " \"['boca']:[3]\",\n",
       " \"['bochner']:[3]\",\n",
       " \"['bodacious']:[3]\",\n",
       " \"['bodes']:[3]\",\n",
       " \"['bodybuilders']:[3]\",\n",
       " \"['bodyguards']:[3]\",\n",
       " \"['boerner']:[3]\",\n",
       " \"['bogdonavich']:[3]\",\n",
       " \"['bogdonovich']:[3]\",\n",
       " \"['bohemians']:[3]\",\n",
       " \"['bojangles']:[3]\",\n",
       " \"['bol']:[3]\",\n",
       " \"['bombadier']:[3]\",\n",
       " \"['bombadil']:[3]\",\n",
       " \"['bombast']:[3]\",\n",
       " \"['bonaparte']:[3]\",\n",
       " \"['boner']:[3]\",\n",
       " \"['bonesetter']:[3]\",\n",
       " \"['bonfires']:[3]\",\n",
       " \"['bong']:[3]\",\n",
       " \"['bongo']:[3]\",\n",
       " \"['boning']:[3]\",\n",
       " \"['bookcase']:[3]\",\n",
       " \"['booms']:[3]\",\n",
       " \"['boomslang']:[3]\",\n",
       " \"['boozer']:[3]\",\n",
       " \"['borefest']:[3]\",\n",
       " \"['boringness']:[3]\",\n",
       " \"['bosch']:[3]\",\n",
       " \"['bosnians']:[3]\",\n",
       " \"['bossman']:[3]\",\n",
       " \"['boswell']:[3]\",\n",
       " \"['botox']:[3]\",\n",
       " \"['botticelli']:[3]\",\n",
       " \"['bottomed']:[3]\",\n",
       " \"['boultings']:[3]\",\n",
       " \"['bouncer']:[3]\",\n",
       " \"['bounded']:[3]\",\n",
       " \"['bountiful']:[3]\",\n",
       " \"['bowers']:[3]\",\n",
       " \"['bowm']:[3]\",\n",
       " \"['brackets']:[3]\",\n",
       " \"['brackett']:[3]\",\n",
       " \"['brags']:[3]\",\n",
       " \"['braincells']:[3]\",\n",
       " \"['brandner']:[3]\",\n",
       " \"['brannon']:[3]\",\n",
       " \"['brassed']:[3]\",\n",
       " \"['brawn']:[3]\",\n",
       " \"['breakdance']:[3]\",\n",
       " \"['breakingly']:[3]\",\n",
       " \"['breather']:[3]\",\n",
       " \"['bree']:[3]\",\n",
       " \"['breeches']:[3]\",\n",
       " \"['breezes']:[3]\",\n",
       " \"['bremner']:[3]\",\n",
       " \"['brennen']:[3]\",\n",
       " \"['breslin']:[3]\",\n",
       " \"['bresslaw']:[3]\",\n",
       " \"['brewery']:[3]\",\n",
       " \"['briain']:[3]\",\n",
       " \"['bribing']:[3]\",\n",
       " \"['brickman']:[3]\",\n",
       " \"['bridgers']:[3]\",\n",
       " \"['brigands']:[3]\",\n",
       " \"['brimstone']:[3]\",\n",
       " \"['bringsværd']:[3]\",\n",
       " \"['brining']:[3]\",\n",
       " \"['brion']:[3]\",\n",
       " \"['britcoms']:[3]\",\n",
       " \"['britishness']:[3]\",\n",
       " \"['britt']:[3]\",\n",
       " \"['broached']:[3]\",\n",
       " \"['broadest']:[3]\",\n",
       " \"['broadhurst']:[3]\",\n",
       " \"['broads']:[3]\",\n",
       " \"['broadside']:[3]\",\n",
       " \"['broklynese']:[3]\",\n",
       " \"['brommell']:[3]\",\n",
       " \"['bronston']:[3]\",\n",
       " \"['broody']:[3]\",\n",
       " \"['brookmyre']:[3]\",\n",
       " \"['broth']:[3]\",\n",
       " \"['broughton']:[3]\",\n",
       " \"['brownish']:[3]\",\n",
       " \"['brull']:[3]\",\n",
       " \"['brunhilda']:[3]\",\n",
       " \"['brushing']:[3]\",\n",
       " \"['brusque']:[3]\",\n",
       " \"['brussels']:[3]\",\n",
       " \"['brutus']:[3]\",\n",
       " \"['bu']:[3]\",\n",
       " \"['bucke']:[3]\",\n",
       " \"['bucking']:[3]\",\n",
       " \"['buckner']:[3]\",\n",
       " \"['budd']:[3]\",\n",
       " \"['budge']:[3]\",\n",
       " \"['budgeting']:[3]\",\n",
       " \"['buena']:[3]\",\n",
       " \"['buick']:[3]\",\n",
       " \"['bukhanovsky']:[3]\",\n",
       " \"['bulbs']:[3]\",\n",
       " \"['bulge']:[3]\",\n",
       " \"['bulimics']:[3]\",\n",
       " \"['bulldozed']:[3]\",\n",
       " \"['bulletins']:[3]\",\n",
       " \"['bullfighter']:[3]\",\n",
       " \"['bullfighting']:[3]\",\n",
       " \"['bullfrogs']:[3]\",\n",
       " \"['bullion']:[3]\",\n",
       " \"['bumblers']:[3]\",\n",
       " \"['bun']:[3]\",\n",
       " \"['bundles']:[3]\",\n",
       " \"['burglarize']:[3]\",\n",
       " \"['burglars']:[3]\",\n",
       " \"['burma']:[3]\",\n",
       " \"['burnish']:[3]\",\n",
       " \"['burping']:[3]\",\n",
       " \"['bushel']:[3]\",\n",
       " \"['busia']:[3]\",\n",
       " \"['businesslike']:[3]\",\n",
       " \"['busters']:[3]\",\n",
       " \"['butchery']:[3]\",\n",
       " \"['buts']:[3]\",\n",
       " \"['butted']:[3]\",\n",
       " \"['butting']:[3]\",\n",
       " \"['bypassed']:[3]\",\n",
       " \"['byplay']:[3]\",\n",
       " \"['cacophonous']:[3]\",\n",
       " \"['caddy']:[3]\",\n",
       " \"['cads']:[3]\",\n",
       " \"['cajuns']:[3]\",\n",
       " \"['caked']:[3]\",\n",
       " \"['caldwell']:[3]\",\n",
       " \"['calender']:[3]\",\n",
       " \"['callarn']:[3]\",\n",
       " \"['callers']:[3]\",\n",
       " \"['calloused']:[3]\",\n",
       " \"['calloway']:[3]\",\n",
       " \"['calming']:[3]\",\n",
       " \"['calrissian']:[3]\",\n",
       " \"['camara']:[3]\",\n",
       " \"['cambpell']:[3]\",\n",
       " \"['cameroonian']:[3]\",\n",
       " \"['campaigning']:[3]\",\n",
       " \"['campell']:[3]\",\n",
       " \"['campesinos']:[3]\",\n",
       " \"['campos']:[3]\",\n",
       " \"['cams']:[3]\",\n",
       " \"['cancelling']:[3]\",\n",
       " \"['candidly']:[3]\",\n",
       " \"['candies']:[3]\",\n",
       " \"['canfield']:[3]\",\n",
       " \"['canisters']:[3]\",\n",
       " \"['cann']:[3]\",\n",
       " \"['cannell']:[3]\",\n",
       " \"['canning']:[3]\",\n",
       " \"['canoeing']:[3]\",\n",
       " \"['canton']:[3]\",\n",
       " \"['capacities']:[3]\",\n",
       " \"['capitalizing']:[3]\",\n",
       " \"['capo']:[3]\",\n",
       " \"['capomezza']:[3]\",\n",
       " \"['capper']:[3]\",\n",
       " \"['capping']:[3]\",\n",
       " \"['caprio']:[3]\",\n",
       " \"['capulet']:[3]\",\n",
       " \"['caracter']:[3]\",\n",
       " \"['caramel']:[3]\",\n",
       " \"['cardella']:[3]\",\n",
       " \"['cardiac']:[3]\",\n",
       " \"['careens']:[3]\",\n",
       " \"['carer']:[3]\",\n",
       " \"['carfax']:[3]\",\n",
       " \"['carly']:[3]\",\n",
       " \"['carmine']:[3]\",\n",
       " \"['carolingians']:[3]\",\n",
       " \"['carpathia']:[3]\",\n",
       " \"['carper']:[3]\",\n",
       " \"['carpets']:[3]\",\n",
       " \"['carribbean']:[3]\",\n",
       " \"['carridine']:[3]\",\n",
       " \"['carriere']:[3]\",\n",
       " \"['carrigan']:[3]\",\n",
       " \"['carrys']:[3]\",\n",
       " \"['carstone']:[3]\",\n",
       " \"['carthage']:[3]\",\n",
       " \"['cartouche']:[3]\",\n",
       " \"['casamajor']:[3]\",\n",
       " \"['cassanova']:[3]\",\n",
       " \"['cassell']:[3]\",\n",
       " \"['cassevetes']:[3]\",\n",
       " \"['cassi']:[3]\",\n",
       " \"['castaway']:[3]\",\n",
       " \"['castelnuovo']:[3]\",\n",
       " \"['catacombs']:[3]\",\n",
       " \"['catalogs']:[3]\",\n",
       " \"['catalunya']:[3]\",\n",
       " \"['catastrophes']:[3]\",\n",
       " \"['caterer']:[3]\",\n",
       " \"['cathie']:[3]\",\n",
       " \"['cathode']:[3]\",\n",
       " \"['cathrine']:[3]\",\n",
       " \"['causal']:[3]\",\n",
       " \"['caveats']:[3]\",\n",
       " \"['caved']:[3]\",\n",
       " \"['cavegirl']:[3]\",\n",
       " \"['cavernous']:[3]\",\n",
       " \"['caw']:[3]\",\n",
       " \"['cazale']:[3]\",\n",
       " \"['cc']:[3]\",\n",
       " \"['ce']:[3]\",\n",
       " \"['cecily']:[3]\",\n",
       " \"['ceded']:[3]\",\n",
       " \"['cel']:[3]\",\n",
       " \"['cele']:[3]\",\n",
       " \"['celebei']:[3]\",\n",
       " \"['cellular']:[3]\",\n",
       " \"['censure']:[3]\",\n",
       " \"['centeredness']:[3]\",\n",
       " \"['centralized']:[3]\",\n",
       " \"['centrepiece']:[3]\",\n",
       " \"['cerebrally']:[3]\",\n",
       " \"['cfto']:[3]\",\n",
       " \"['cgis']:[3]\",\n",
       " \"['chagos']:[3]\",\n",
       " \"['chakotay']:[3]\",\n",
       " \"['chaliya']:[3]\",\n",
       " \"['chandrasekhar']:[3]\",\n",
       " \"['chanteuse']:[3]\",\n",
       " \"['chapeau']:[3]\",\n",
       " \"['chapelle']:[3]\",\n",
       " \"['charactor']:[3]\",\n",
       " \"['charecters']:[3]\",\n",
       " \"['charing']:[3]\",\n",
       " \"['charlene']:[3]\",\n",
       " \"['charting']:[3]\",\n",
       " \"['chaste']:[3]\",\n",
       " \"['chastise']:[3]\",\n",
       " \"['chastised']:[3]\",\n",
       " \"['cheapjack']:[3]\",\n",
       " \"['cheep']:[3]\",\n",
       " \"['cheerleading']:[3]\",\n",
       " \"['chemotherapy']:[3]\",\n",
       " \"['cherbourg']:[3]\",\n",
       " \"['cherishing']:[3]\",\n",
       " \"['cherokee']:[3]\",\n",
       " \"['cherubic']:[3]\",\n",
       " \"['chessboard']:[3]\",\n",
       " \"['chevalier']:[3]\",\n",
       " \"['chia']:[3]\",\n",
       " \"['chica']:[3]\",\n",
       " \"['chicanery']:[3]\",\n",
       " \"['chided']:[3]\",\n",
       " \"['chides']:[3]\",\n",
       " \"['chiklis']:[3]\",\n",
       " \"['childishness']:[3]\",\n",
       " \"['childless']:[3]\",\n",
       " \"['childs']:[3]\",\n",
       " \"['chinnery']:[3]\",\n",
       " \"['chins']:[3]\",\n",
       " \"['chipmunk']:[3]\",\n",
       " \"['chirin']:[3]\",\n",
       " \"['choirs']:[3]\",\n",
       " \"['choisy']:[3]\",\n",
       " \"['chopin']:[3]\",\n",
       " \"['choreograph']:[3]\",\n",
       " \"['chorines']:[3]\",\n",
       " \"['chowder']:[3]\",\n",
       " \"['chri']:[3]\",\n",
       " \"['chrissie']:[3]\",\n",
       " \"['christa']:[3]\",\n",
       " \"['christened']:[3]\",\n",
       " \"['christmastime']:[3]\",\n",
       " \"['chromosomes']:[3]\",\n",
       " \"['chugs']:[3]\",\n",
       " \"['chun']:[3]\",\n",
       " \"['ciano']:[3]\",\n",
       " \"['ciao']:[3]\",\n",
       " \"['cinder']:[3]\",\n",
       " \"['cinderellas']:[3]\",\n",
       " \"['cinemaphotography']:[3]\",\n",
       " \"['ciphers']:[3]\",\n",
       " \"['circulate']:[3]\",\n",
       " \"['ciro']:[3]\",\n",
       " \"['citadel']:[3]\",\n",
       " \"['cites']:[3]\",\n",
       " \"['citroen']:[3]\",\n",
       " \"['clamoring']:[3]\",\n",
       " \"['clank']:[3]\",\n",
       " \"['clarice']:[3]\",\n",
       " \"['clarification']:[3]\",\n",
       " \"['clarks']:[3]\",\n",
       " \"['clarksberg']:[3]\",\n",
       " \"['classrooms']:[3]\",\n",
       " \"['claustrophobically']:[3]\",\n",
       " \"['clawed']:[3]\",\n",
       " \"['clayface']:[3]\",\n",
       " \"['cleanly']:[3]\",\n",
       " \"['cleansed']:[3]\",\n",
       " \"['clef']:[3]\",\n",
       " \"['clenteen']:[3]\",\n",
       " \"['cleverer']:[3]\",\n",
       " \"['cliffhanging']:[3]\",\n",
       " \"['climates']:[3]\",\n",
       " \"['clings']:[3]\",\n",
       " \"['clipping']:[3]\",\n",
       " \"['cliques']:[3]\",\n",
       " \"['cloned']:[3]\",\n",
       " \"['cloney']:[3]\",\n",
       " \"['clothesline']:[3]\",\n",
       " \"['clouzot']:[3]\",\n",
       " \"['clubhouse']:[3]\",\n",
       " \"['clued']:[3]\",\n",
       " \"['clunkiness']:[3]\",\n",
       " \"['clytemnestra']:[3]\",\n",
       " \"['cn']:[3]\",\n",
       " \"['coasted']:[3]\",\n",
       " \"['coasting']:[3]\",\n",
       " \"['coattails']:[3]\",\n",
       " \"['coaxes']:[3]\",\n",
       " \"['cockeyed']:[3]\",\n",
       " \"['cockiness']:[3]\",\n",
       " \"['cocoa']:[3]\",\n",
       " \"['coconut']:[3]\",\n",
       " \"['coeur']:[3]\",\n",
       " \"['coexistence']:[3]\",\n",
       " \"['coffeehouse']:[3]\",\n",
       " \"['coffers']:[3]\",\n",
       " \"['cogan']:[3]\",\n",
       " \"['cognoscenti']:[3]\",\n",
       " \"['cohan']:[3]\",\n",
       " \"['cohere']:[3]\",\n",
       " \"['coherently']:[3]\",\n",
       " \"['cohesiveness']:[3]\",\n",
       " \"['colder']:[3]\",\n",
       " \"['colette']:[3]\",\n",
       " \"['coliseum']:[3]\",\n",
       " \"['collaborating']:[3]\",\n",
       " \"['collectible']:[3]\",\n",
       " \"['collegiate']:[3]\",\n",
       " \"['colliding']:[3]\",\n",
       " \"['collin']:[3]\",\n",
       " \"['colloquial']:[3]\",\n",
       " \"['collusion']:[3]\",\n",
       " \"['cologne']:[3]\",\n",
       " \"['colonised']:[3]\",\n",
       " \"['colossus']:[3]\",\n",
       " \"['colouring']:[3]\",\n",
       " \"['columnists']:[3]\",\n",
       " \"['combating']:[3]\",\n",
       " \"['comedi']:[3]\",\n",
       " \"['comediennes']:[3]\",\n",
       " \"['comings']:[3]\",\n",
       " \"['commandeer']:[3]\",\n",
       " \"['commanders']:[3]\",\n",
       " \"['commemorated']:[3]\",\n",
       " \"['commissions']:[3]\",\n",
       " \"['committal']:[3]\",\n",
       " \"['communal']:[3]\",\n",
       " \"['communicative']:[3]\",\n",
       " \"['commutes']:[3]\",\n",
       " \"['companeros']:[3]\",\n",
       " \"['comparably']:[3]\",\n",
       " \"['complacency']:[3]\",\n",
       " \"['compleat']:[3]\",\n",
       " \"['completionists']:[3]\",\n",
       " \"['completley']:[3]\",\n",
       " \"['compliant']:[3]\",\n",
       " \"['complying']:[3]\",\n",
       " \"['compositing']:[3]\",\n",
       " \"['compositional']:[3]\",\n",
       " \"['compost']:[3]\",\n",
       " \"['comprehended']:[3]\",\n",
       " \"['compute']:[3]\",\n",
       " \"['conceives']:[3]\",\n",
       " \"['conceptually']:[3]\",\n",
       " \"['concerto']:[3]\",\n",
       " \"['concieved']:[3]\",\n",
       " \"['concocting']:[3]\",\n",
       " \"['concubines']:[3]\",\n",
       " \"['condieff']:[3]\",\n",
       " \"['conditioner']:[3]\",\n",
       " \"['condolences']:[3]\",\n",
       " \"['condoms']:[3]\",\n",
       " \"['condoning']:[3]\",\n",
       " \"['condos']:[3]\",\n",
       " \"['conducive']:[3]\",\n",
       " \"['configured']:[3]\",\n",
       " \"['conforms']:[3]\",\n",
       " \"['confounds']:[3]\",\n",
       " \"['confusions']:[3]\",\n",
       " \"['cong']:[3]\",\n",
       " \"['congorilla']:[3]\",\n",
       " \"['congresswoman']:[3]\",\n",
       " \"['congruent']:[3]\",\n",
       " \"['conjoined']:[3]\",\n",
       " \"['conlin']:[3]\",\n",
       " \"['connotation']:[3]\",\n",
       " \"['conscripted']:[3]\",\n",
       " \"['conscripts']:[3]\",\n",
       " \"['consecutively']:[3]\",\n",
       " \"['conservation']:[3]\",\n",
       " \"['conserved']:[3]\",\n",
       " \"['conspiratorial']:[3]\",\n",
       " \"['constabulary']:[3]\",\n",
       " \"['constellations']:[3]\",\n",
       " \"['constipation']:[3]\",\n",
       " \"['constrains']:[3]\",\n",
       " \"['constructively']:[3]\",\n",
       " \"['consuelo']:[3]\",\n",
       " \"['cont']:[3]\",\n",
       " \"['containment']:[3]\",\n",
       " \"['contaminating']:[3]\",\n",
       " \"['contemptuous']:[3]\",\n",
       " \"['contends']:[3]\",\n",
       " \"['contraband']:[3]\",\n",
       " \"['contracting']:[3]\",\n",
       " \"['contractually']:[3]\",\n",
       " \"['contradictive']:[3]\",\n",
       " \"['conveniences']:[3]\",\n",
       " \"['convergence']:[3]\",\n",
       " \"['converging']:[3]\",\n",
       " \"['converts']:[3]\",\n",
       " \"['convoy']:[3]\",\n",
       " \"['convoyeurs']:[3]\",\n",
       " \"['convoys']:[3]\",\n",
       " \"['cooled']:[3]\",\n",
       " \"['cooley']:[3]\",\n",
       " \"['coolneß']:[3]\",\n",
       " \"['coombs']:[3]\",\n",
       " \"['coordination']:[3]\",\n",
       " \"['copulation']:[3]\",\n",
       " \"['coquettish']:[3]\",\n",
       " \"['corbetts']:[3]\",\n",
       " \"['corcoran']:[3]\",\n",
       " \"['cordially']:[3]\",\n",
       " \"['corker']:[3]\",\n",
       " \"['cornelius']:[3]\",\n",
       " \"['cornette']:[3]\",\n",
       " \"['cornfields']:[3]\",\n",
       " \"['cornflakes']:[3]\",\n",
       " \"['corns']:[3]\",\n",
       " \"['cornucopia']:[3]\",\n",
       " \"['cornwell']:[3]\",\n",
       " \"['coronary']:[3]\",\n",
       " \"['corporeal']:[3]\",\n",
       " \"['corrections']:[3]\",\n",
       " \"['correlating']:[3]\",\n",
       " \"['corroborated']:[3]\",\n",
       " \"['corsaire']:[3]\",\n",
       " \"['cosas']:[3]\",\n",
       " \"['coscarelli']:[3]\",\n",
       " \"['cosmetics']:[3]\",\n",
       " \"['cosy']:[3]\",\n",
       " \"['cottages']:[3]\",\n",
       " ...]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.argwhere(counts==3)\n",
    "[\"{}:{}\".format(w,c) for c,w in zip(counts[idx], np.array(words)[idx])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features 74638 mean 76.08121868217262 median 3.0\n",
      "features 1201 mean 1071.6511240632806 median 783.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Frequency of filtered words. (bottom=400,top=4000)')"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABDkAAAGDCAYAAAAh58ugAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xm8JGV97/HPj2HRuLE4JrLoEMX9KhhEEk304oZEg0aMGKPo5V5ixBuN3kQ0ekGRuOQq0QR3EHBDBJWRRUVW2ZmBYRAGZJgZZoYZmH1nlnPO7/5RT59Tp091dVV3bd3n+369euZ0LU89tXR3Pb96FnN3REREREREREQG3W51Z0BEREREREREpAgKcoiIiIiIiIjIUFCQQ0RERERERESGgoIcIiIiIiIiIjIUFOQQERERERERkaGgIIeIiIiIiIiIDAUFOURk6JjZH5rZ9Wa22cy+VGM+Xm1my+vavoiITA/dfvfM7PFm9gsz22hmPzGzd5nZr2Pz3cyeXW2uJ+VviZm9tq7thzwU9pttZp8zsw8Xna4MNzPby8zuM7On1Z2XQacgh8gACTcBj5nZlthr/7rz1UAnAWuAJ7v7R+vOjIiI9Ea/e5l1+907DvhDYD93f7u7/8DdX5+UkJmda2afLTGvQ83MZgLvAb5ZQFrvNbMb2qY16vyY2SFmtt3Mvt82/W/N7CEz22pmPzezfWPz9jWzn4V5D5nZ36akX+v+mtmrQhDws23T/8nMHgmBw3PMbK/YvFlmdo2ZbQtBi9dmWdfddwDnAB+rYt+GmYIcIoPnze7+xNhrRfsCZrZ7HRlrkGcC97q7V7VBHXMRkdLod6+7br97zwR+7+4jZWdkEM5FyXl8L3C5uz9W4jaa5Czg9vgEM3shUZDn3UTBtW3A19rW2RnmvQv4elinUcxsD+ArwK1t098AnAK8BpgF/DHw6dgiPwLuBPYD/hW4KAS/sqz7Q+CEeNBE8lOQQ2QIhIixm9mJZrYUuDpMP9LMbjKzDWZ2l5m9OrbOwWZ2XajaeqWZ/VcrCp9UtTJeldTMdjOzU8zsQTNba2YXtiL0sbycYGZLzWyNmf1rLJ0ZZvaJsO5mM5trZgeZ2VntVWxD1doPd9jnPzOz20MU/HYz+7Mw/VzgBOBfwhO/9uj5weF47Bbef8fMVsXmf98mqpjub2azzWydmS00s/8VW+40M7soLL8JeK9F1YHPNbP1ZnYv8LK2bX/MzB4O+32/mb0m5bSKiEgH+t3L9bv3aeD/Au8I80+0hBoCYdmTiAqdrbR+Eabvb2YXm9lqM1tsZv8YWyfp97Dj8QrrvNuiJ/hr48cqIT+D+Jv9RuC6hH35RLg2lpjZu2LTn2Jm54dj+5CZfTIcv+cD3wD+NJyLDSnn5/lmdm1Y5h4z+6tY+uea2dfM7Iqwzo1m9kdm9h9h3+8zs8My7lv7Ph0PbACuapv1LuAX7n69u28BPgX8tZk9ycyeALwN+JS7b3H3G4DZRAGR9vR73d9vWPQZ32zRZ/6Zvewf8FHg18B9bdNPAM5293vcfT1wOlFwCzN7DvBS4FR3f8zdLwbuDvucui6Auy8H1gNH9phnAXB3vfTSa0BewBLgtQnTZwEOnA88AXg8cACwFjiGKKD5uvB+ZljnZuDLwF7AXwCbge+Hea8GlnfaNvBh4BbgwLD+N4EfteXl2yEfLwF2AM8P8/+Z6Mv+uYCF+fsBRwArgN3Cck8livz/YcL+7kv0A/BuYHfgneH9fmH+ucBnU47jUuBPwt/3A4ti+VsKHBb+vo7oycPjgEOB1cBrwrzTgF3AW8LxfTzweeC3IX8HAb9rHcewv8uA/WPH6Vl1X1N66aWXXk1+6XdvPC/9/u6d1trX8P69wA2x9w48OymtcCznEgVK9iR68rwIeEMs7fbfw7Tj9QJgSzgHe4VzMpJ0nsPyA/WbHbb7stj7V4f9a117rwK2As8N888HLgGeFLbze+DEpPPU4fzsASwEPhHOz1FE1/ZzY8uvAf4kHJurgcVETWpmAJ8FromldylR4CLpdWlsuSeHvB7E1OvrEuBjbfneEvJwGPBY27z/QxQUSTqevezvZiaur68w+Vqfn7J/X4st98ywf09MyMNdwDti759K9BnaD3grsKBtH/4L+M9u68amzQb+serv22F6qSaHyOD5eYhcbzCzn7fNO83dt3pURfLviKpLXu7uY+5+JTAHOMbMnkH0xOJT7r7D3a8HfpEjD38P/Ku7L/eo/eBpwHE2ufrnpz2KYN9F9IX+kjD9fwKfdPf7PXKXu69199uAjUTV9wCOB65190cTtv+XwAPu/j13H3H3HxFF2d+cMf/XAa8ysz8K7y8K7w8m+tG+y8wOAl5J9CO93d3nAd9h8pOGm9395+H4Pgb8DXCGu69z92XAV2PLjhL92L7AzPZw9yXu/mDG/IqITGf63ev/d68fLyMKFH3G3Xe6+yKigM7xsWXafw/TjtdxRIXl68O8TwFjKdsftN/svYkK2e1a1951wGXA35jZDOAdwMfdfbO7LwG+REKthhRHEhXEPx/Oz9VEgYp3xpb5mbvPdfftwM+A7e5+vruPAj8mCjwA4O5vcve9O7zeFEvzdKIaCcsS8vREoms7biNRICdtXlH7e1ns+vpXotowB4X9e3HK/n0glsZXCbVNMuxf6+8s+5e2bstmoutIetT4NnMiMsVb3P03HebFf2ieCbzdzOI3QHsA1wD7A+vdfWts3kNE0fgsngn8zMziNyWjRG0rWx6J/b2N6EudsI1ONwrnEd2kXhn+/0qH5fYP+Y17iOgpXhbXAX8FLAeuB64luqHYDvzW3ccs6thunbvHb1QeAg6PvW//Yd+/bdp4Ht19YahSexrwQjP7FfART2hbLiIik+h3r//fvX48E9jfzDbEps0gqgXR0v57mHa8Jv1WuvtWM1ubsv1B+81ez9QCe9K1tz/RU/w9mXxu857X/YFl7h4/1u1pxANnjyW8fyI5mNmhwGuJBUfabCEKQMU9majwPpYyL4ss+xu/vraY2Tqmnu+OwnfIk9z9xx0Wad+/1t+bE+a15rf2L23dlicR1SyRHqkmh8hwiXc4tgz4XluE+gnu/nlgJbBPaBfZ8ozY31uBP2i9CU8aZral/ca2tB/n7g9nyOMy4Fkd5n0fONbMXgI8H2h/YteygugGKu4ZQJbtQ3TD9OdEVUivA24AXkFUhbTVjnYFsK+ZxW9U2rfR3sHbSibfMMePKe7+Q3d/Zci7A1/ImF8REUmm373itf+2LQMWt+37k9z9mC7rdDpek34rzewPiKr5dzJov9nzgee0TUu69lYQNSPZxeRzG893Ukey7dNWAAe1+i1JSCOXWN8dSa8rwmKvJmpas9TMHiFqbvI2M7sjzL+HiZpMmNkfE9WM+X147W5mh8Q2+5KwTpJe9jd+fT2RqEnSivD+npT9+0ZY7TXA4RaNgPIIUW2bD5vZJUn7F/5+1N3Xhnl/3HYtxvcvbd2W5xPVBpMeKcghMry+D7zZzN5gUadnj7OoY7UD3f0hoiq8nzazPc3slUyu8vp74HFm9pcW9Sz9SaIfp5ZvAGe0OnIys5lmdmzGfH0HON2iIcfMzF5sZvvBeGdLtwPfAy72zj2TXw48x6LhyXY3s3cQtfG9NEsG3P0BoicXfwdc7+6biJ5qvI1wwxSqX94EfC4cuxcDJwI/SEn6QuDjZraPmR0I/O/WDDN7rpkdZVFv2dvD9kez5FdERDLR714xHiXqd6PlNmCTRR1xPj4c2xeZ2cs6rA/px+si4E1m9koz2xP4DCllkgH8zb6cKADTrnXt/TnwJuAnobnIhUTH6knheH2E6Fom7OeB4TgRmxY/P7cSBen+xcz2sKiz3TcDF2TM7yTu/kafPJpR/PXGsNi3iAJ3h4bXN4ia4LwhzP8B0Wfxz0Nw5zPAT0OTnK3AT4HPmNkTzOwVwLFEn4F4R76z+tjfY2LX1+nAra1mNe7+wpT9e39Y/1NEgarW/s0maqL1vjD/fOBEM3uBme1D9H1xbkj/98A84NRwLb4VeDFwcbd1w/4fQBSUuSXlNEkXCnKIDKnwZX4sUcdMq4meqvwzE5/7vwVeDqwDTiX60m2tuxH4ANGN2cNEPybxXue/QvSF/2sz20z0RfzyjFn7MtEP+q+BTcDZRB2AtZwH/DfCj12HfVtLdIPwUaJO5f4FeJO7r8mYB4hujNa6+9LYeyMa8qvlnURPKlYQtWE91aM23p18mqjK5GKi/Yvvw15EnZytIarS/DSicyMiIgXQ715hzibqi2KDmf08FMTfTFTYW0z0O/Yd4CkpaXQ8Xu5+D3Ay0VCZK4madyzvkE5LY36zzexdZtap1gFE19UxZhY/x48Q7ecKogDA+929NWLH/ya63hYR1VL5IXBOmHc10ZP/R8ysda7bz89OouY8bwz5/Rrwnlj6hXP3be7+SOtF1ARju7uvDvPvAd4f9nUVUfOLeH8XHyD6DKwiGm71H8I6ENXCeIiJmhm97O8PiT7j64g6O30XOYRgTHz/HgO2uvu6MP+XwBeJmsI9FF6nxpI4nqip1Hqi6+i42LHptu7fAud51J+I9MjcOw2nLSLTiZmdRtSz+t/VnI+/IHqCMautvaWIiEhh9LsnZTGzfwNWuft/1J2XQWNmnwRWu/s3e1z/XKJRcj5ZaMYqEGoO3QX8hbuv6ra8dKaOR0WkMUIV4Q8B39GNnoiIDDv97g0nd1dNzR65+2frzkNdQu2N59Wdj2Gg5ioi0ghm9nyinqSfDujJh4iIDDX97omIlEPNVURERERERERkKKgmh4iIiIiIiIgMhdKCHGHInNvM7C6LxiP+dJh+rpktNrN54XVomG5m9lUzW2hm883spbG0TjCzB8LrhNj0PzGzu8M6XzUzC9P3NbMrw/JXhuF5RERERERERGSIldZcJQQcnuDuW0KnSjcQdaz0fuBSd7+obfljiIZQOoZoiKmvuPvLzWxfonHNDwccmAv8ibuvN7PbQpq3EI1J/VV3v8LMvgisc/fPm9kpwD7u/rG0/D71qU/1WbNmFbb/IiIiw2Du3Llr3H1m3fmYLnQ/IiIiMlWe+5HSRlfxKHqyJbzdI7zSIirHAueH9W4xs73N7OnAq4ErW+MSm9mVwNFmdi3wZHe/OUw/H3gLcEVI69Uh3fOAa4HUIMesWbOYM2dOvp0UEREZcmb2UN15mE50PyIiIjJVnvuRUvvkMLMZZjYPWEUUqLg1zDojNEk5M4wHDHAAsCy2+vIwLW368oTpAH/o7isBwv9PK3C3RERERERERKSBSg1yuPuoux8KHAgcYWYvAj5ONP7vy4B9mahhYUlJ9DA9MzM7yczmmNmc1atX51lVRERERERERBqmktFV3H0DUZORo919pUd2AN8FjgiLLQcOiq12ILCiy/QDE6YDPBqauhD+X9UhX99y98Pd/fCZM9XcWERERERERGSQlTm6ykwz2zv8/XjgtcB9seCDEfWh8buwymzgPWGUlSOBjaGpya+A15vZPmGUlNcDvwrzNpvZkSGt9wCXxNJqjcJyQmy6iIiIiIiIiAyp0joeBZ4OnGdmM4iCKRe6+6VmdrWZzSRqbjKPaLQViEZHOQZYCGwD3gfg7uvM7HTg9rDcZ1qdkAL/AJwLPJ6ow9ErwvTPAxea2YnAUuDtpe2liIiIiIiIiDRCmaOrzAcOS5h+VIflHTi5w7xzgHMSps8BXpQwfS3wmpxZFhEREREREZEBVkmfHCIiIiIiIiIiZVOQQ0RERKSNmc0wszvN7NLw/mAzu9XMHjCzH5vZnmH6XuH9wjB/ViyNj4fp95vZG+rZExERkelFQQ4RERGRqT4ELIi9/wJwprsfAqwHTgzTTwTWu/uzgTPDcpjZC4DjgRcCRwNfC/2UiYiISIkU5BARERGJMbMDgb8EvhPeG3AUcFFY5DyiEeIAjg3vCfNfE5Y/FrjA3Xe4+2KijtWPqGYPREREpi8FOUREREQm+w/gX4Cx8H4/YIO7j4T3y4EDwt8HAMsAwvyNYfnx6QnriIiISEkU5BAREYlZuGoL0YBfMh2Z2ZuAVe4+Nz45YVHvMi9tnfZtnmRmc8xszurVq3PlV6annSNjbHxsV93ZEGDZum1s3zVadzZEJEZBDhERkWDOknW89svXcd5NS+rOitTnFcBfmdkS4AKiZir/AextZruHZQ4EVoS/lwMHAYT5TwHWxacnrDOJu3/L3Q9398NnzpxZ7N7IUPof597OSz7967qzIcCff/Ea/tf5c+rOhojEKMghIiISLFm7DYD5D2+sOSdSF3f/uLsf6O6ziDoOvdrd3wVcAxwXFjsBuCT8PTu8J8y/2qOqQLOB48PoKwcDhwC3VbQbMuRuWLim7ixIzG8f0PkQaZLduy8iIiIiMu19DLjAzD4L3AmcHaafDXzPzBYS1eA4HsDd7zGzC4F7gRHgZHdXnXYREZGSKcghIiIiksDdrwWuDX8vImF0FHffDry9w/pnAGeUl0MRERFpp+YqIiIiIiIiIjIUFOQQERERERERkaGgIIeIiIiIiIiIDAUFOURERERERHKKBlISkaZRkENEREREREREhoKCHCIiIiIiIjmpIodIMynIISIiIiIiIiJDQUEOERGRQO2rRUQkK/1iyHS1futONmzbWXc2OlKQQ0REpI1hdWdBRBK897u3cen8FXVnQ0RkWjvs9Cs59DNX1p2NjhTkEBEREZGBcO39q/ngD++sOxsiItJgCnKIiIiIiIjkpCaOIs2kIIeIiIiIiIjIkLhs/kq+9Ov7685GbRTkEBERERERyUn1OKSpTv7hHfzn1QvrzkZtFOQQERERERERkaGgIIeIiIiIiEhO6pJDpJkU5BAREREREZFCPfDoZnaOjNWdDZmGFOQQEREREanA2Jhz9g2L2b5rtO6sSAFcvXJ09MjG7bzuzOs57Rf31J0VmYYU5BARERERqcDsu1Zw+qX3cuZvfl93VkRKteGxnQDMXbK+5pzIdKQgh4iIiIhIBbbuHAFg02MjNedERARGRseGsmaZghwiIiKBKh6LSJnUUeVw0fmUQfeu79zK8z71y7qzUTgFOURERNqY1Z0DERERmS4um7+SWadcxtK12yrd7q2L11W6vaooyCEiIiIiUgEFUEUkySXzHgbg3pWbas7JcFCQQ0RERERERHJ7+zduYtYpl9WdjYE3LC2fbl20ls9dsaDubCjIISIiIiIikpf65IDbNXqKxLzjW7fwzesW1Z0NBTlERERERERE6qKWbMUqLchhZo8zs9vM7C4zu8fMPh2mH2xmt5rZA2b2YzPbM0zfK7xfGObPiqX18TD9fjN7Q2z60WHaQjM7JTY9cRsiIiIi0p+xMWfd1p11Z0Okdj40jQwG122L1zE6pvMgk5VZk2MHcJS7vwQ4FDjazI4EvgCc6e6HAOuBE8PyJwLr3f3ZwJlhOczsBcDxwAuBo4GvmdkMM5sBnAW8EXgB8M6wLCnbEBEREZE+/MdVD/DS069k1abtlW7X1TZARGJuXLiGv/nmzXzz+gfrzoo0TGlBDo9sCW/3CC8HjgIuCtPPA94S/j42vCfMf42ZWZh+gbvvcPfFwELgiPBa6O6L3H0ncAFwbFin0zZEREREpA+/ufdRAFZt3lFzTkRkOlu5MQq0Lly1pcuSMt2U2idHqHExD1gFXAk8CGxw95GwyHLggPD3AcAygDB/I7BffHrbOp2m75eyDREREREZQKrIMTjGxpzPXnovS9du6yudV3z+at7xzZsLylXxdE3WS7W7pJNSgxzuPuruhwIHEtW8eH7SYuH/pP5WvMDpU5jZSWY2x8zmrF69OmkRERERERHJYcEjm/jODYs5+Yd39JXOwxse49bF6wrKlUhzKVxTrEpGV3H3DcC1wJHA3ma2e5h1ILAi/L0cOAggzH8KsC4+vW2dTtPXpGyjPV/fcvfD3f3wmTNn9rOLIiIyDHSXIdJY+ngOjtYD9mHvEHK4905kcJU5uspMM9s7/P144LXAAuAa4Liw2AnAJeHv2eE9Yf7VHtVBmg0cH0ZfORg4BLgNuB04JIyksidR56SzwzqdtiEiItKVhnIT6cz0ARGRBrEh+NUe/D1oljJrcjwduMbM5hMFJK5090uBjwEfMbOFRP1nnB2WPxvYL0z/CHAKgLvfA1wI3Av8Ejg5NIMZAT4I/IooeHJhWJaUbYiIiIhIH+pqBq/299PXj25byuoGdnSra1KkmXbvvkhv3H0+cFjC9EVE/XO0T98OvL1DWmcAZyRMvxy4POs2RERERNKY2eOA64G9iO6TLnL3U83sXOBVRB2jA7zX3eeFUd2+AhwDbAvT7whpnQB8Miz/WXc/D5nWqioTz1u2gYP2eTz7PXGvajZYso//9G5+8oxl/PQDr6g7K5KR4j9Sp9KCHCIiIiIDaAdwlLtvMbM9gBvM7Iow75/d/aK25d9I1JT2EODlwNeBl5vZvsCpwOFETffnmtlsd19fyV6UqK7mKiozZfeWs27kgL0fz42nHFV3VgqzbuvOurMwha5JyWJszPndio28+MC9687KtFFJx6MiIiIig8AjW8LbPcIrrSxzLHB+WO8Wos7Pnw68gaip7roQ2LgSOLrMvEvzVRkgenjDY9VtrAKmzmAGik7XhK9du5C/+q8bmfvQwMe4B4aCHCIiIiIxZjbDzOYBq4gCFbeGWWeY2XwzO9PMWu0ADgCWxVZfHqZ1mp60PQ1pP02oCr9MF7rWJ9y7chMAj2zc3nEZHa5iKcghIiIiEhM6OD+UaBj6I8zsRcDHgecBLwP2JerkHJI7xfeU6UnbG8gh7XspxIyOOV+7diFbd4xUsj0ZHk2sGKBrUvJwhTIqoyCHiIiISAJ33wBcCxzt7itDk5QdwHeZ6OB8OXBQbLUDgRUp06e1K363ki/+8n6+8Mv76s5KLVSFX6YLXev56HAVS0EOERERkcDMZprZ3uHvxwOvBe4L/WwQRlN5C/C7sMps4D0WORLY6O4riYa4f72Z7WNm+wCvD9Omte27xgDY0ktNjiF4CtrLk/9Vm7fznd8uGsjhSoe5oHvJvIcb2RlqU1RxuQ7aJ8IUyqiMRlcRERERmfB04Dwzm0H0MOhCd7/UzK42s5lED9zmAe8Py19ONHzsQqIhZN8H4O7rzOx04Paw3GfcfV2F+1GaYS64NtE//uhOblm0jlce8lSe90dPrjs79WnQdffIxu186IJ5HPK0J9adFRkyD294jG07R/iDPVVM74eOnoiIiEjg7vOBwxKmJ47F6dHj9ZM7zDsHOKfQDE5jA1iRYYpeAkSbHotqvYyMDsEBGBK7RqMaSWkdSU53Cob25vRL7+Wy+Sv46QdeUXdWBpqaq4iIiATDUB1eRGQYqcw8WKoMSprBnCXruGNps4dozXqPccfSDSXnZPgpyCEiItJGT6BEqrNkzVbe+93beGznaN1ZEclFYfHuqvo9Pe4bN/PXX7upmo2VQNdSsRTkEBEREZFK/P7RzVOmffayBVx7/2p++8DqGnJUrWFoclMXG+Do846RUcbGpufJ1zUvdVCQQ0REREQq8a3rF/W8rgpLzbdy42OAzlW7537yl3zykt91X7Bgo2POjpEhriE1INdZllFVBjeE10wKcoiIiIhIburDJr8BrozQ1a2L1vKnn7uan925vO6sVCbPsL4/vHVpiTlJ9oEfzOW5n/xl5duNG+ZrPit9V1ZPQQ4RERGRaWTtlh0sX7+t7mzkNgwFhWGu4XB/aIo096H1pRRsB7WsnCcQUrRf3fNobdtuGeZrXppLQQ4RERGRaeRlZ/yGV37hmlrzkKX6tgyu6VKwHaTdfMtZN3LS+XPqzkYpmv5tkuX7bpCupUGwe90ZEBEREZHq9Nv/YREBil5qZQxDwVlV93s3qMeuKdftvGX1DEs6qOdNBptqcoiIiIiIVKApBd4yTMey7DCfz6LoGEkdFOQQERERkUqpucr0oKf4aoaQx//5yV28/N9+k3n5QemnZ1DyOUzUXEVEREREcqv6Ce0wFBNU6O9dEwNjKrx2l+eav2ju9BmZp13zru7+uDtW4xeeanKIiIgEqlYrImUq+jumzpE7OmlglqRGuh6aGaAbdgpyiIiItNENiUh3/Tyk663jUZWWprNBrQWj61aGwc6RMTY+tqvubGSmIIeIiIiIyJBQobo6OtTdDWpwqmpNv5T+x7m385JP/zrz8nV/NhTkEBEREZFK9VJbqqh75vVbdzIyOlZQar0pquBXd0FikmlUms1z3Ps5RXMfWs9HL7xroANXA5z1wg3ysbhh4Zq6s5CLghwiIiIiAsCOkVGe/YnL+dmd3TsALLx/iWKTS7RrdIzDTr+Sj//07gq21lmZhZ26C1KOOuQsynvPuY2L71jO5h0jdWdFKvbTO5bzjm/eXHc2BpaCHCIiIiICwPqtuxgZcz53+X11Z2WKIgrvI6NRIr+Yv6L/xGTaalVayXJJFnHd1hm4Wr15B+8++1bWb93Z0/rTqIJPV3mOxUcuvItbF68rLzMlqzvMqSCHiIiIiAD5Cm+Fb7uKbYSNjNV9B16QpN0Ykl2bos7hKDsa1oMd850bFvHbB9Zwwe3Lelq/igBNEy+NJGnHYkB2YWAoyCEiIiIiwMSNdurNeF1340NQoOzl2OVdp73/hi/88j5+fPvS/BvO6O7lG5l1ymUsXbt1yrwiR6pqciEw7Ryp6Y5I9RTkEBEREZFIk0uSBbhz6Yboj5rKnXU0O/j6tQ/ysYvL64Pk4jui/luuvm/VlHlVF/A3b9/FuTcurqyjztb+1d0PSpP1Etj72EXz2aJ+SAZa3Z3lKsghIiIiIkD8yXvzSm1FFJjf+e1bCkurCZIKEnXtWROO6Gmz7+W0X9zbqJEgpnsApJf9//GcZXznt4tKSbuphmhXGkFBDhERERGZJEvhYZBvyusqHA1K3wF5tO9T/NgW2Vwliw3bos4xd+yqdojgYTyv4xpQ6+lvv30L7z771noyIpPUXUMjq93rzoCIiEhTDMZPt0h5xqvf15yPso3VdKNexWYHpAySWxMDCcN6rItUxHm76cG1/SdSpwZeu70ac5iRYX/q/mgoyCEiItKmiTfTImX6ZYduAAAgAElEQVS7bP5KNm/fBZT/tK6Xz5gKlM00XltD52c49fl7WMnoKk2PIgzRZ2PMnRlNP94oyCEiIiIiwMk/vGP877R78iJub+sOWNS1+aIDqMlDyNbVFieWhxKy0O3Y/WTOMq5K6Py0TFl2s+5rXQZD88MGkdExZ48ZdeeiO/XJISIiIiKZDUPHlnUVPKdHc5VyMtDtaf0/XzS/0O3d/ODa8T4+pHeqGcngRDAyyPr9Uvf3kIIcIiIiIjJJHTeotZfNpSeWMCDPoBdst+8a5Z3fvoX3fvf21OUGpRPGvjR4FxuctaFVV39GeZUW5DCzg8zsGjNbYGb3mNmHwvTTzOxhM5sXXsfE1vm4mS00s/vN7A2x6UeHaQvN7JTY9IPN7FYze8DMfmxme4bpe4X3C8P8WWXtp4iIiMiwSSu81VV+HfQC5b0rNvHJn/+u0DSbekiamq+sWgW5+x/ZnGn5tM/EsAxXLOUalKtk2gc5gBHgo+7+fOBI4GQze0GYd6a7HxpelwOEeccDLwSOBr5mZjPMbAZwFvBG4AXAO2PpfCGkdQiwHjgxTD8RWO/uzwbODMuJiIiISAZl38YmPekf8If/XV1936Pjfw96TYe4IdqVzNrLeYNR7JPaDOgF8ruHN3Jz28g2YxlHZ647uFdakMPdV7r7HeHvzcAC4ICUVY4FLnD3He6+GFgIHBFeC919kbvvBC4AjjUzA44CLgrrnwe8JZbWeeHvi4DXhOVFREREpGZJDwO73RIPaDlhXHyfy3wYOiAPWnMb1Dv5YT0fTTKo10bTvek/b+Cd375l0jTV5IgJzUUOA24Nkz5oZvPN7Bwz2ydMOwBYFltteZjWafp+wAZ3H2mbPimtMH9jWF5EREREuslwH9tP85GL71jO1h0j3ReUccmBoeYUOJJyUmThs0nlWGsbNbeMvO0aHWNzEz4jTTrwHTS+3D0AxzArBTkCM3sicDHwYXffBHwdeBZwKLAS+FJr0YTVvYfpaWm15+0kM5tjZnNWr16duh8iIiIi00UVt7Gnzr5n0vtu5YABubfuKJ79Mp88Vx34aN+XpBor23eNVpehAnU7llmuyV7PxlULqh0Ot6MB/9wNirK+Eoruy2g0Y3p1f1+XGuQwsz2IAhw/cPefArj7o+4+6u5jwLeJmqNAVBPjoNjqBwIrUqavAfY2s93bpk9KK8x/CrCuPX/u/i13P9zdD585c2a/uysiIiJDwMweZ2a3mdldofP0T4fpuTs879SpusD6rdN3eM66CwBVumTewzzvU79k4apsnXg2QbfhaqswqXA6ja6XLJat28ZtiyeKdsPQXKXTKW5ah8sNy05HZY6uYsDZwAJ3/3Js+tNji70VaHUzPRs4PtwoHAwcAtwG3A4cEm4s9iTqnHS2R2f8GuC4sP4JwCWxtE4Ifx8HXO1Nu0JERESkqXYAR7n7S4hqnh5tZkeSs8PzTp2qV7onPUq9baqpRNGkphlN1qQ7XjP49b1Rh6sLVg5OkKNIKoIU78+/eA1/882b685Gbr1cCf1ePkVdfq2vfTVXgVcA7waOahsu9otmdreZzQf+O/BPAO5+D3AhcC/wS+DkUONjBPgg8CuizksvDMsCfAz4iJktJOpz4+ww/WxgvzD9I8D4sLMiIiKdDMhvt5TMI1vC2z3Cy8nf4XmnTtUbTx+F4sW/X4qKE/XSgevAGobH9T1SgE/K1i0Y1/r0jQ3Ipbh790V64+43kNy86PKUdc4AzkiYfnnSeu6+iISbBXffDrw9T35FRERapvG9tAShxsVc4NlEQ9k/SMYOz82s1eH5AUC8a/r4Oo2mgF+58hzfpn8ftQ9g2Gnf+r2kGn4YOhr4j1KDD/ygfU/1cij73cWiDtFuZoy5MzYgUY5KRlcRERERGSShNumhRH1+HQE8P2mx8H8vnaSPa2JH6I18ctzALDVRpyeyj2zczqObthe+vdZFHt+uJ8wfZu2BnqLVWpjX564wTT6U3a6x3cI1ruYqIiIiIgPO3TcA1wJHkr/D806dp7dvo3EdoQ/IfazkcOTnruLl/3ZV3dnoWZU1WvIG+dKq+uuzJP3ot0+XovqEmeiTI+t2C9lszxTkEBEREYkxs5lmtnf4+/HAa4n6Bcvb4XmnTtWlB4NeVqyqdkwTjtN0qL3RyQnn3MbnLl9QaJpNOKdSrk6fmarOfbfttGpyjKq5ioiIiMhAejpwTegk/XbgSne/lJwdnnfqVL3SPelR/DZ2644RPnTBnazdsqPjMpJPnf1sXHP/KmadchmPbCy+6UrLoF8beZ9Cx5urXPf71Xzz+kWxxNLXvXv5Ru5cuj7fBmvQ9L5hBl1Zn5ni+uQI6dVdRSOj0joeFRERERlE7j4fOCxheu4Ozzt1qt54sfvYn8xZxiXzVvCUx+/BZ459URWbTJ4/GPfWmRS1L4mjq3RJ+we3LAVg/vIN/NFT/qj/TLQKP10Wq7JwlLdA7u78ZM5y/urQ/XncHtWO8vzm/7oBgCWf/8uOywxKwbIuwxyAacqpn+iTI9vydffrpJocIiIiIjJJ/Aa1dVO72zCXJCpQWWGl4rKFtVW0L6twk+fqy3usr/39av7l4vl8/or7+konMS8px2MQanBI/+r85sx6DXcNpI33ydGQqEsXCnKIiIiIyCTx+9jWTe2MUF+5rBv2bunW/WSwSNMlXlTkbpY5gsmW7dHI0Ks3R02yqqhVtHDVFt76tZsyLdv3MKIFZHhAyraptu8a5a5lGyrfbj+Hrinfe+qTQ0REREQGWvw2tnVT2wpyVLHNYVdmgTFroaiq41328KpV27JjhL/492v6Tmfd1p0F5KY/u0bH+N4tDzEyOlZ3VnqSNwBwysXzOfasG0sZSrks/X5XFPV9MNEnR8btanQVEREREWmq0XC3WndzlbpvmvtVRvZ7eco7ZDGHUqTVfHhw1ZacafWbmwIKuh3WP/fGJXzq57/jh7ct7ZpG/n5O8i1fhbuWbwSiQFWVBqG5SjcTfXI08MQmUJBDREQkaEq1UJG6xQt5Y+M1OdqXKXab06nsXWagoeoySPu+NKEMVHUgp4zNVXEYNzwW1SbZuG1XBVtrjiZ+19Sdp65dcoQMjjbhA56BghwiIiJT1H27IVKv+G1sVR2Pdu0HodStV2tAygmZtK6KsvepzI5Hp6xf4PJFHJbyOnMdlt+6bPvRCt42sQlV078SWscsa/8ude+PghwiIiIiMkn8PrbVJ0d7kKOB5YRpJ3EI2eqzAcDSddtKTb/M662Oa7nKYWGLCCDmzW6/gZl8a+creA/SV1dVl0m389Xqk2NA+h1VkENERERkOspayGofXWVi/WLzM0gFj57EDli5zVXqL4XE8zCI5zXtECbNStvHQs5HSae0yOvwv/+/a4tLrCQN+Gjk1newqOg+OQYkyqEgh4iIiMg0lPXmt6rRVbppQuG9TkU3WRi0w1lF04ph7Zep22cny7XQLSCyeM3WHDkqWsbmKrSaq5SZl8HU7RoYH0I2a3OVmr9gFOQQERERmYay3oI2ZXQVmSrpHFZdtEi6LJL6PKiyzJP3Uu0UQCmmP41mpJGkzE90k4Nog9QXSVVDyHbT+kw1+bzGKcghIiIiMg1lbq4y3idH9L6sWEfXfgMG5Oa6k36z39T9by8wOm3NVWrp76LfBArJRmN075NjyHa4g6Z+htI0JcsaQlZEREREGi9zTY6x6P+6m6tINgNSBslvQC+/QrrkKK0qx4Ae1KDX41LXbqcFlsvKUlHXzvgQshn75Kj7a0hBDhEREZFpKOvN71iG5iruzlULHs18A5xksItb5au70DBIuhVif33PI8w65TIeXL0lcf50qdnQMrSBsTZN3s9OWctS4+6SeQ9z3e9XF5uhNuNDRZe6leIoyCEiIiIyDWUtyE0EOaam0HL53Y9w4nlzOOeGxX3kZ7j13bY+IYHEaV2OZKXBpAIfmReZ78vuXgnA3cs3FpJe2m4WETApa4SN6RpYrKsmR1JfNUX40AXzOOGc2xLnZb1yun0/jed9QL6oFeQQERERmYaKHF3l0U3bAXh4w2N956uTJj+Fnc7ay22DfJ5aec+7D73sc5MOU5Py0oussYO6R/zoZftNOzdpAbftu0Ynlqs54wpyiIiIiEhH4zU5Su6TY5CeKl82fyWzTrmMdVt3VrbNzGWG4hdMlfW89VsbIc9D8KwFrEqapTSgT45O+1lmjYa6C7lpyqpRUYYyaoAlLpexBlhact+/5aGMuSqfghwiIiJBk2/KRIqWuyZHKBiU1kFe1/nN+YCee1PULGfhquQ+Hbop6rumCUPIdjM4xckJeY/hAJWZk5X443ffI5t5zZeuLS39PJr22ahCYfvcYQjZe1dsGv+7nz6ZiqYgh4iISJuBv2EVySB7nxzR/62aHM25jR0sZfWrIFM16Tu8iNNW2uAqFYWgHly9tZR08x6X1meo6sujr5ojFX3us9d+muwffjA3eZ6aq4iIiIhI1TKPrjKWPLpK0YXubsWAYSrk19lMoEkBgDyKLJBnTmlIrrlu10QZu9mkmlftmvgZaPwQsuPpTU5wrKFfzApyiIiIiExDWW9NR73V8Wj0vq7mKk3Ua0eGva2WNLpKT5vv2Y6RUf7neXNYuGrzxMSEEuOgnUtrq4pfdweV7crKT/t+D7smB146DiFbUZ67baVVG6W9RUr82mnSdbR73RkQERERkeqlFZzi5dbRTjU5+tx+E5+mlqmqAkCZhaK5D63nNwseZcuOXVxw0p/m2n6/+19Kx6NV9DvaoIJfu2n2Eaz8XLzhzOvZbTfjWTOf0HMafee54Joc7Ql2yl/dASXV5BARERGZhrLegraqIxc9IkH7zXHX5iqFbr1evRzKJhWWuzZ/SJk/96F1LFlTTh8NRWnQoQbKP/dpBdJeN92k67Wl6izd/+hmFqzc1H3BBsg8CsuUmhwTE+oObMSpJoeIiIjINJR2Txsvg3fqMb+JhZiqVNVhYy+qPi9pRyJp3tu+fjMASz7/l6XkB/IHkfo9m2nXQxMKfp2uiWGpTZV3Nyr/jPRxoPuvyFHQUNGtpk1T0m8m1eQQERERmY7Sghyxm/LRsbB4zVGNurcfV0fBNXGLCROz5qySphpMXEtVNlfpV9689nI9NOhyLiUvfSeZI1NNuuaHXfsx7NTxaN3HWkEOERERkWkorWAWL092Ci4UHXQY9vLHsO7flOCDj/8j1F/YS1N0E7RB0eBTMkW/37O9Dg3brlVbqf13o0NFv9opyCEiIiIyDWUeQrZtwboKRk26l66juUr2zjSbc6SKPEqFDiHb5RrOWzOj7Ouh31PabX/S5g5CGCR7Hpvz2ciqKTnuNBLPpD45mpJZFOQQERERmZZSCzYJpYYpN7eF5mYwClN1KLq/hyoCNE0q7NRt0A9F7x2PNm/PG5il0hXdlGdKnxwdR1epl4IcIiIiQd0/yiJVSh1CtoKCcHsgpdvnb9ALKH0/je9x9IOytV8rZW2+igpETegkNEm/+epnNJxh0trNJgZgOmlaVtuPXcOyN05BDhERkTZ6oizTQerNaVJNjrY1mnbzPajGxpyH1jZ7SNV+DOT3acq1nbeA3OQCdVrgaO5D63nuJ69g3dad1WUoryE6F2XJvM89HppOHY/WTUEOERERkWmoafem3QvDDcswxeTorGsW8qp/v5aFqzbn3lYV53DuQ+s5/+YlHecnNm0qLTfTU/99cnSbP3WJb1z3IDtGxpizZF0p26xT07770jSldlGnUZI6Nlep+SCXFuQws4PM7BozW2Bm95jZh8L0fc3sSjN7IPy/T5huZvZVM1toZvPN7KWxtE4Iyz9gZifEpv+Jmd0d1vmqhaPfaRsiIiIiEsk6usr48gXfs5bdx0fTdDreNz24FoBHN+1IXz9zx6NZ85PN275+E//3knsyLt3/9qrUOlZ5m071so0p0xtwRFL7ewkZ3223gayLk6j+I96DkgNcE8ulL9m6CtprbsTf1x3YiCuzJscI8FF3fz5wJHCymb0AOAW4yt0PAa4K7wHeCBwSXicBX4coYAGcCrwcOAI4NRa0+HpYtrXe0WF6p22IiIiICKTe/cYLfR2f4FVcZGjQ/fO4Iop/o2EMxhlDUph098nnqsNuXXnvo2zfNVpJngZd6Zd+wgZaQ4MOwlU5TUfCzaSo783Oo6sUk37RSgtyuPtKd78j/L0ZWAAcABwLnBcWOw94S/j7WOB8j9wC7G1mTwfeAFzp7uvcfT1wJXB0mPdkd7/Zo7DR+W1pJW1DREREROg2bGT1HY8OYjmliPv7kbExAHbvEuRoBZV+MmdZpuWq0ut5+1/nz+GMyxZk304FJdlWgS2t4FbXEMr96PSEPW1XWk/od+txf5tY+G1inrqpKsvdjs14kGPKeskr1n2oK+mTw8xmAYcBtwJ/6O4rIQqEAE8Lix0AxL+1l4dpadOXJ0wnZRsiIiIiHaU0tz3NzB42s3nhdUxsnY+HprP3m9kbYtOPDtMWmlnjapXmveGfsnjRzVeKTa5cBZZzWzU5dp+R7bb8ny+aP/53U49Z1mrrD63bljnNOkILSQGjOqrkl73NpNRbNTl6DXI0Ues4DlKwo//+WIrd2fZrcayhx3L3sjdgZk8ELgY+7O6bUqKfic0/e5ieJ28nETV34RnPeEaeVUVERGQ4tZrb3mFmTwLmmtmVYd6Z7v7/4guHprjHAy8E9gd+Y2bPCbPPAl5H9CDmdjOb7e73VrIXGTShT4A8qs7tzpExdt/Nkvsk6CUzHdYZaQU5ujVXKbhPjqKklYG7lY+b1IY/rsjPRhN28bL5KxOnp52e1rlpcowj76FtwKlorG7HplW7b0pNjtiUJlzrLaXW5DCzPYgCHD9w95+GyY+GpiaE/1eF6cuBg2KrHwis6DL9wITpaduYxN2/5e6Hu/vhM2fO7G0nRUREZGikNLft5FjgAnff4e6LgYVEfYgdASx090XuvhO4ICzbGOlV8pOWbxtCNuf2Zp1yWc416vWcT17Bv/78d4Wl1+l4jYxW2ydHlYXWJhV64lqHwKdMaaZ+D+MpP707Pf2EE9V/XzENPPkNzFI3fQfcit7ntvQ61eSo+7Nf5ugqBpwNLHD3L8dmzQZaI6ScAFwSm/6eMMrKkcDG0NTkV8DrzWyf0OHo64FfhXmbzezIsK33tKWVtA0RERGRTNqa2wJ8MIwAd06sE/S8zW2TtnOSmc0xszmrV68ucA/S9foUtK7iYB03zT+6bWnyjAIPQvY+ORKmJRyUMg9T1r5asuZ1aDQ7RtJRlj45mlaT48aFa/pOY1D6rWmSiT452oLd8dFVqsxQF2XW5HgF8G7gqLb2q58HXmdmDxBV4fx8WP5yYBHRE5BvAx8AcPd1wOnA7eH1mTAN4B+A74R1HgSuCNM7bUNERESkq/bmtkQjuj0LOBRYCXyptWjC6rma1dZVs7TfQmffbcXb1m9SQaDKAnnrifkgdmgJU/M9aWAVK64T20GtgVJEgbrsyzEp/da0pvXJcWlC05t+gm9N1+3c37NiY/r6mbeTbclBGV2ltD453P0GOv9evSZheQdO7pDWOcA5CdPnAC9KmL42aRsiIiKpmvprLZVKam7r7o/G5n8buDS87dSslpTpjZD7ci+7oNV1fnWfzyq/CkbG63unbzRrnppQY6KMLDSrqD0c0oIDrXPYa/CtAZdhR03OW7u0rN64cA3v+s6tKUsUZ2oTr+T3E9PrPciVjK4iIiIySBr24Eoq1Km5bauvr+CtQKuzhtnA8Wa2l5kdDBwC3EZU+/QQMzvYzPYk6px0dhX7UIQsH4F+b2Kb/Dmr8va81SdHLxKbhWRdtwEFvSbkIS5buClnmoUkVsfoKtHUViuqBn9cJ/mbb9zccV5dAcCytrp4zdbu284aHO22QPjCbk9vrGkf4qD00VVEREREBkirue3dZjYvTPsE8E4zO5ToXnAJ8PcA7n6PmV0I3Es0MsvJ7j4KYGYfJOpbbAZwjrvfU+WOdJN/CNlyb2a7FaKqvJfutep2L2m2anJ0S6uo4192cOmxnaO889u3ZFq2tqe9bcegyQG3Mo33s5BwGiaCHM0/OPHr6LYl61KWm/z/IEj7LqpjP6b2yVFDJjJQkENEREQkSGlue3nKOmcAZyRMvzxtvbrVXZ24XZNy02nEgHK2FYIcBaVXd6FjzkPrE6cnd0Zabl56VeQT/15TimehjuPU+gz0OrhKE09tXddbnWGirN/z3Y7NeHOVlOUmzav5AlCQQ0RERGQayl2To+AO5zZtH8k1rGylNTky3qHnecjdKf+7Rsf6Wr9uVT3oH9SOWQdB/HofG3O27BwZD/S0+u1o6OUH5O/ctgn91mSVmtUK92Oi1s9gHDsFOURERESmobRb1XiBslOHc/1as3nH5G0WnH4/JjpdzLZcP0YzNlfJvv3BKIQ0WaGjq3RKLE9Tp2KyMkVS4Og/rnqAr171AAfs/fiw7eG5noZpXyDbdZG9T470BbP8DjTp+KrjUREREZFpqO8hZAvKR9b0mjS6SpEBmZGMbWOaU3yYrP0pevt1ZSmlozyXYJOCYMMmfh4umx8NArVmSxSEbF2eeY9/Ex/4NzFPg6awjkxLpiCHiIiIyDSUewTZjLUbytp+lVoBlV539YLbljLrlMvYumMklmay8ZocBR2Rop7cViFPHspordIekOm1SVZa1oo4ymUVzlPz7a3/82/8W9c/yM/ufLi3TOXQ63Gp/8rPLnMfGJ2Wybyh9Nk2PrpKSkeoDTqwaq4iIiIiMg2l3ZBmGkK24DvaJj2pH3963WPJ+hvXPQjAqs07OHiv9Nvt0aw1ORKOd1KQoO5yRnz7TTqng2D5+m3s+4Q9+YM96y+ita6tXjrh/bfL7ys4N8Wo+7NRtCzfwUV/T6el1qThZFWTQ0RERGRayjc0YdlP/rs2V6my49EabtbbN3nh7ct40am/Knz0laK1x4EmjQrSZd0GlYky67UGVBav/MI1vPec2xO2Wc6BSovhtQc3BvBUTTVeO6WmzffS705DjnyW0VX+8+qF43/X/dlWkENERERkGup7dJXistI4rX3rtyZCP8GSU2ffw5YdI+wYyTb6ysQ2e97ktNV+nuPH8JZFa/nujYsry8ttS9ZVtq2WpOu06cG1uKwVrtICBo/tHGXp2m0F5ag4qc1V+lw/T1rjo6tkS6529deFEhEREZHKDcrNah08xBWqHLW0a2EkuXpNoxlF9ldRT+OX4791CwDve8XBPeai/5OUN3D10NqtrN26s+tyefrkyNqEraihfvPs8qpNO7ov1CX1k743h98+sIYln//LnGnV41f3PMKnf3Fv5dvNGrituwZKppocZnZVlmkiIiKDrOHlBclJ9y/p8vbJMWXxPj8wdTQJyWqi49H+CmzxAl+33e1UKEg7Tkf829TLuftQkMUGC/pKLc/oKgVmu/MxKLYT2Lq86t+v5a+/dlPm5VOHBc1xKNKWvXXRWmadchn3PbIpe4IZ/fKeRzItl5a/3z6wpqDcFKtTlv/+e3ML3c7seSsYGe1ca6zo742ypQY5zOxxZrYv8FQz28fM9g2vWcD+VWRQRESkaoP2Yy6T6f4lm14LckV9PvI+8a22T47wR5csllkYntLXRYMK3umF4g7BmgrzX2UNnG6KuG5LG10lw4HKc97Slrzid1Eg4qaFazOnlySe5SyB0u27RvnEz+5mXaxmS13x1V6uy6qCwWdcvoBvXr+o63INjk1P0q25yt8DHya6IZjLxFf9JuCsEvMlIiIi0ivdv2SQ+2bVh+MpdxZjOaroZ9XtuBVVeMjcBr+g7XUruKXNL+ta6nff6ijI1Vmzqd9hSquUp2NbiGoo/PDWpYyOZrvaimxyMzXtUpItbJtrtqQ0+xnvkyPraFDZt1uG1CCHu38F+IqZ/W93/8+K8iQiIiLSM92/ZNN3QbDPAmreQl2VwZWitlRkwbXq4EWvmlImdneu+/1qXvWcmV0LrXkKtYVdG+3vUzuYrH50lfFt52qu4jRt4ODWsXM845CrDasJVOG2dkvZ8dacXoYUrkOmjkfd/T/N7M+AWfF13P38kvIlIiIi0hfdv6RLLTgl3OtOtOAop2PBBpUrxgt2ne75m1QIapyC+nBol/eQ/+zOh/nIhXfxb2/9b/zty5+RvP0uaST29ZowMb22SjZjddbkSMllnnxVvQdZspa0TGpzq55z00x5AmQzdksJcrRqcgzIAcoU5DCz7wHPAuYBo2GyA7pJEBERkUbS/Uu6rDernW5ui77Z7VrgrLRPjvSOR3vJS6/5b62WfSjILgsWHKCpqg+jvIGlFRseA2D5+qnDgnZLq9dLbdYpl/W4ZnGdf+ZR9JmrugCcZ3OGZW6uUvSR6ScoWuUxTa/JEc3L3FylkBz1LusQsocDL/Amd4MtIiIiMpnuXxqs1/v+KmpRjNda6XFbvbTpb79Ks6ZQZh8CZSvrg9nr4cj2TZGzmVXGxWutyVFQnxxVNCnLe27jOZoYFjfb8k3wyMbtPPtpT+x5/Tznb0aGcVcH5dc00xCywO+APyozIyIiIiIF0/1LitxDyLat0H+fHt23mbZ8mbp1PNq1JkABJYEpfTZ03Fb6+6ZIbDaQI7N5aozEk23yiBaTt1n5JjMdnCZ3NFzGeWra5+fvzr61sm2ddc2DbNi2M3HeoMVRswY5ngrca2a/MrPZrVeZGRMRERHpk+5fUqQVXjZtH+mr6n0W7ffMTSpb1DIKQqcj0CUvTTpuMDk/Zlb7kNyFN6vKmd4gjEZRVFOZpgUIoEPANmWPmxjU6SeYk3fNz162oJC81F2BMmtzldPKzISIiIhICU6rOwNNlr+wlv5+mEw0V+mtgN5Pc5WxMcdsonA20SdH8ugQdY9o0Vd/A2nzCiokXX3fKv7l6OflWqeOjinTmquU3SdHWvoPrNpSzsYrkvfQNTFQU+WILzAUvbkAACAASURBVLtGxxKnD2XHo+5+XdkZERERESmS7l/S9Xyv2nazvWn7LkbGkm+Mi9x+lU8Gx8bSm6uU6Y8/cTmved7Txt932+9ej0pZR7Os89RrIe++RzZ3ntmW1avuW9XbRlI0uU+OogvOZezCgpWbCtlefF/rKqj3eo77yW7Rn8cBiXFkHl1lMxP7tCewB7DV3Z9cVsZERESqNihPKCQb3b+ky3vzO7Xvh2jCi0/7dVFZap4KOwqJJ3XVfat4wp4zpsxP7tci/X3V8my+iA4v123dyUtPv3LSNLN8tWniS27bOVJYR5x5jKV2hln2SW3uj98bv/LbvtbPXWOtxEPxkQvv4q9femDu9fqprVXU7oyPrpIxwbq/h7LW5HhS/L2ZvQU4opQciYiI1GzQOtiSZLp/SZe7JkXO5f/px/PY9Nguzn7vy5LT67O5TJlaeSvyq6DXJ6rd1mp/OtzEPgUgeT/y5LTT9/LChOYU7vX3CdDSKRtTpjcju32p+to7/5YllW6vLk26NJr6/dIua8ejk7j7z4GjCs6LiIiISGl0/zJZ2WXAn935cCnV/6swPrpKt4hnn1GQeEG8U6E8PuzlYBQvenPZ/JX89I7lPLppO9CMfe3cB8pkRXSumqdPjvVbd3LiubezbmvySBh5FfVdUHVcadm6x7ouk/QRTh9CtglXXnHi+7p6846eg39D2SeHmf117O1uROPOD8guDrala7ex3xP35Al7Ze0jVkRERED3L93lba7SXmOgWlXeXGfeVJ95Smui0AqwdCt0VdVcpVPhqNfOWdud/MM7AHjqE/dkzidf11daeZurTFXcQcxaaM7TX8N3b1rCVfet4ryblvBPr3tOr1krfOSbJn65Nqm5Sq+KyNNDa7fyqn+/llPe+Dze/6pn9Z6X/rNSiawl5zfH/h4BlgDHFp4bmeIv/v0aDn/mPlz0D39Wd1ZEREQGje5fUjTxZj6LKlqT+XhNjk55aAUgcqSZMC0+kkH3tDqMrpKz2FH08ZsyFHBbdlKfACdMXLNlZ5hV3gXajPBHcrqJNQ/a3reumz1376lS/rQ0qePR1CFkm6e/2iXRuq1aL9f/fnXuIMcXfnnfRGoD8sORtU+O95WdEelszkPr686CiIjIwNH9S7p+b1Wrv9etcHSVEvrkSDISq8rR7Xh2aq4ypSZHl21u2j6SKW/dttNxuZLOU9G1Dial3Tbyxvhwvhk6eu0m7+gqWfZy50gIcszoL8hR/OgqBdaAKaoJTZ811upiNrmpWp2+fu2DvPLZT21EXrLK9MkwswPN7GdmtsrMHjWzi80sf9ewIiIiIhXR/Uu67Der+XrVHw7Z+uToGpjotpUMB7XfbbS7/verM2+7n+1AwYPT5CiQ93Ot1jGUK0zkebeEa679XBVdk6O4gMJgSO+TY7IdI6OcfcNiRkPEa9vO3oKEdRnvRLmggFbWoFHdvxdZPxnfBWYD+wMHAL8I00RERESaSvcvKfp/YlnsXWzXAnGFN81Za3L0W2shSy8n409zST4GU0ZXKbOZR4aCUnzz3YM8KfMy5ah43mXbZdVUmejstvuyrSDHHv3W5Ohr7anqLtgmyVsDqH0fzrrmQU6/9F4umruMhas284L/+yt+esfyAnOYP091pNG67pt4jpNk/WTMdPfvuvtIeJ0LzCwxXyIiIiL90v1Lirz3qq2b3KIKRu2FxSbdOxf99DOeZrdpLVk33am5SlVNEbJu5xM/u5v7HtmUK+1J28m2mVx5Ss5L7+vm2k779Z+rdkEryFHMSS4scNOkD3GQe9/aFt/02C4Atu4YZcHKzQC5R43q9yz1c36KPiUNPMWJsgY51pjZ35nZjPD6O2BtmRkTERER6ZPuX1L0/WSv4rvdKjc3UajorblKt0LN+Px4rYdMfXIkRUq6bKwgvTZXiQccbl20ri3Nzk+Hq7i+xgN3Nmli+rZLylfasMXt+dk1Gk3ot7lK0YGwZsu2s2XU1Bm8/o86q7tvnqyyfjL+B/A3wCPASuA4QJ15iYiISJPp/iVF/g75SspIkLW8VdSQpWnGxlrbSl+u0yHp2kwjR1661XJ483/dwP887/bY8jkSr1iTOnWMm9zEptg8Zi4UtjWRSsvHzpFRoP+OR9u33Xc6DXzOP6hDyLbF3XrWabSjXtNp4jlOknUI2dOBE9x9PYCZ7Qv8P6KbBxEREZEm0v1Lmr7baOdYNksHm13TyLHBPmXvXC97ppLSjE/rFjBxPPEYLF23jaXrtk2ZXlV/C3lHd0laN2md9uNVRXALkkdUiUuaXUTW0ptITd7q+OgqfdbkKPoz1ZQAQcu8ZRtYtXnHlOm99AVT5641atQab06wMk3WIMeLWzcIAO6+zswOKylPIiIitRiEH27JRfcvKfL3yVGszT0OZVqF9qfqZW+n32Xa1uglKxlS7TXdiaPYnsLEEJnZ0j790ns55r89nT955j6xNPrf30k1OdxT9zV3zYB4IMt9PFjTns7EELLdRzNqNVeZsVtRfXIUo2m/oG8568bMy7aGbG3ifUBfNTkKOitpAcm05euSNfy3m5mNf5uEJyGpARIzOycM2fa72LTTzOxhM5sXXsfE5n3czBaa2f1m9obY9KPDtIVmdkps+sFmdquZPWBmPzazPcP0vcL7hWH+rIz7KCIiApRfsJHK9HL/cpCZXWNmC8zsHjP7UGtdM7sy3Hdc2UrXIl8N9x3zzeylsbROCMs/YGYnlLSPPeu3Gnee9ZOW3bBtV1/5KVPWjkeLzPKU/bfWNjr3W9FIaU/J+7xmzr5hMW/7+k3585RDmYc5bf/TRldpX69Vk6Pv/BSSymCIH9fUIFan9fvZdi/rlFRzqd9k3ZNrlDVN1iDHl4CbzOx0M/sMcBPwxS7rnAscnTD9THc/NLwuBzCzFwDHAy8M63yt1UkYcBbwRuAFwDvDsgBfCGkdAqwHTgzTTwTWu/uzgTPDciIiIjL99HL/MgJ81N2fDxwJnBzuPU4Brgr3HVeF9xDdoxwSXicBX4fxgMqpwMuBI4BT4wGXJuj1CV8vN8lF3BNX+YS127GxiY4Tkucnpdmlz9Ci9q+sw9SxucqU99kLkN5heh5ZOursmJ+Ep9PR0/yUdfroyyY+3G97Ku3DFqdtZWcYQrawGhhF9ckxCKXfLrJe51Xq57AW3d/KoJziTEEOdz8feBvwKLAa+Gt3/16Xda4H1qUtE3MscIG773D3xcBCohuCI4CF7r7I3XcCFwDHWvRtdhRwUVj/POAtsbTOC39fBLzGqmrEJyIiIo3R4/3LSne/I/y9GVgAHMDk+4v2+47zPXILsLeZPR14A3Clu68LTWauJPnhT216rXY/KDe5/ZhorpJ8C1lFobDVnCdeEM/UvKWAfJWln0Jwp3WLLliX2bFien8frZocCUGbtve7QpCj7351iu6To9jkEmUfWjlfk6MsHb7WpsIgR7flu/VZM55Ovs0WLmufHLj7vcC9BWzzg2b2HmAO0ZOS9UQ3D7fEllkepgEsa5v+cmA/YIO7jyQsf0BrHXcfMbONYfk1BeRdREREBkg/9y+hyethwK3AH7r7ypDmSjN7Wlhs/L4jaN2TdJreGP33P5c9hWwF0fRlqrxpTms6EFdkgahTSj33hFHRM75cna92ekpec/Amvg9da3LkDg7G/57cP0fceE0OS54fN5q1pJlRlus4y+XUpABoz8coR+2sqjQp8JLeY01zFDPuUHZfB54FHEo0lNuXwvTEWn09TE9LawozO8nM5pjZnNWrV6flW0RERKYRM3sicDHwYXfflLZowrRu9yrt26rlfiTvE/CkfiqGoXp6krSbSogXRLuk021+lry0anJ06RAz6zZ7VUS67flvXT/9FJv6aa6SpIhz1kvaEx2Pdl9vrIDjlnf9QfuotzpnTZK2K0UHG4vQ1/VMvmulUzBr4nuo97xUqdIgh7s/6u6j7j4GfJuoOQpETzcOii16ILAiZfoaouqgu7dNn5RWmP8UOjSbcfdvufvh7n74zJkz+909ERERGQJmtgdRgOMH7v7TMPnR0AyF8P+qMD3vPcwUdd2PZL1XTbvpzdz3QcZtpaZR4c31REAn/fltro40u6w/KIWHbuL7YTb5+snzcH3K8ehYCyRL4Cd7LSEv+Fl1fNtpfXLkOf9F1+QoanfTjtuOkdGCtpFNq9+Slm41MTqNelOXeH4bkqVxRXzmylZpkKN1cxC8FWiNvDIbOD6MjHIwUeddtwG3A4eEkVT2JOqcdLZHR+0a4Liw/gnAJbG0Tgh/Hwdc7XUfZRGZtlYnjM8uIs0V+vE6G1jg7l+OzYrfX7Tfd7wnjLJyJLAxNGv5FfB6M9sndDj6+jCtOQp5Mp9xuSzNETImVkW18eJuHQsJ70z8lek4dm4S0V8u8qfVqQZC+/yy+4ZoT79TXytZ8tJfvyLxN5PntY7NbrtNLWy3b7MV5CiuU8nuMrV+SknoR7ct6zyzBLtGexuBJtN1XnHJsohrLu2az5ROAXmpUuY+OfIysx8BrwaeambLiXoYf7WZHUp0nJYAfw/g7veY2YVEbWZHgJPdfTSk80Gim4IZwDnufk/YxMeAC8zss8CdRDckhP+/Z2YLiWpwHF/WPoqIpJl91wr+8Ud38pP3/ykvm7Vv3dkRkWxeAbwbuNvM5oVpnwA+D1xoZicCS4G3h3mXA8cQdZq+DXgfgLuvM7PTiR7YAHzG3bN2yF6J/KNEtBVQi8xMlu1XuMXx5ip9DiHbqTyQ1Mlhp/1rSpmiylo7UTrt11tyykX3PdIeiOlXPLW0tCcKo921KnL0m9Vh7nh0JK25So5OSZswfEUxodJizo7TvJolSUoLcrj7OxMmn50wrbX8GcAZCdMvJ7qBaJ++iInmLvHp25m48RARqc3ti6PyzIKVmxTkEBkQ7n4Dne+jX5OwvAMnd0jrHOCc4nJXrEL6WMiYyGB0VTdhLJQiuwY5eux006f8kbJebP0sWyuydk2/6bQfv362OTbWKc0MVee7zW9rNpQ43K87Zta12VHWfEztn6R92c5NWwrveLSgi6EpATmYWpOj22e509C9iftUceCjEcc1Z2Ct7ixX3fGoiIiIiDRAv0MLRh1hFqdrWhXeNXfb1HjHo32m4x3fJC3ruQqjRrGFo6x5TX9Knlw7Izmo0L79PAElL7yD3CydhnZbz1NaUIw3V8lQdWCi49H+5Fm/EQXtHNr75IhL25VM10rVzVVqDxlMaFJe0ijIISIiEgzGT7dIMQqpAp31qd6Afbi6tWNvzb9l0Vp+dufy3OlnHnqnB+WNrlJ8oKBTkhfNXT6luUHHpj8dggL5shurNdFD5CrrpsbcWbxmK5fMe7hjfyVJI/e0LzvRJ0dxTRCKSac5H/Se++RoSHMVK6jn0Tz90aSmEwtIdqrp1CSlNVcREREZVEW38RZpotxDyCZOK+7GtsyhO/PytgJnJ9+9cQkAbz3swA7pdEi/y/ykvGRvrtK5mUMZ8lwD7a0sHLh9yTrWb905afr/+cldvO8Vs1LXHU+jj+Yqea5pT5ufowr/6758HSNjztknHD5p3sT+Za/J0bcc6SxYuYlzb1zMe19xcBHJlW7XSLbaNS3dhk6dvHBveWqC/ocdzrhczdeCanKIiIiITEN570GL6qW/X0XGIL/wy/t43qeumDI9a3OVbiYXKKamOikg0SUgklf3TlPzB7l6qck/+YH01ATe/o2bOel7c6dMv2PphsSUsw7yMbm5SpflY/PHenhSnX4sJ+aNuTPScWSUlJocbemP1+RI2WoeWc7rz+et4LRf3JueToZtVfUMIa25SpJchfLKm6tMlfU47hrrrUZLx7x48vVed1CjnYIcIiIiItNQrzelWQrmRW2r7LS+fu2DbN81tRDQbaSLfvOQp5wXH2Y103ZLOicdgzA50snTX+ayddsyrVt0zbtOwYy1W3cyZ8m6TP2HdE6787yxLtdcXFEdj9ZRLq2qMDzS3vEo2dp/NKW5SlxizZOM6/7D9ycHELsFqbt/zhsWzehAQQ4RERGRaSnvk/xsfST0qlvNgiqfFE70j9BfCSdPc5XOQ8j2vuNFt5PPUvsjdZPtHY+mLLxzZHIhtdOySdO7dVo6ZYSWtr+T9vO4b9zEcd+4OTkPiVOn5iWe1/Z12kf0mTS/beGihpDtlJee02nQ4/xdKUPIJpk47hnWy/m10Pf3SEKesnRQC/x/9s473o6i/P+fuSWNJCSQEFogoUvv0qtAEH6g8kUFBSx8UYr6tYOCgqCCFBEQLICAFEWQGloIoYQEUkklkN57z7257Zz5/XF298zOzszObDnn3OR5v17JPWd3dmZ2d3bPPM88Bcs3tKZqW0ZnGKKyU6smpOQgCIIgCAMrN7ZiXXNbfEGC6GTYyiPyVFpcCbRPJ5jdhLcSK6t+b3VNxfbBInik2I51nxK4i2jLOfsrOZZXoIrJoW0ukoklOS7nqiu7cM1m7X5b4d5kgFEec/EDvGzJke6mZK6orB0dR+K4JVlZS4UUWgn6Io6D5Rta8Y/354b35/Qe1MYmCf7ymrrPOkjJQRAEQRAGjvrtmzj0N8Oq3Q2CyBxnGVexOm6rvHCdFHcUirj55elYtam8ClnJzA2BUJKXIKFsU9OXFO1kfsVSK2octBwSgTtHCuluxIwV2OeXr2JTa0epeaVFhLlTKuHZqKwJfdYLvtHsKnqrj0LWFjo1KLXm+bybTjc7q5aMKgLwnX+Ow00vTcf81U3ZVZoQvTtLbY0hUnIQBEEQBEFshWQxJ80r1saIT1bioZFz8asXpqaq84f//ggPvjcncV/S6jjiXXD0gqy8g3M7pZK9dY0bSYVOlwCgxvYziAz5xzc/RVuhiNkr9cKiLvBoeX+8e4ypbl0P/Tps3BCK2uClbmTuzlRDcq7clbjL6ltO1FpaXgBY29QOIByLpdJZ4IIsT5r9H85dI5XPuUMxkJKDIAiCIAhiK8TaCkP6G2x3cQGwLwqgPJkX/eqTTJqfm7gYtwz92Pk4/9pkGZMjacDKjZ7VgXWbQRaSuACDbhdUl8I2r8CjctHA0sG+inJdGbir+HQ4Bv0MZW4xHCsrT0IWIIKia+XG1sCSIys5MrOUtDWEORuO4TjDPpfXQWpliagcVPSqWvFQdWPlaw9+WOGemCElB0EQBEEQxFaIe0wGNzP9NFQ7o4EfXC+vbqiVBTFWH3C/Z7Umu6aJsxFY11jcFOfrJLqRxKj/dJlN3py+3K1N2XNHPj9FM0+PW4ijfvtm5gFH2zqysl5wq6elvYDP3fUOPpizOpP2w31JeJzJlSWhki6tslTlqmUbeLS+zrdQSdWFgPYCxeQgCIIgCIIgahTbeaoppaDtaqVNubgSlZxXd4I5vJa83FWmLlmPPw2f6dwfm0CaKnRKgER1BdYtfl2GVf44Sw5N1o7LHxunqa9c3mQxIcfkCNXh9X/U7LAyICths62gSZnhyIgZK/CL56YYy4jnN2vFJsxasQk3vTQ9k/ZtUV63oF8ZKXwyfImorJjidBw79u4GAPjykQND25PqW/zTaW0vRPdVUAFuS0OV2ycIgiCImqEzrE4QRFY4uyt4f0MxFhyPTcJDI+eCc469B/QqtV8BQ21uEDizoCxsC23GHaRxFzEfkt1L7ZKHxqRuI+KS4TAG07hUxB0quxWZ+qWz5HDvh3wtSn+D2BAVEBP9NlWCaxJutFBWVOx3NmE7pv45uatkeP9cr9mMZRuwbEMLAKBOTpecslutHVGFmGu63kpAlhwEQRAEQRCEAfUENon7RBJufnk6bhn6cUWj9xclgTMpYSWG6BIR3ZYV1jVW4HLK188lhaxMXmVl4pQpzjE5NHWrshUBGksOnTVVRjcxK0uOpLg8ZbaKBvnaiIclidfBuZubRh6vqzpLd5W/vlMOtpxVNwKFWEchcm3bqzx+VJCSgyAIgiAIYiskk/gOtq4RFuVqKwWh2ZLDVvkRThlq1aSxLqdrFNPFrATkvAQ/uX+BdU0CxZPrpTfH5Egu0Jmyq8hxYOyemcRdCfWhtd3+nNI+p2mto6zdsRy76XLdK4HqMonXzvYyyueT9vqrLDlULlzVvo6k5CAIgiAIgtgKsc6uwt3K50VFY3Lk3FhSgcqmODdYC2RF0rEQVVw4HOsstJalueCaWEh4ce24uqvoXFRki5FyTI5oH/Me+y6WHOnT1qY7Pm90Y5sxVvWAyCKmvoR3uT1zequhEq3txUiZ9hSKv7wgJQdBEARBEMRWSHphJZqHYvbKTZrC6dqqNKpsBkmIExh4aJv5ImUdULQawmYapYaTuwp3U8SEFENaIbf019VdRSRkyaE5P0NylQhZ3cI2xep83m3mXqehUlN7+kDLlXVXiTs86btp5KxVTvdbprUjGr9FF4y3mpCSgyAIgiAIYivEVWhWlZe3vTdzlaaO2psEm8jMlcOlbKy7ipvgVCuLzqFAtSkkv1SBR/2+RLarzexVTfkxENxjcqgta7RBWJUxOfJ5fvx6VYJr3DFZkKdlhGsv/b5kl55XX9H6ze344v3vY96qpmwaM/VD0Y3lXlBSFdp74lWksvpRxeSo9juflBwEQRAEQRBbIWmnoDyDOpwbrFRTenkzE3SpTDMRIDO2+MiSaOBRF2sL74PFTeHgkruKdTNaZYqfpSJNdhVzClm/HS+7ioVSMSuFQzUsOTjneHjkPOfjrAOPStdGtHwwW3nEu6tMXrxOec1WbGzBxAVrY9sYNn05Ji5Yh3veMqRkVhwfislhfR3sytnS2l6MdC2NdVNekJKDIAiCIAhiK8RWQPLLqUrb15GsTLX898vxEdLVE3aDQORzaJuFr3w101Lq63FQVDjEB5B3VSowrW60+0JyGtN8Ucmhjcmh7VN+qIJJ6kh7G/xnatz8tXh2wqJ0lRlI2k0bd5WFazbj7+/NiZQ5/7738cX7R6VqPxNCab6z7Yk6hSzF5CAIgiAIgiBqgNRTX14J4atsRl9t82cXbOMqRFfm07ftlNGlwjidb4qYHHF1GYtqygaWHCmixXLNZ+V3lSuNW8vWuFlypOuFf55p4kLEMejaoVi4plnfB5USy9Fua/Wmtsi2petLbiAlhYj+Otko7HSuVK5k535ToqW9EOm/0l2lyu+ehuo2TxAEQRAEQVSFhDE5klg3JJ3vtrQrYgVUINhEmnMFdFYv5c+61XqzzJ+tVqkaSqM0Vu0uMTl0CgPd/ZQzoBhjcqRYtQ7H5Ajv88+vTtVJrXVB4q6Ejk8T78SV37w8Hd271GO37Xrk2s6MZRsTHWe6FOKtGdRP3//WjqhLR7n+8pNnUqzE3ZI0r8E0t1s1VijwKEEQBEEQBFETuAq5EVcDjTCoPDbhymWbMHnOWg4z9ansOpBPdhWhhHVdN744DRc/+KFTm5VQZBjPUbp8aVxO8oiT4uIi5bfrnEI29FnvriIr1qxcvGowQK4N1/13Sug+2o6LQ256A8+OX2xVts40UIxuUuGduneAUhnlsbmtYHB7Eeo29DGrJzezeryK2gscf3s37KrTQSlkCYIgCIIgiGrimkUgSZC+rCjkOHk2nldKS44kfYgzcW8vcMxaoUnRqyCu7xVZvM/QHccpfafjsWHlg1ow9GNypAk8KjWq7YO8O6/nrDO5gAGlrCSbVdZdCkxKCJ+Fa5px6h1vY8WGFu170eUadWssidbN7YVcfIvCCpLo+XHOcecbn2DOyqbIMVmOp3vfmhX63q6w5Ki2uwopOQiCIAjCo3NN9wgiGbbxIiKoBFblSnh2sQQ6crTkMGEKAgm4KD+E/ltchSxOsdKJDuKaC63YRwKPGuIWSGWzSCFrVTYmu4pzClmu/hwJPOrp81TCq03daXBSIHWSH0qb6/joqHmYu6oJL3y0JNhme3qqct0b6wEAm9s6QuM3/AyUD97U0oFB1w7FWzOWW7Zp7t3KTa24961Z+GjhOuMxiWJ7GNomdxWCIAiC6ATkvXpLELWAtSWH476kQpDquDxTE1qdl+ZlYOsvH29BYNcfF3xLg3rGnLKX5IJ0+aIpZPXoAnMmigkjKa1MQT91faqrS2/JYbrfEcWamJknp5uVtbCbFBfFji31BinXPO7s3FVUlJUcRe2YEuufuaIUN0S2jFD1o7QtpgNV0jW0k7sKQRAEQRAEUU18gcI9JkfyckmFNDHIY9bz91B616iNOoD8Y5yKzU5auC6b7CqmAJa6xitEFgEPZaFTPd6irh9iNg/T2Odc3U+/VWdLDqjHWVxMDpGFa5qxYHU0W0hUWcPVwXozpFLDJm07Nu4qynZTtNmtS0nJ0dzWkbp+5bgWPlunGlZZ25naTXABVJYc1XaFIiUHQRAEQRDEVkjaoKFcU4faXcXCVcPSkqMShlaBQK1pzFZ+cpnm3//2bIfSegpC3yshZrjFykjeo5kO8UiA8D363SsfY5/rX0WbpzST4xTYBGutC2JypMiuIn6O6NXCY07cfeNL03HS7SOM9QHAv8YuxH43vGbdn9+/OiP4/N7MlVbH5DGm0gSk1WGMO5pFc4pKujV4lhztBa1CgvPk78O466Q8JrYlXV1qJZyKPGMnJYWUHARBEARBEFsRSWNy2CovshRXRNeArAUhs8DpxqpNrZi6eH20jRh/FFXGmrQE7irG9BLVsWw3uYlEyiasU7X9vxNKGTla20vCmCm+R8mSQ6Fc81PIZhSTQ27DrzZNRp/Xpy1LfOwlD42xKpeHQiIPZBcYa8Wk5emphkFdnb9PH0Q4FKvD0dokpDhRHFpQvaMtt5nqjSNFVuXcICUHQRAEQRDEVgTTBSaIQSWgWqfgTCgXqaL2Z4UpDoNrutIz//guzr13pLm9mD6ovifBF+DjlBxZEaeYEQW5qBuJSdmg3icLYUkUQ0HVDnEP/BgP6bKrcMWnErL1kM1YkK9RoykQRUZ0DhWHefyrxowwSq3qNwbN5fpa8tQRFZRun7bElQAAIABJREFUI4o+WNTl8m5SpZCtti6MlBwEQRAEQRBbIUnnoCxGHEhqjfCn4TMxfv4ar40SqslzHkRX1X2B005RsKapLbbe0Odgm0MnLRGzdMQJYpWmUplffvCvj7B6U2tku8oVRP7ONSPYH/fuMTmEz8KXolSPq2JNrhsAulRCydFJtBzydbS1kEljyeG3oVMAu7ZhPi56Plm+L126mCbzUV6QkoMgiMyZ5eg3SxAEQVQOcSJugxy/INgOtUm2vGnR2marCfOCNc244IHRoW2hFLIWdbhgtCJIUW97oYjlG1oj9VRKDPBN1uMMOarhdqCzmMm0Uo93Po3GmPDb84UyG+saH19BMnTyUu0+F6LXIqxYS3J/GuspNZiOcABYwz7r+qL446DIdaoyqbxlW+ZWy6isjKyt7VK0TO4qBEFs8YyYsQKfu+sdPD9xcbW7QhAEQagIzOHzEXLlak+4bYQyXoUNeVpymNxV/J1JRMZrn52CTa367ApivZGYHFm4q4gpZC2PmbRwHW4VglC64NJnlxXfNMJm3PGBksNX4ElWNqbsKioOHdhH3z+FBU+pD3KfvHYMDemyAPmkdVdpaS9g9somc6HaW7RX4mo1VFYu2ZU3WkkF/6mOc+uXy7HKmByJ27I/kgKPEgSxxfPp8lLO7+lLN1S5JwRBEMlgjD3MGFvBGJsqbLuRMbaYMfaR9+/zwr7rGGOzGGOfMMbOErYP8bbNYoxdW+nziMNeiAwLhMF2jUm2agVz9spkFn4hSw6NRUkWyOdhI3DqGDa9HPxRF3TStC0t5b6bO3/jS9OD3+zz//w+/vLO7PytO8wyuhW2tySpq46r6b2Li4g58Gjp+8QF63Dv8JmJrk2XhnSi3bQl8XO3aqcGtUW+jyZ3lZALnuX9j3NH0Svfkl8/8UjV461M5Roo9JK7WcVBlhwEQRAEQRC1zyMAhii2/5Fzfqj37xUAYIztD+CrAA7wjrmfMVbPGKsH8GcAZwPYH8BFXtmq4xJ3NLwKnXzynzSAaDj+Qbx1xYQFa/Hm9OWJ2hIJXAcyTFhrcwUyya7iu6tYzPInzF8bbj9jM3b56smCZxZxCxL1WbLgCFWhqW/J+hb1DhZzHqF29S4T4lC/c9in+gpDdYcrqUjg0ax0HHHuVBk141q3bbtmZRg3Zv1Jbl1h3q90V1HX5NyGSUmyVVlyaFZBtmOMDWOMzfT+9vW2M8bYPd5Kx2TG2OHCMZd55Wcyxi4Tth/BGJviHXMP89TVujYIgiAIIo7OkhqPyBfO+bsA1lgWPx/AvzjnrZzzuQBmATja+zeLcz6Hc94G4F9e2ZrB2pJDN+nV7FMVb89gqS8IzGgQjr50/yhc/tg4jPhkBf4YIyiG3VXCvb73rVleY0l6qq83tnzW7iqO9WUdQFCuLY9XrO4a27ir6OpzuW9JA4XKfdAFIjXWJ5VJa8lh81boLL+SEUuOhClkdccZVRzcbly6ZNIpHVsuqOqWKShuOMuRRVsON9o2FkglyVPd9wiiqyDXAhjOOd8bwHDvO1Ba5djb+3cFgAeAksICwK8BfBalycKvBaXFA15Z/7ghMW0QDixc04yx82zndwQRhYRFojOT5eotsUVxjbcY87AwH9kFwEKhzCJvm257BMbYFYyxcYyxcStXRoMlZsWGlnb8d8IiYWJtIdAo3C3ESb9qIq+q11XJoXK18Gu1eT6/+Y+x+NPwmcYyukCEG1vasdrLlpL6TWBwT5DbzQpfuKuzkOrk5vPOfhIdL5WdK/itRQKPKsa5LS4uTaY4MBErF7duAEgfeHTsvLWxZSo1v0vbjMlqSHUOrnFyTHFbilz/LKW5fkksOfzT0cWGkSlnILLvp2vGoUqQm5JDswpyPoBHvc+PAviCsP0xXuIDAH0YYzsBOAvAMM75Gs75WgDDAAzx9vXmnI/mpTv2mFSXqg3CgRP/MAIX/mV0fEGCkEjiv0wQBNEJeADAngAOBbAUwJ3edtVbjxu2Rzdy/jfO+ZGc8yP79++fRV+V/OTpSfjR05PQ0l5WOKze1IrT73wbczQxM3TWGqWdapNsVfG2jnSWHPV1TEjrmqoqJ7Jsy85dJT2+oMOYe4WJLDkMx0TdVdyrj9Qp3RT9+NTXUXZX0exz6GddXIBXjcVQxJIjyaWXvqd1V7EJPlt74qyaxGONA81tHRgxY0VM/Sothxe8FDximRNqwtJaxBVVoOYgppKhfSj25RVQuFJUOibHAM75UgDw/u7gbXddBdnF+yxvN7VBEARBEASRCM75cs55gXNeBPB3lCxMgdIcZKBQdFcASwzbq8byDeGYApwDr09bjtkrm/D39+Yoj0my6phlTA4f0fXClwdembIUExbErzzr0J2aKESnteri2i/+puyFA1/gqI/LIasgD1klZPkj1Z9Fey46Dn88y5ZFJuVDHE5X2WAxErU8cLO0AjpZTI64dlI+G/L1C1ugmdotZUj65iNjMWvFpmTpgU1KBLFPhtGjenzFelX9snUbMbtrAb9/5WMsWrs5th4flSVHtQPUNlS19TKuqyDWqyPGRhm7AiWXF+y2226uhxMEQRAEsZXAGNvJX0QB8EUAfsyxFwE8yRi7C8DOKLnQjkFprrI3Y2wwgMUoBSe9uLK9DhN1FODBRNoUN840WVWbpkQ3pY3JUV9XXi33lRBXPTEBADDv1nMS1alb2czSUMQU90PeX/qeXjDw5Q0bd5VIfxIIJm5H1MaK79rmdgAaRYhme1JCblHi9hilxqRF7mmX3TK9JDvLaguvtjjHwhTiY8xdVUqj29zWoRXujZYa0CsSbC97fR1DUVIOx117pbLB32RQsIlMX7IeL01agrc/sXedLKRUYudBpS05lnuuJvD++nZArqsgi7zP8nZTGxEqZR5KEFsjNWi5RhAEYQVj7CkAowHsyxhbxBj7NoA/eAHPJwM4FcAPAYBzPg3A0wCmA3gNwNWexUcHgGsAvA7gYwBPe2VrBs7LgrB2Qh4qL0+4NXEmFBPxtEqOhjqWaywAnfBQaRfMbN1VWKxQZMrwkQdy/Umak2+Jblyox2Y8rsOsjsWkV7FsR742L02KN/yS76+L9U7ixymjMRJrJZWynWgKWbu6bQPPGq1BONc/S9xOUaRSUoaVsdH9JmWDrbWSv6fNYO3k069nVwBA+9YUk0PDiwAu8z5fBuAFYfulXpaVYwCs91ZLXgdwJmOsrxfg60wAr3v7NjLGjvGyqlwq1aVqgyCICuC/dJ+ZsCimJEEQRG3COb+Ic74T57yRc74r5/whzvklnPODOOcHc87PE6w6wDn/Led8T875vpzzV4Xtr3DO9/H2/bY6Z6OHoyzE64PkCT7amjpkHho5F4OuHRraltZdpU4Q3rLSO+jSeYbMyTPMrmIbvyQtQXaVBLP8vH3r46wXEtWZonCQQpaHCzl1i5mtLlTBexVdUQeNdMTFQ6n2xNJsSapQkxUJuneA6lkRi2qVb0JPTO+XJO5mBUulsw027wK//6oUstVe7MzNXcVbBTkFQD/G2CKUsqTcCuBpb0VkAYALveKvAPg8SmnXmgF8EwA452sYYzcDGOuV+w3n3A9meiVKGVy6A3jV+wdDGwRBVJB1nikoQRAEURuo4iH4q4U2E3JVfarDHnh7dmSbvCroSp24WJ6RlkO70CqcVOqYHBoB972ZqyJtyWWS4gs6dRYpZCOZJBLcJpc+V1LusVXKRco4dtIthaxe6ZVEwZRmvCR3V6kMadtxFe7L2VXsrqsh7iiKBksOa3eVGEsOFaaYHLp3UfQAdRnVMb4epqMG3VVyU3Jwzi/S7DpdUZYDuFpTz8MAHlZsHwfgQMX21ao2CILIhtemLsVhu/XFgN7dqt0VgiAIIgUcHHXear+N+bK6iJ1LwNDJSxVb7bFxvUgD13xOgir9banecs0L1jSr28rgFH05J4l6Jm9Ljkys2iP+Kupi6swpOmVeGdc+6u63qh/i52h2FXcXCfm7S9+T3oo8hkgl6gxZWRiVt3aWFkY9AZfGmhT01EaBUK9IBxxnBaIOABptw26sSQpQRRlfCbxVpZAlCGLLo6NQxHcfn4Cv/FWfXphSyBIEQdQmkUmrYMmhW4gzpxrUCIw5CcpL1pWyw2TnriJ+Vq+wJ8usUK5g5opNaGkvROrNE1NAxDiSHOmifFLFdXElquNwaD/SH1UZu5gMQX8SuoikUVDY1B9bNmF7lQo8mvY9klRhZ2/JYbZyK5putodJQZbMksPObcTmyshVqc63bMmhSl1bXUjJQRCENf4LS04rRRAEQXRujAKBb76sstqo0Ex2TVMbbn/9EwDxK+fWcOXH2Nn5z56ZFLibxHHzy9Px46cnqZvn0dgPWQiQol++a215WHKILj+V9NO3ymbjlbE25VcQNxpDsm5ImeZuyRGtXFZcuih8EioBqi29WiIrjUypjEv7/fQqdvXHvTJ111c3BiKWJzHvOdVek9tI2FrJrlzcdr+PZMlBEARBEARBVIWoiTRXxuTYffseymOiAqJ64lt70103wibhUVHi6XH6wNovfLQ4csyHc1d79Urt8OjWLARIX4CxqSqLuBAmGAtfT5U1UVrcYoLYuYS41OmidDMJmlZtKp7BpCS35MiGuMuWdmwkVfjY2vGYAo9yzsOWEJK7SuQ4xbVo6ygo+mlGHbzWV+TpFSqh0jxaPq7xLILmZg0pOQiCIAiCILYCTIFHxQn58Xv1C5eLCT5qs80Vk4CSIOmAug1NEEiNK70VP/jXR/r2clYoBPUWxfNya8O2uK3lQ+ScLUzgXdFV4RI8MjwW3PoUa8kRuljRtn3SuBnp6syDPFM5Z4nLuBT51iPj8PHSDQDcY3KwQGlsSMvN7RQdG1o6FMeaFbDK7CrKbsTfQ5vsNH4XlLFAqjxOSMlBEARBEBIUW4bYGuAoKwzECbkuQJ/KEiEv/3xTrVm5q4Tm4Go5NGN45JvDYqk1aRLZ5KF4Ccv46etPc/9tFR8uvXSLyVGuWZYLVQJqbH2R8eNgvZDUkqNCsmvaZlysC0ypYuVyPmaXD7t9rmM57oxM8TfCVkTxbcjnoLRcCVLI1p7ii5QcBEEQBEEQWwEqJYU/yRYnsHXCxLuUJaASvQtTzUXAvFYgVZYcebQUuKskqLyl3V1DYlZISWWlwqrV6qzat7Ey4ort7nclJruKZvtf3gmnWq60607emXSqTZK4EzJyCunQ2IixFNG2n+Kyx90ypbOK4n3gYuVkwv+taE+ZIjwPSMlBEARBEASxFcLBBUuO8vY6jf94Xq4pavQV60RK19XEsCGH3mIlLS7uFFkoWEThKnblV/p+6h1vp24/0oZlLIAs6nc/Vr3NpU4nSw5DtUmUDmmy1SS9alndw1iBPWU7EXeLBPXdMnS69vg4JYrudcSTdqZ8tGG3ab/4HJY/d2+sD5Xyh7PN9fPLqt691VahkZKDIAiCIAhiKyAiEHEhhawwSa2rk1cvNfUhPyWHqV6dUOkqJOqEb00IhdSoLGnycVeptnhRpqPA8ejo+cH3SqUf1WGjvHhu4mJ84x9jreuMj8mh/iyTSMlhaCv22CpbcuQ/FpIrgHxGzV6trz2pJYeAq+PVP4VnSdmu5TbxFdG7e4O6rogCLVpT2ZKjdt45PqTkIAgiUzJL7UcQBEHkCgdQ580E9e4qXDpCriMn144kxxgOuvHFafjjsE+t2quUIM7BcxE0fQGmVH9cJ9K3b6qitSNsxp6F/iXiAqMpp7q2EaWAn3lC2GabHljXHxOm08/C4t8pJkcF2kjTgbTtmCwRkjx3jMUrQMtWEPpnT289FN+H5z9aEuqPbd3yPvHaypYc5TLxdSOwBKw9JYdadUMQBEEQBEFs2XAeygbgUy9YcpSsNXikDLzt1bDk0K1/mibaj4yaF21D27jYj+xOUGVJk2YlXkcWWTriGDd/LWYs24D9duxtLCdbleSh1HGzXsi8+UjcBnP7BheHLAKPVuBaZOaukk01WuT3wYI1TYa+xPfGxl3FVzyI701VW1wq78KoWauw7469NH3UK/ZCilyTFYr31yXFsepaVFvvQZYcBEEQBEEQWyEc4ZVHn7iJdyUs9kxCR1bN61Z285qby7oHpWCQQetJsnQk4f+8dLmmPkcEpVx7FEZtuq8R3HK9ZnZjq5Kr4cUix6wVG2PLKa0FMupDXLDXrGNyvPnxCqG9KDavFVtFATjXx+TQbLd9r1384Ic44pY3sWjt5sg+W/2m3Icjd+8bW0alQFG5O9YKpOQgCIIgCILYwukoFDFjWVioEeesooBVL2dXkcqrFCNZk6TqNN2xFl4yhCvaSnUO3sG+JUfp3rlXOHTyUrzw0eLYcowxbG4r4HFDnIDo6rdzd6LtRluxarsWMK+GJ6gvUr9dJQ+OnIMLHhgdW04ld2dljZO3W5hT8FgLFUfJXUVQWMUGHtVZcqjrzgKlYk9hiff1hz4Mldmlb/fIcTYKynpFdq5agdxVCILIFIrIQRAEUXvMWRU11RbNpkUBq16XXsUjMMm29CNPgqla3e+Mc+DRULYB9fYsiQikxWhbaYQFzkv3xqUOVcmrn5wAADj/0F2Mx9Yx4A+vz0BTW0FbpqMYDjRRSXcVF+uAXO04QpUbrF4SaDlULlA2fLRwnVU5JgeiQD6WHHkQFxg0bX0xhhyGmBx5uvmZFC92SpeydZNmu4AfpLqojCdTXcUHWXIQBEEQBEFs4ahXZBHMQ0UBKxR4VAhe6U+SWbAvR4WAYbKuW/V07omFOXkyYUhnWSALi9FyaSwd/EP9RAd5C5F1jGFdc7uxTDQmR549iiep5UNm7RstOdL3Jeuz0b43MiDvK+96PV3d8IwpZLk+qHBJAVK5cWfTlHjmqmC84naRBk/JUSkXORdIyUEQBEEQHjX4O00QmaDzrfcnriF3lTrzZJ8JJsq19MykERxCVh2a7fZ1WZYTlEw+aYKGyu4qNp1Jc//mrNyEtpiUILLwk4dSzKXGeo0gm+c4tq06C1eerM+jTnm9MnJXiens8g0tePfTlYnrNys5kp2DWOWw6csj+323lyLP5n727dHoVF51yoHSwrI/5XOIV1CWLTlq6IfAg5QcBEFkCmWQJbYEHn5/LiZZmvMSROcg+nIWlRQFYY4a8lbh+pgc4r6sMU3I1YKXe190XgSpV1ktLEQA7/rHlEnSbKX845vaChg6eamxTKEQ7ovarN0NecVd7xYQ3dZQb3dsXpiaS3Lfoi4U2Z6QbZrSJMRV8+rUZbj04TGJ63eVu3XTV9325RtasWJji7KwrZVbnNWY7l2nrc/Qpm6PbgzZuOf4j5PKkqPaCnBSchAEQRCEBOfA+X9+v9rdIIjMUBlniH7jomBfJ6WQlT+zkBIk+5nskx8uMGdX0WznjgJ0OA6HZnsSdxXNdptAfmljcgBlFxGFoUjF6ZDdVSrYtqotl7gKmfXDcjzZ3Po4wdmqDs7R1KqPoyKS68JVzhffnK5X7goPvffC+8LlRHRZRTg3Z1exVW66Xn+lJYeNu4oUbBqws+Twrf5q0JCDlBwEQRAEQRBbOip/80KxPGUXJ7Dh7CpcMbvN10T5F89NSahcqI2ZdjT2hr89XE7l7pPGt13lelRtooJS+r7JI9nlvpuE0krgkm43SX02NTwyah7esXQDUVkSZHWl8n5ebV5P4uk1xLjpqahnDJtaO7C2qU3Rvmasac5brdBwjROiaE+hyDYhBpaWaoqU9cdHpWPb2EBKDoIgiJypwXc/QRBbGaqpcqFYDo4nCn+6mByBu0rIJDsfTO9NXYBA13etNrtKTpYcEeGGK4TUFD8Y+17/GtY3tzutquYtnEQsOXJozsVdJRIjJIP+xK2268aZjE4B44TFCb04aYl1dbkGHs15bmSjNAopdzXvvVBQTqlKxhiO+/1wHHbzMGxoaceYuWuCcuKzJaaotbEg83HVuygzCsVZ/2izrthbcqjGbrWnvqTkIAgiUygkRxmKT0IQRK2geh+JMSFEgSCcXSU6WQ3F5MhpJhtXrUo4d04hGxI2uPJzEmyF7qLi+qWVc5du2Fx2V6mBwLByTI4sVu/tf1sVgpd2dT05LtfYVDTJvRfbnruqCfe8NSv2mNZ2e78utSVHNoMq77FpdA2SvjMwvXLXUCdjwIaWDgDAg+/NDbbLVlpyYGO1W0l0Y5buKqp96zeXsiOJzfhxc6xicpC7CkEQBEEQBFEtmEIFLVpyiBNaOfBo8Nmb5voT75JSoTqzW+Vk3rUOi7qTZVexE6TVKWTTX0+XOl6ctASfLNuYuk0dsiVHHsKQS5WRlLYVGL9hyyB9e0msasQjvvPPcVbHtHbYxeMA8g08mjeu1zMuq5SKYvjmlj9CP9bFfnHpGBnVezsrnhm/CIfc9AYWrtms3C+/R1TvFV8J9vHSDdl3MCWk5CAIgiAIgtgKES0JdClkOYQVSe+vP/HWrUhmgUlAYUw94U7TF50JeRJshBu/TbloFi4LYh1xQvyEBetw1t3vpm7Tpi9APu4xujpVl1LeZlrlrjQ28VhMRTa32ykv4tL+iqhcwzJzV8mhThHTo+S3t3R9OTuKLiZHyF0lUpG+fvEdFXJXUbXB1OPY2V1F9V4M/ob36dLzBopvi/ZMiqFqP1Ok5CAIgiAIgtjCUSkFikWNu0rMzDo4KsdZrKlmxtT7XVfldauo4ZVWpyqjlYXqDX8vmbRHFR9pqaXAo3nEwLBF6dJUBbt62xbTuqu0WLqh2JYDNFmZMnNXyfde2DwHQ6eUUyDbpGuV+6xVaMLsKqbapapLF39IxxvTl2sbM14OoZmiprwpJkctQkoOgiAyxfWFTBAEQeSPan5bEPzGRbcCMbuKGBxTrqOosETIihUbWoz7s7Dk0LkRpD0na3cVpaVBBu4qjql080S25MhGAWM3z1ALkrK7ir5sVtgqzZK5q5SPabG05Gi1LAdUzpIjD4yWHJGsNDyRwC7H2gg+c/39FPfFWZC5TqknL1pv7KNN3dr3l+J86mt4zk9KDoIgCIIgiC0c3aq2P6FtbisLPnLgUV0dcauVabjjjU+1+xiYMcBeWtK6rugDj8a7q2RhaFCwFKqrQS4xOTR1qqw20qToTYqt+1BHIV3fbAOKOrmrJO2MBfkHHnVrwCaFrOl59TOr+G3rx7p6PGQReFTZmsYyQ1e13jolitldpbovH1JyEARBEIRHJYLQEUQ1UMcnKCspmls7gu2iu0pp1bH0uSBNlk2rlXlS8l+Pbk/z/IbrS3dOuqMjlhyIajmyDDxaawoOoLKuNDqXgF7dGoRC/rXKr18hJYehGRflg6o+2+NbO2ojJkfethxmq5noNitLZOk4MXvQhyElh36si+9U0zYg28CjcvXPf1ROJaxLcRs6XnnNMuhYTjTEFyEIgiAIgiA6N6pV7fLWJsGSo16zBKYy9a+GHM0UfQHcrQSsJvMZnmA0Jke0TBYxI4ohy4HaYtFadSYHF2TBSqvc0lxfcfX5nrdmobG+Dt271Kfulw5xrJoE7/YESo4kuCgp8hRiswz2q8LFaoeBJerD3cPVFmemd2NpX3SvalsmlhwZHaF0V6GYHARBbC3UslaXIAhia0UZ/6EYDXwJyO4qXLDcCJcVLUEqCWNqgcR1NX7lpnLcD53AlSiFrHYFV3ZX4ZH60+o4Xp2yDJMUfvlbMrrbvlGwTvIpcB6JI3DnsE9ztTApWgrzbQ4WFpUi18CjmdSiJ+6eur4vOKLP67h5azV1m2NyqIJ7qmLpZDGlLsf/sDtfk7uKPMc3uqtYtZYfZMlBEARBEASxhaOauOrSlfbr2TX4rJqEB5s4MHKWOg1hnsiWHO2FIhrr65wVLv/zl9HK7WnlXVufdq5oK62w/afhM4X6qy1m5MuMZRswbt5a9HCwwuBcnT3od6/MyLJrIcLuKvp7ksSSI293MVXGkTxi3+SCyV1F1X6CDmldUsCN+wLFsbRdxibjSxxc+qsiFGvaoJxhUj0UeJQgCIIgCIKoGqoJtJhdxefnQ/YLxSwQza5V7ip/HjE7245aInZls0O2CF0d4SwJ5c9j560NhM+2jiJWbmxN0I6/khrdHlFyZByZs9rB//JkyN3v4frnpzrLppUWzELuKoZySWJy5I3qSmU1osTnLI87Eqcw1GXa0aEKeKwNesvNWY5U72PVo5+FO4jr86F3s4nuaagnJQdBEARBEESngDH2MGNsBWNsqrBtO8bYMMbYTO9vX287Y4zdwxibxRibzBg7XDjmMq/8TMbYZdU4Fx91NpLoamN9nX71UJ6EZy2QW8PCwnuLF08kjRVESOEhVfPchMUAgB//ZxKO+u2byduQ01aqXIgyvKRbqn5DHp2up1npOAIFS3+V9o74M5Hv6R1vfIrrn5+SsGfxqKxeslKc5T0+jSlkpewnNlZPUWcVw7Xg4bLhLCrlc/ePZ7KJhEeDLkBSEiyvtylW0SED+4S2mbOr2HYsH0jJQRBEptSuTpcgCMKaRwAMkbZdC2A453xvAMO97wBwNoC9vX9XAHgAKClFAPwawGcBHA3g175ipBqoFACFojqmhmy67E/E5awd1ZrDMoQn0L4lR5pJdUggkepp9VbYX5myNHkDinqLPHoNs44NUW1Boxapq7D0o3MLk7Gx5FCNj8c/WODcJ1tUQmx2lhz54mzJkaBDequHuOwqPHK80lIiC0sOQ/0+Yit6Nxtgr/49ccVJewj9q11VQu32jCAIgiAIogpwzt8FsEbafD6AR73PjwL4grD9MV7iAwB9GGM7ATgLwDDO+RrO+VoAwxBVnFQM1by1oBCyGVg48KhQQJ782gpveSD2JVByZFR3XrEsIqbuiAZ+rWSK1c5KJLuK4zWrrruKvq/tFoFHdY/c2Hny6yobVNeqvaOIpevTZ8nhoeuSPXHDQvU8xpW3dVcxBh6FWrmcl7tKOU2ybXn9dsbCz5858Gh132Wk5CAIgiABHGe+AAAgAElEQVQIgohnAOd8KQB4f3fwtu8CYKFQbpG3Tbc9AmPsCsbYOMbYuJUrKxfI0ya7yoQFazFj2cZSealotZQccnaVlvaScJjOXSVsTp4Hcv9UPvtZtr2lqktkdyrX81S5YKQhTmciGmiY7q9dTA51BT94aqLFse6ohNjrnpuCY3//FprbotlrXNjQku74OOKUX65COFccYxNcNDg46JfwLg1tz9eSI215/9zF58/Uv8UZpItOAyk5CILIlhqOtEwQBJEDuth81jH7OOd/45wfyTk/sn///pl2zkfrriJtYyycNnLEjBViP/1PpeOrZHUgxxLZ3Jatu4qiQQDZu2OqfPyraR3TWYjcB8dLVquBR22yq2gtBxz7ZItKyTFnZROA8nOXlBuenxpfKAXmmBzh/aqgojZoFQJS/XI5VXyevCw5Ji9aj9tfn2GdXcXkZsPAQs+fqX8/fWayW0czhpQcBEFkC5naEgSxZbLcc0OB99eX/hcBGCiU2xXAEsP2qqA2j1b4qyA84W0XZt5RS4Qqvu+FpluC7CppLDlKf9+asRzn3jtS11QqVKbuWaeQ3RpxVbZlHXg0Ls1nOIWsvpzqcfrN+QdIZfQCaB6Yzq0W9XE7b9st+Jx1TA4Xd5Uij4vJ4X0WtyvKNmYUePTPI2Zbj5E4RZqtu0q1qYqSgzE2jzE2hTH2EWNsnLcts6jljLEjvPpnecfW7h0gCIIgCKIz8CIAf65xGYAXhO2XevOVYwCs99xZXgdwJmOsrzenOdPbVhVU89YiV9kSlNxBfDqE1WXZvaK6MTnKn7MIPOpz26ufRLZlUW+hyPHJ8o2x9WabXUXt+9/Zkaf1HY4XLU4pkTUhJYejyszWNWfZhhbXbllhzp5Re4OrV7fG4HPcsOCWGXv9W+CSXUWOtyN+/mT5Rjw3cXH0eEVd1VAi6MZoocixsbU9NCZJyaHmVM75oZzzI73vWUYtf8Ar6x9XtUBfBEEQROehBudsRBVgjD0FYDSAfRljixhj3wZwK4AzGGMzAZzhfQeAVwDMATALwN8BXAUAnPM1AG4GMNb79xtvW1Vwya5SF1JylAv4K+b+MVVzV0F4Iu4LuekUBJ5LimHOnkY2fnTUPEWLHLLYWouCY9Ycsuu2qY5PHXjUUTDboVdXfP/0va37IxNyV3G8vXLMg0oPD1P8kqN/NxxPj1uo3e9CVuNezgylbQ/R9Nkm1xMdWmsRyUhO/PyT/0zCgjXN3vFiXdFqsojJUe6D/kREJxTTe/SVKctC7ipZWZrkQUO1OyBwPoBTvM+PAngbwM8hRC0H8AFjzI9afgq8qOUAwBgbBmAIY+xtAL0556O97Y+hFAH91YqdCUFszZDhFEEQnRzO+UWaXacrynIAV2vqeRjAwxl2LTG22VWAcEyO9pCpfe24q4T83YN0jOndVdT70p/nknXRIHxFpbtK6qZqnm26uosf3zhuEB7xFEWMAc9NXBTsc7Uocg08yhiwa5/u+vpc3FWcWo72tdLuTHFC9l/fmZ1ZW1kr+OKuVVgJoQ7CHCrPEXlgtYoRhN+PercWs5VPlkFy7d1VzAVFS6pKW0W5UC31CwfwBmNsPGPsCm9bVlHLd/E+y9sjVCuaOUEQBEHoaGkvYF1zW7W7QWxxRCeuxWJ0NZOxcArZguCjMnbe2lCwwUIVBXJxIu6fQ17yX9pq1za3K/X/KmGiHF8kPbLFS62QxMT9/x2yU/CZgeGH/54UfHdWciSRy1JY+IQsnmo83W0128/i+RUFcNOwGDtvrbXCKGQdIu3Tp5DlxsCj8vGMqbMt1bGE4zUFcZdFVHxlaWmSNdVSchzPOT8cJVeUqxljJxnKukYtr6lo5gRBEAThwufveQ+H/mZYtbtBbGGoJvxFi5gN7ZIm47bXylH6q2bJIVlA+MJBqhSypn2+IJIiv8rcVU2RbR1Fjh/866PQtqXrq5t2sRIkUXKIwqssd7ved1fBXc4oEd1vJs1zIl+ryrurVK6tLE5NvFqmcfHkhwusx43p9unqKGqs5EzHq8ozMDRY3IQT9+4XW8bUH5MiR2b7nl2DzxSTQ4JzvsT7uwLAcyjF1Mgqavki77O8fYviwzmrMejaoRg/f221u0JsRWwFrsIEUXX89HwEkSVKdxVFClkgbCLdIaW1XNNUtjKqZuBR8XwKnKOlvYBx89zmRDv0Kk/Wyyuq0Ul7Fmfp++CLzF3ZhDbp+i5el20AyVr83U5iHSBaF8lHW2ReDdeVwF3FlMMgzmTfZkVfhyxEVtpdJU6Izao3jLFMzi10K2KqE5szpZANMqGolMKGY3SBR0WKFmVsFAkH28S5sby+ce4q4nuzoZ6UHAGMsW0YY738zyhFG5+KjKKWe/s2MsaO8bKqXCrUtcXw7sySe83o2auq3BNia8QYmK1y3SAIgiAsUU1ci4qciAxh82g5c0VrRwFvzSitQ1Ur8CgQFQ5uemk6fv3iNKc6jti9b/A5b7cOlSDcpSE6DV+RZZaMGlRwAMniDIiHyAqHvC05gJi5TZy7imUKWRVVt+SokLtKyb0jWyWHawpZHf67QVVcH5ODh96dunLLN7QGdeu6Y6PksLEys9VJx5XboTdZcugYAGAkY2wSgDEAhnLOX0O2UcuvBPCgd8xsUNBRohNw44vT8PTYbKJUE9lSLHL884P5aO3IzleaIFxp7Sjgrjc+ydRnn9h6UM1btZYcgqQgu6uIlkbVclfhCJ9PkQOfSulZbZDPDUgnzJpQrXiq5MdNrR3JG7GgR5f6XOu3IZmSQW/J4ToOTYKZOnZKusUdm9V6HdEUspV95ioZcyELBY6YDSo2hay0X3dtwyFVuLRPf0zICi5mDMntd2ssieiMZadIyMryrreQpreWY3JUPLsK53wOgEMU21cjo6jlnPNxAA5M3VlCyYxlG7DvgF5G0z3CHT9q+JePGmguSFScFyYtxg3PT8WKDS348Zn7Vrs7xFbKI+/Pwz1vzUKXhjpcc5o+nSFBqFCtWqqye5RM88vfZXeVUFDSGrHkKHKeaLLdIUT6yzu7ikqwV92T5rZslZhyC7Uwc0sWk0P9GXAfhyZLEoboNStybhRQbbKr1NcxrVLRhNzVSusV4+7V8vXZWB7NXtmEPw2fmboe8flxteSIy4AiK1dL+/T1i0pUDo4u9XVo7dD7Vol1iUo9m3ebjUhmuh4uIp04JuorGbTFkdrtGVGTjJ+/BkPufg8Pvje32l0hiIqxsaW0srauub3KPSG2ZvzJUUu7owM6QQBKU45S4FGVNYOYXSW8v11QetRKTI5ikStdP+IQV33FLAd5oBKsOxSWJE0aSw5/ZdcFDuDfkoVoLSxQJXFXCSs5JHcVV0sOR4VFkZvdAeIuaZHzQDB01ZdFLDkqrFiMU+A0ZaiUe+Dt9OloRaVs3JUKWWgYSpdjckT36QOP8oibUtw7Sry3/iNSi5YcjfXl86hlSw5SchBOLFxTivo9ZfH6Kvekc9PWUaxeVPqcqYH5E0FskdCjRaTBzV2l/Lldyms4R8gSUi0lB5eUM0UennjbErLksFhjT/MM2lpyNLWplRz7Dujl1J4fiPAv76QXHLMmSaxCk7Dtasnh6q4CmC054igUy5ZG6QOPJu9HEmo5sKSKUJDXmHEhW1XoSofdjcL7tDE5ePTd2cXwjuII910c71aWHIpt23Spxz4Degbfsxo7YUuO2h0fpOQgnCABNhv2uf5VXPPUhGp3gyCITkilfbKJLQOlu0oxmi2gFHhUzK6iH2/VtOQQm07srqKKyaGJyZAWlZAuB3UFgObW6Mr4P799NG77n4Od2tt52+4VX/W3JVng0eyyq5isWVT74oZ5vLsKBEuOdK41lc6qWKnAo1khvudU7ycxJk2zpFDUxtcI/kZ/ffXH8IilmMmSY9LCdaE00+JlT/K8lPogxygxjT27Ng7aZdvQu5YsOYgtjtr82excvDJlWbW7QBCERC2/2/xJT43KLUSNoxo3S9a3KAN2hpQcBgmv0uksRURxo8iBxgTuKu2WWS+yUCyqXNdVFp2bFJYcJ+7dH9t0cQujV8Ou8glTyOr3ZZldRdVOKSZHcmEuqRIOqL4QWcsr9SrCys/ofvFsmgSFok1MHs6Bm1+eLu3THSO7zri51PnjjYHZjQHN+GzL2L3wvosPC1n31PL4qOFXIFHL1OrqAFF9bNJYEQThTi340hOdF92v9r8UWb2YMDs0/d5XzV0FgGgJXixyNCay5IiaAKh+w7KI16GqV6VAymp6Vcvvi2SBR8vHRII/ZpldRTOHSXM1/cCjSai2JUUShVQ1iZNPxOspWnKogoqW69TXp1OwcYSf79enLXe6lmLRpGOH83AMpSyU0nWMoUHQoJKSg9hiqOUfTYIgiK0BUjETSbCd4DLGQoKASX40eLLkTtiSgyeLyaEIPKpuKz0qaxAXJZHr9KuOMWW/xVToO2/bzVjHtCX5xF9L5q5S/iwLsllmV1HtYohJIRvnrpIi8Gi1hcikrhLVIu49J94qMWgqN2g5/OdUtVv3CHPOo+5wDpcyHJMj/t2mqlp2mTEpA23fL3IgVNX4vPrUPe0qyxlSchCJoEk2QRBEdSBDOiIRDuNGnLaaBPFqBtAOZVdJ6K7yicJVJ6+1HNVz66bkcOuYTjYV01rG1XnpQ2Oc2rQliQtGyJJDumyuFkWmWJq6a2KyUo07nWKRB4Kqq+tTtZUc1XaXcSXu6or3t7lVtOQwueX5hezvnSrwKFAOCBwHEz6kseQQFYBZpPwuWXKU+9OtsT5SplYsuknJQTgRDFuaZBOENb9+cRqeGb+o2t0gOjk6eeTKx8fjh//+qLKdITodtsIVY+GxZlJkVC+7iqzk0LurnHPwTnZ1xqSQbC8UU6VvVskXpngn3z05vBrqKjbYlI9bIOYA/vnBfMeWLdpNGZMj4q6SoSWHqmuMMaPyKz7waNmS491PV1n10afaSo7OZskR904yWXLojpy4sBTstd3hfce5ui/nHGT3PhKVMTZjQDcExT5k8bqur2OhMZHEgq5S1G7PiJqEvFWIOGiMqPnJfyZVuwvEFoIsjL06dRmem7i4Sr0hOguKRUUt4kqcajUyqLOKZkVi+tdiUe+uYrsSHcTdUO0DxzVPpsuINmbemsg2kwKpq2SZkuS3Vbw9u/TpHtkvC+fypeKc44bnp7o3HEOimBzCnZHHnWt2FXPg0eg+BrPSKNaSQwg8OnLWKifriErFxNi2e2NV28+KuFdSncaSo3Rs+eBbv3RQ8HnOylLWk81t0cxH2n4g6q7CYL9G7HeTIYUlB8LvGKO7imO/fFRdE+co1YzhSEoOIhGUwpAgCKKyBJN8ev0SCXAZNuJEtt0QeGPGsqi7R6WQVyh17ipZrIRzXgocmDUmS46oAsLtPGS3C9V1kAVY+Zi8XjVJbonYtfmrm0P7XN2mlm1o0e5TxuRgZiVTnNtPa0cxlFnDZUxWKkvO+YfurNxebUsSV2Jjcgifw5YcPPQ87tC7a+RYOeWsCTnoJwDMXtlk7fEiXnYrSw6NmkI8pyws72zeS7IrYbUgJUcnp9IKMv8hIp/wzktTawdmrdhU7W5ssTwzfhGem0iuKUT2dLIFNaLGsF1Rk4MsVsslRSRiYQAeUr4UOdcKzrYr0UFtFXzQTAKZ3I20vVJaD8RUmvbe9+8VFRSTYnKbmLu6KbN29AqL5DE52gvFkGWOiyWHTdDJLNAp0aqd3cWVuOc9FJNDUlqI4101DpqdLDk02ZMsVYfieSS2puE5uKtYKDnEdmRFTyUhJUcno6NQxC0vT8fqTW1Vad8fy6Tk6Lx885Gx+Nxd74S2TVq4Ds9SzIhM+Ml/JuGH/ybXFCI/6PVLJMFlgtu1oR6fHbxdfp1xROWKIk7eOefa82swRZkUSJIq92dD9rWqW4fZkkPakMRdRXhbqFaDZQFFVrqkDSyru6S+APmVIwdij37bWNVl0guMmRt1BTJhWhlX70oXk6O9wEOWHC5xLqod8sD2+akFLj9hMP500aHGMqGYHK3hmBzi86hSLLi4qxQlyxCxHRsavBvPGLOy5tHG5OBhZbDr8TLyWFcdJ753qqkkJyVHJ+PdmSvx4Mi5yrz2laDzvOoIHarJwPl/fh8/zihmBI0RgsgH/9mqpo8r0fnY3FbA3FVNsFaPebPWn5yVToDPki4KVxQxVkiBc+0E3jYriX+0qrRuon7sHtvjpymuk0kAkPstm6N/Zqfexrrl82hQSMuyICdfQpMSJgsaGxj69FDHgpDJ0qLAVJdqvDBmntvEda2to4iuDeUsFC5nUilLCl0zncmS4/pz98du25mVZqJ+SbTk4Ag/jypF2OZ2eyUHeGlhOrLZ8vdbtPZJmq2Eg4ee6SwUDkx6jSiHh9BMJI1uBSElRyfDJXCYz6hZqzBnZbbuCRSTo3PS1lE9szGCINLRieaaRA1x1RPjceodbzsHZ8xDuLFNnyjTRRLQOQcKIXcVvdWB61moTtukQPnaZ3dzbKGMWwrZ8Pd9BvR0aktlERB3i1tzmjP4zbq4YmQ5HI1KDs02k7IsTpHWXgjH5HC575VyV9EJ0tW2JHElzhVIvPeiK4UcQ0N1S12ehwKPBh7127FBtKCJGwIXHrGr1XsuqxSypu9A+H3ZkURwzYhONnSJJC/5ix/8EKfd+U58QYf2aSGxc9IWeqHTTSSIzgg9uoQLI2eVUlY2tdoHzQPc4gbYkjSIocpdJZRdxeCuYjtvMmVXWb+5XXlMHbO3FFFhEnYjZuHSfptWxXeFyvw+71X6uHlGHWPWS2a67B9JMI1t3TUxXSlddf75t3UUQ4q6Jge3h0oFHtUNhWplV/ncZwbgocuOdD4u7h0j7m0TlBAfzl2NjS3ld6RqHJgWCrfpUh/63lHkqQT8eu/GM01ffI7dY3vcfuEhyn1y0GjT82hrLRINVhwtIzaTtzWYCVJydDKqv5JX9Q4QKRDvHglKBBGllp+LIPBzlftBdC584WpDi1pQl/F/J7LOqvCVIwcmPlblrtIRismhVxjYm3rrn6zHRs9Xbq9j5lgNcZhiSURTNTo2JBVXxuTIWQrQrhx7Xamvs3vnjvz5qSF3j7SYrJHVljzm+XfcvWmTLDlcyOI5HLR9j9gy2nCrGQkelx27u1P5Qdv3wOmfGeDcTqySQzifdz9dGXweNXt1qJzqnrZ22CunCgVNTA7L4xtFSw7DPfDHsk29mbirWLyXxFZIyUEEzF/dhCNvGYbF6zYr98s/1nNWNWH8/LWV6FoImmR3TsR3UV73sPqKOILYMqFni0hC18aScCiuUtqQZcDBrx41ELf9z8GJj5f7wnnY17tQ5NpVSsbsUpYGlhyOD1oaa4jpSzcY6g1/NzWjs0wQL4nK7SHvVfo4oaquzs6SIytB28fUrU0KiyfOeUwKWfV2//xLMTkSKjkUlbsEBb7h3P3x7JXHxZbL+/dlh97dnMq7BGcVibNAsz1PlZuOyV1FHlIdRa60/LBdSPGVNYyZFTd+fTb1ZqFwiLqrRMuE3FUouwrh86+xC7FqUxuen7hYXUAaTM9NXIwLHhiVf8f85r32R8xYUbE2iewQlWTkrkIQnRN6dAkXfEsOWcmhiyXh/85nKQD36taQ6ng5JgcQnrAb3VWgVkTs6Ch0KeuOCUjpwls/PhmXnzA4+C6fs8kiRScEiebyO24bPd+srXVk4qxrbF2isu6laf6jUgYWOTdef52i6/Cbh2HUrFWRmBwuqO6RTV2/+Px+eOX7J+LbJwzG9j3jU/nqFElZKj9+98WDrMsmbTfeksO2fYUlR7tDTI5iUZk+1ZThRKRRUEqausyDv/H1mpq2z64iH1fesEuf7pF2yJKDUDJl0frItmov5PntV3PQEsmphCXHlgwF3CWqwfuzVuHlyUuC7zQOCRd8gWhTa9ldpWtDHbbpalY8ZCkAqzJ7uKDqS4cUNFDnGsEYUwqhcvYYU3YVHQzqukXOPnBHq7o6iuEnu0cX6f44WnIwsJCSYac+3TDy56fizP1LLgC7bdcjl6CWYkDUOEuOesastLZZxw5xncIWOYzXX6cg2NDSgdvf+CSSXcUF1di3qeuKk/bE/jubM/CIVEK+uNghSG/SjCJxCiDbseTqrqLKTCTHxADs597ifTdaMjlYciRxV7nkmN1xiBAwWh6P4jfV64SyqxBKvvzX0ZFtWZvsEbXL7a/PwEMj5+ZWf2dcDeac4763ZmL1ptaKtktPXXW57r+TcfWTE6rdjarxtQc/xDVPTqT3P5EIf9Ivr1DHWfNlKQD7QnjSEdytMSzUcc4jlhzGoHqKhmXFQNLfxLjH0jYVb0chnO6xuxTI0BgTQmvJEY5bsmvfcmyGX3z+M7lYcjx/9fHBZ9Wq9c+H7Bcoau3dVbLqnb5fpnZLlhx6TJeRc6ClI7klh+reJnV90fHTs/bVjiEGhuvP+UzqNuasbHIqr+qOzTjo1a0Rf73kCEO9tkqO6Dan7CpFrrTksH3R+C56DHYZbqxiciR4yW3fswsuP3GP4Ls/Dxn2w5Pw7k9PVcfkoOwqRBKqPcWlSXbl+POI2bj55emZ1hm25MhHy5FU+27DuPlrcccbn+Knz0zOrQ2i9nhqzEIMnby02t2oGTqjgpKoHr4wL8YaMMYX8N7hWS7y+woTWwHjUiFI4QfXnR4R6lo6ioElRx0rCRSmubSqWVnA9yfmsnLBBGPxgpet2498vXvISo5I2+UtKkuOze0dWNdctt7xV3F9Ab+OZRt3xUe0QFFZ/V55yp6BRFbPmNX7LOte2i5o/+MbRwEovXNN81/TuP5o4ToUihwrNrQ49dFHdW8bM75vg/tto3UdYgwhITcpugxFOlTXVH6W+mnccM46QG89ZXvlXLOryHRolBz2gUfLLwS/L747SLg+e1MOc3YVNV0b6pXvuL0H9MJu2/cIXSf/t0N8vsiSgwiIe/iqrWMgFceWw6aWDgy6dihe+EgT/yUHVm5sxYQFyQPl+j8YzW1uAfRqheEfL692F4hODL1/iST4k9D3Zq4KttkoozO15PCEMlvLAVGpoYol0dTaEQjQXRrqUDS6q6jPVyfUuVo3xF1LG8XOV44ciH0H9DIeZxKy6xX36pUpy0LffSWH/7e+juWSJlhEm1yF+Yo0y5gcKSe/Xzp8F6lfdoJX/14lIbpQjFpyiNfO5jQ+mLM6vhCiCgzV+Enr/iVz9oE75p5O2NZ6xkfVHbmPr/3fiXjpmhOc6rXthep6uLjqF4pcafkhXob/OWJX7fGB9RtjwXOieiW7BExu63BXOHRrNI81ZQpZiJYcpOTYqhh07VBc998piY7Nc5Wc2PIR47wsXFvK4HP/iNmxx42ZuwajZq+KLRfHufe+hy/dnz5Qbmddyf72o+PwtQc/wDtC2jKCsKXaSm6ic6KaGNsIZZnG5PDqarQUzuSYA/I7v7WjGAgQjfV14JxrhahS3IzodlnA9o929VuPu0w2z+0lx+4eEVJk/39TNTbKCv/6+AurdXVMqRypBH5/i0VuZVWa9t0nC6y2cwh/vBYV2VXE8WMjYN7yxQOt2pSfO9VzmHVWHF3cGiA75brrc6W6pvJw7dezKw4S4kXYYKvgSvtojJ+/VhnEtl/PLsFnk9uRaGXlfzLddxvrns3t5f6Muva02PKAZ8lhGAVil8ruXeVtlF1lK+SpMQuM+02p0KpJtdsn1Nj+ePzPX8pxXlyyq3z5r6Nx8d8/tCtsGCPLN6SLpbElKPnen7UaVz+x9caXSMN9b83c6rICDbn73ci2re0aEOlQCS/M4CYQZFfJUMnh12XrHiFP/lUKjA0t7UHZIuco6jJ5MM1quCYmh+vjFSfg2lgrqIMchoUDuYhoOm9zr/wVVf861bNklhzdG5MF0BTx+9tR5BVxV5FP09aqwL8+pbijkvKBRYVQE58dvL1Vm42SdK0SbPPQTWVsHBLB1ZJDGZNDuNJfPlJvBWEijSWHuV67mi85dhCOGtQ3tpxoSef3RfUu8VuVFZYvf+8EPPato0PbmlrLitPe3RtD+3TvsThLDnVMjvLnJMFOs4KUHDVG3DNVbRFP7N/4+WvxcI6BMQk7nh2/CHv+4hUsXNPsdFxnFpMq2fd5q5qwQaGNT0O1n+NaYGNLu/OYveONTzF96YacelQdxDgJKmYs2xh89sdNZ352icqjmrzavIPysOSwdYGpt1CGvDm95P7XWF+HQlEfZ4EF/0ltaM7PZVJeUqCYy9hcRv+yiIJSXJDDjcK7w+ZeFRXuKjbXOQ/88WB7rdO6UsjHW6fx9C05ilFLDvGa2/RPFXhUjrsCRBWBKsE2rr0LDW4QOrSBRzMaIi7xLAD1opbYxbMP3ClRP2x1LXl4cl109EDU1zGcd8jOsWX98cWEz6Ygn7Ilx4G7bIuT9ukf2ia6eqvOT2UN0rWh3hiwVOxTOe11uXw7KTkIa6ptySF04IIHRuE3GQfGJNx5cVIpteSslZucjuuMi8HVsCQ65Y63cfvrn2RaJ1lEAeff9z5O/MMI5+OqGKg7Ne/PWoUl6zYH358ZvwgH/vp1zFy+0XBUGQr8TCRBafmjGEpHD9outCvLeA313uTXJWDiFw7dGT8bUspMovq5mu1la2ioZ2Z3FY0lh6wYKHKOe4fPxJqmNus+luo3n5ONa4EuyOFRg/ria5q0m5tayoEcbe5VNPBo2JKjvo5ZrTBn8RryBer2YtHOkiNlm/I9SuSuIu0TL7mN7k6+R9eevV/IZcC3XrKJt2FSchywc2/cfuEh8R2yrDOr3x2XzCSAWggX+5hUCWur4MojRskQTzETpKs2NCHuCqzrVEoO76/NO0C05FCd36s/ODGyrVtjHdoN906spqtn9RG25CB3FUKBavBvCeb6Wyot7QUcecubwepS7dMJtRw+Vez6xAVr8ZP/TIoIDuua2zDo2qFWddSKsMoNJt55M2eVWzo5n6Fm5awAACAASURBVDiT0GKRY+ri9cYyLvUff+tbieuS+dqDH+Iswf1kxIwVAMLWGja0F4q4/vkpiaP1E+lgjM1jjE1hjH3EGBvnbduOMTaMMTbT+9vX284YY/cwxmYxxiYzxg6vdH/bFdHtVW+gxob4WABJCSw5HGzi7/7qYbjqlL1KX7xTOHmf/thWMrOuY6zkrqJVcmhickjv4fdmrsKdwz7FJ5ZKR8BuTmbzvtelq/zPd4/Db794kPIYMQuMzb3ayQvgeqSnyNhx226BZc13TtoDH1x3esWCBAaWHAU7I3/Tde7ZtUG7z0e+PPJQ+f7peyuP860qVJdFHMs291gs8/r/nYTvnLQH+vQox2fwA+x2sXhGthfiOmRF1nE+ZOKUHBNvOANDhKwoSgsWb9uFR+yKE/ful20HJVznaUncrozvD2GXf2+U7ip+piLNuNmz/zbBZ9GSQ3V6e+3QC7ddEH7fdG2oV6fCVdTTzYulJL6LVb8/lYKUHDVKnK9s1ah2+zXM4nWbsWpTK373ysfV6YBizLR2FHD981Owrjm6MpWXJUeeQySrul3dJEQufXgMnhm/KOLC8omDoJpzQHtrrnpiAvb4xSu5t7NgdTOO+/1wLF2/Ob5wSh54ZzbOvXdk4iw+8nOxeF22fRYDkZWDdKkfxjmSdZZffvjHK/D4Bwvw6xenZdo3wolTOeeHcs6P9L5fC2A453xvAMO97wBwNoC9vX9XAHig0h1tkwJYAurJsuxKkktMDttsGtLb3heFrzplTzzw9bCeqKTkMFt52QQ8feKD+cb9++3Yy7hfh80pqwQqOfBo14b60PU74zMDgs9x9+qI3fvie54g/6Mz9sXwH58cShs6qN826N+ra8WU3n5/C5xrYwydum/Z1J4Zbp9oHfRbTXBP+fIWOcdBu5QDVvbr2QWD+22Dcw8Ou0CExo1Uh6gkc13133fHXpF77hK35qpT9sLNX7ALZGpL3vKFPJ5lVMFdZfxrdNFndwtdv798/XA8891jrfphO/fN83LYKURY0BH/VFWPuV9Vo+Yd8MI1J+BOz7JnrZBSWh6zG7wUv185Kmw51rWxzqjkEOvp5sXrERValEKWCMgiFVme1IhsRgiYhsQLE5fg8Q8W4LbXZkT2dWI7DusATyqen7gYJ/5hBEbNcs8WM391c/AMyhMz21R4QPWfY59Xpy6LL5QBT4yZjyXrW/DcxPTpiuMmB9OWlKw4liRUTiQ9Lgn+hE13Tqfd+U7ouz9qfLPzzuhytgVzPoBHvc+PAviCsP0xXuIDAH0YY8mcyRPS3BYVLhiivwG+QGcyjU6KL4haKzk0K++MMWzTpSFStmDMrmK32r9RER/nH988Ci9eczw+uWUIbkkoVNq871VlIgEo6xim3nSW8vg4wfi0/XYI7m99HcOe/XuGjvMtOEy+91niGpPDdHaiRcXnNXEaVDE5Xrj6eFwsuAKN+MkpuO/isAKtMZThQlZKiPUbOmiJf01EhdXFGlelLg11uOSY3dM3KpD3vKS13WzJIY8FVX/8SyMP0yEH7oQjPXe7OHbp0z2T4LkAcP05nwk+f+O4QTh8tz7G8izoPw99VyGOKT9o/3bbRC14encrvdt0VnI9uzZgl77dI9sb6+tw+QmD8dnBpeu2dL3aMrRbQz3aDIoK8RT8IKXib04HuasQMncO+xQt7YqJScw76B/vz8V+N7yq3PfF+9/PomtbHNf9dzJ+9cLUILZFJXlm/CLnY257bYbWhH7o5KV4XLEapTIXCyLJKxQG65rb8MDbs52yOBSLHN97aiImLlxnfYwrceaDnHPlcyPykde/jx1dBICSObP/wyPPzVwmCKtjfL7veuMTnPXHaFaNzoo/OazE/DmtS98TH5ozX2WJP2ZsfYRveKFkuWHjz0vkCgfwBmNsPGPsCm/bAM75UgDw/u7gbd8FwELh2EXetoqxfnN7ZJv8Lv36Mbuhi+Su4qK4jcOP/O9PxP/3xMFOx4tjfpuuYQGlnpljcoABPbqqhZovHbYLvnSY/nbstl0PHLxrn5IVhYU1yMFCOsubzjsAT/7vZ61+G2SFUt8ejbj0uKgQq6urR6NZiaNbiS27jZT2q1Zdv3PSHiELh6SjYp8BPYPPvmm9yT1G3GP67RdXsHXXJ6rk8FLoMv1v0++/dFDIuikSeDSFJYcKvy1RuaVbnc8DnTWQeGrH7rE9brvgIAzcLio0x/HdU/YEADz9HbXFRYcU3NXvzlP/e4zQl/TXo3uXenx885DYcja/yv4127F3N1x79n741xVma5JgLhR8N5QNDDlYMK/9zE69I+UGbtcDgDnekW58Xn/u/vjB50oWXusUvxOAmyVHV095tJ3nhvXN4wfhzP13VB5XCUjJUQMUihz/GbcwosUUvy5b34LXpi6L/XG56aXpaGkvKoXTiQvSC5/ViCUwfv4a3PVGtoEfRZ4asxCPjZ6P7z81MZP6XOS4n/xnknP9D7w9W2tCf/WTE3D981OD7/4kVTX588fIp8ujAUt/8dwU3PbaDIyevdq6X6s2teKlSUvwpCAk6lZp0qbA1B3+9LiF2O+G13DefSPx8mS10qq8ep6sD/4LPbrqkKg6Jfe8NcvJL7zWyfK1UU3jhZcyVoSqcsrbULRYBSJy5XjO+eEouaJczRg7yVBWdZcid5wxdgVjbBxjbNzKlSuz6ifaC0WtJYfITecdWLbkyMFm0xfW/In4Ln26409fPdS5HgZgG8kqo44xFI3ZVaLWHz53feVQXKRZLffb81EF0JOfwV//v/2Dz6fttwOO27Of0dVCrsf/WbrmtL3RtSGqmNH9zsiKHxmdkOIrnzqkzCsAMPaXn8P/njgYPz1r34iFQxJe+t4JmOZZoogxOY7fKz62gun3VYxNohu6Ucug8DtUNR+46OjdJEsOqU9Cp7J4F/tWNSFBv4JKDl1bvtXD1JvOwqPfOhpfOWo3vPez05RlTfiWJ0cPjlpcfP/0vYOYMUF/vAtx7J7bY2dvX7mLyWcCtvfKZvGhITQGWCSDzrkH74Q9hJgYctsmmUp8D99x4SG46bwDMGj7bSLlTvEyqJgyV5msKXp1LcU42tiiVnJ0a6hHh4WSo46VY3IcMagvnr3yWNxwzv7h57PCkJKjirw5fTlWbmzFP0fPw0+fmYwnP5yvffi+8rfR+O7j460fa92P/bL1LZi4YC1WbmxN0mXrqc8+17+KQdcOjfXBs+GCB0bjnrdmpa5nfXM7FqxOHoshjlqUNwKrA8WAMI0lP26AU+onxQXYrLGqkH87OOdYtSl+TPoxClZtasWHc0oKmIVrmjHRi7/wypSS+8XkRetxzZMTMWr2qohg6l+TpLm76yQlCeccz09c7JwD3qQZN7Fk3WY8PW5hfMEaZLkmUOYnyzYmvh468rAa+Z5BEfrge3Nw3X+nONXnasnh4z/PFIi6OnDOl3h/VwB4DsDRAJb7bije3xVe8UUABgqH7wogoi3jnP+Nc34k5/zI/v37y7sTo7Li2GdAT9xz0WGhZ6S+jgVKDtG6T8zssdcO5ZV4ExcesSt6SYqIckyOslB9/qF6CwrZrUUUQntICotuXerR1NaBkTNXYdD2PXDt2fuF9tcxoFc3vaWDeTW1vHe37UoCxm/OPyAwERcZcsCOofK+0Cqu+D/wNbWyQBYwdX3SCUUqhcilx5YtQXR+8X4QV184838X//TVQ9G/V1f88pz9rYPF/vSsfUOWLKo++goqMSbHdWfvh0Hb94iUD1lRGO5Sr27lQLSynP7slcfh5vMPiKxk26ykA+Xr/Y3jBkWufUjAlWpyySIk1xd6Liuoyda15d+znl0bQkL8Ebv3xQmCgqpXtwb84YKDrdp68Zrj8atzywrBH52xDxhjoesodse/JGV3YatmAm790kE4c/9SDJu4K3qKFwumW4xLy6PfOhp9PfeRZZq5zY3nHYB+23QNvvttu2YUGrhdD1x23KCIjPjvK47BmV6wVpPLmphVRcZ/N+rS2XdpqAvcVc4/NJr61lfi1jEWuKu0thdwxO7bVVRJp4KUHFXk8sfG4ZKHPsQyz9dKDmQosnhtaeXeNiiUTmv3zUfG4ov3j8JRv33Tsbdu+Pmwmw0PVqX5/D3v4aTb3VNWqli+oQU/e2aSUokzN2HWiCQ0t3VgtiF1bHkiEd1n85J1sXZQTUIKmomVLNQ9NWYhjrzlTXy8dIOxjWs9IXLe6mZ85W8fAABO/MMIfPH+UcryF//9w4hgWidMrpLgn6X/KL46dRn+798f4YG3ZzvVkzRA7cV//wA/e2YymjQ/SLXA0+MW4sH35gTa/2lLSvf1sdFRV6qFa5px1t3v4rdD7a6HaUzOWrERQ6csDW1raS9gXgWeyVuGfoynxti7uqxpasMyzwfW1aqI287QicxhjG3DGOvlfwZwJoCpAF4EcJlX7DIAL3ifXwRwqZdl5RgA6323lkqgUnI8cfkxytVzXzAT3Rv9SfX3TtsLL1x9vLad75+2V/D59gsPweQbz8ShA8v+6f4EXI4BoeLyEwbj61K8AV+Ir6tj6CGtDO7YuysmLliHtkIR81Y3R+IylFxcDEoO7xxVJvjib1X/Xl0x79ZzcOmxg7BD79LKsv8ITr3pLNx38WGhR9IX0kUB++yDdDEjtN3TlhOvYE+F0uU355djiHz35D2V9X3n5D3wsyH74qKjS8os/3fxkF3NsQVUXH3qXnjxmhOsyooxORrq67Br36iSQ1R8mGR9efVc5Ijd++KSYwdFrm/ZGi7sPqBi3q3n4MbzDoi6q3iV3nbBQZH6v6BR4O0zoCd+c/4Byn3+eBHHXJbBf3Wcvl/Js07XlPy8+Tx75XF4/PLPBt+7NtRH+nvSPmqF7cG79sG3Thhs7JeoVPIvSZ3F/VLx1aN3w5ePHBipV8UN5+6PebeeE6uoOnmf/vjs4O0BQKvc69JQF5prbtujpJCz6X/griJ0Q74X++9cdl8xxTsyzRf9d8cmhQx6/TmfwYDeXYNFqH0GRIMv+63W1TFl4NFqQkqOnHhj2jKMn78mttyMZRsDhYDJp881KJTOMslmtdyE6d2wWWESW0v4Lh73Dp/plAlDxU0vTcPT4xZh+Mcr4gtLzFqhV0qoGDlTHyDzqicmYOEafaBEZlgpNgXvTOKWpHq/6sarvPV9LwioSWGjwvVaAuXViqRR5Ms/sqXj13qZa1ZuCsfZiBOsfcHf59PlG2PjiQAIrLDSGipkHUV/U2sHPlm2EdOXbMDPnpmMW4Z+jIffnwsAMM0V/juhFIz0kVHzrNoZZXCh+vaj44LP/tld8+QEnHLH24ktd/KAc45jfjccI71x79o1P0AiA/DKlIrJy0SJAQBGMsYmARgDYCjn/DUAtwI4gzE2E8AZ3ncAeAXAHACzAPwdwFWV7OzmtgK2VwSrA6JCvR+cU1zR85XX/Xp2NSoK9pEyjzDGQs+cL8D5E3GT+fP15+4fWUW9+6uH4nun7YXDBvaJZErZadvuoT77q4niOZgsOfyu9OvZNbJPZ2EmKyZ7dm1AQ31d2JKjLup+oMM2poPqt/n0/XbAz4fspyhdYuB23YMVZ5lujfW46pS9gmvq3zOdcH3WAQMycVv+3P4DcMTufYPUrar5iKiIk5uc9Kszg8+iBUKvbo34s8K1JhKTQ7qtLuk/B/fbBqOvOw3Xn7s/+vXsgvMO2SVU/8+G7ItzD4mueAPAGz88GZceOyi07Q8XHIy7v3JocM3F+VqSlXDX9Zs/e9ZFurZsUtoCJXeuZm8O09VTPPXTjDstoquO8Pm0z5QUMf5z7HKON3tKJdu1gSBIsXeA6r3g079XV4z4ySkhZY9I14a64Jn60Rn74ICdS8oQ0V1Kr0yJbt9p2/A7W7RqM1lc9e6uf//511RUPF996p74xef3w+Un7gHGGNo9OVU1Fvz3QT1j+P5pe+NLh++Crx6tdwGsJKTkyIkr/jkeFzww2qrsLE+w++Obn8aWNQWpXS+kBtIJl+Kk41cvTFUK0HLaQhGdyeCoWavwmV+9FonhEPce6igU8cdhn2rNpGwpFHno/E3cOexTfOVv6nsz6NqhVnEyfD/Wq56YECiOVOf6t3dn46Q/jMBNL01DS3sBH8xZjc/d9Y5VP32+/tCHyu2/fG4K3v6k7Lv9zUfGRspMXVzKNFEs8qhQq7k5kxauw7ufrjQVUaIStnSCpfgjvnJja1BOLj5u3hrMX92Etz9RK5Pkaxk39/rrO7PxjnduYlu+gL5wTTMWrTW7NPlmiXExOe55a6axnjFz1+Cv75SsP9ZvbseZf3w3MvbaDNrwoYqYI4OuHYrrn5+iVJYUixyDrh0axLeZHmM1IzNtyfrAWuXxD+Zjxcaweea3/jEWZ939bshFaeXGVsxYtgEL15YVcVc/OSGkzBo7r6wMbmkvxFrz+GPalne9d1x7oYj2QjF0325+eTruGhb/3s2au9+ciTZBeBL75KKAZYzhqicmZNo3wgznfA7n/BDv3wGc899621dzzk/nnO/t/V3jbeec86s553tyzg/inI8zt5AtB+6yLcbfcEbwbvzbJUegf6/SpP0ySdjq4wWLWyf8lsrv1LMOKJl8/+e7x+KtH5+MOy88BF0a6nDsHttH2u4IKTl8S47Sb6cqGLaJAb274cdn7htMqMUsAztKvvzdpNVOxkqBAWV8wdRXVG8vmJX7Qlpc+sOIj73wub7e91WPF1T9Iv696dOj0VA6zMDtegTH+dxz0WEAShlDXr7mROu6/HeRyvR93q3n4K+XHBnZ/uyVx+LBS8Pbbz7/APzl60do2+ndrRHPXnkcBveLxhjwOXW/HYLP8txz2x6N+OlZ+wKIKmTOPjAa6LCpLTzH9OcgX/SCzp6yb7yLmB/bZuc+3bDTtt1x6r47YNz1Z6B7l3qI4RAOG9g3ogR780cna+v98lED8YXDdgmuuXikzgJH5r9XHWdVToWvUDxi977K/Y0GSxmRjgLHZu86+4qBNAox8bm56bwDMPq609Dbc02ycfH0g3Re4r3n/HeQ/3549FtH417vORGRlbmqU7j61PJ9Gdxvm6BfMl3q64K+qsYYA1O6munald9j4tg3Bak9dd8dgne3jKr9n561H644qXyO/rtcpZDxx3p9HcO2PRpx15cPtcpmVQlqoxc5wBgbAuBPAOoBPMg5vzXmkKox21uNbusoxmoYdcqLv707G797pZwmVCdcrhGyOjw2ej4eGz0f0246K3ionxqzQOlX3lEo4sO5esuUMZ6gMmr2Khy7Z3iyUxQiJi9etxkdBY5B3g/bQyPn4k/DZ+LhkXPx1BXH4MBd9P6cD7w9G1eeon7hX//8FDw1ZiHe/NFJ2LVvDzCmfnB9TNf5mfGLcMeFh6BQ5NjzF68AAP7y9SPw+rRluOTY3TFr+aaQ6df9I2bjgJ1748eCgDpvVRMG9dsmuCf/eH8e/vH+PEOrZQZdOxRjf/m52HI2WSD+9u4cACWXiv9338jQvoUKYf7212eEVso3tnTErvhzznHTS9OVK/H+y/2Xz03B69OWCccAG1rasbGlI5QpRpwYjJixIqS4OcgwNmz5/avlZ6Sto4hXpizF2QfuiG88PAbj5q8N9s279Ry8NWM5fvPSdG1da5va0bdHB6YuLgnlSQL7/v7VGfjOyXsGSokx0jN29ZMT8PdLj0SxyNFeLOKaJyeiyZto/fzZKdi2e5dgZc2/do9/sABvfbwCo647Pajn+uenoK8nwNz/9mz86Mx9ce695fHw6fKNgRliocixYE0zBvfbpuQOtaIJB+26Lc6/7310FDnOPXgnXP/8VDwzfhGe98zXl29oCd4BT48txwuZs7IJQ+5+L3ROQycvxdDJZYWYqCz52oMfYvz8tRh//eewfc+u2NxWiCh62jqKeGrMAhw6sE8wiflk2UY8O2FRxFxzxrINwUpse6GI/W54Awftsi1e+l7JpPqhkSVLkx+dsQ86CkXMX+Mes2fQtUOV25etb8GA3l2DSV6zMMn+p5QBaen6shLofYfUxlkHQiW2XL51/GA8NHIuThYm2nV1DG/88KRgDuLHZ1i/OZr9yX+/yELuHv174oIjdlW2edI+/QKl5UGeObc/EU+bVnDCDWcEz54YsPAbxw0Kgt+JyCugAILUif7v+c59yvX07t6IlRtbQ8pIkRvO3R/X/XdKxM1CDPLaGLirRI/fe4eemClYIvoC3RUn7YEde3fTujv4nL7fDtrV7J227YbzPEsCkxJBhS/MmGJByOdzxO7RQJKXSAq0OHTnUl9Xsgjyu/PGD0+KLIrZWDuMnbtWuf2QgX0w79ZzrProL2ipVvavPnUvPDWm/Nsnn49NPBvfmubQgX0wZ2UTnrj8s8EzKaJyzzl8t774wwUH42fPTo5tR8ee/Xtivx17YYakaJdTGetoKxRx7sE746GRc/Hot47GcxMX4fIT9sA1p+2ldcm856LDQpZm4p08WXB1aayvKz3DDjqTF685PiQLnbRPf1xz6l6Bm4xfv+zS3MMivexPz9JbTvl08Sy7/D6ISpsLDt8Vr09bhv89aTAa6xn+6s3Vfb59wuBgXiie8o7bdivFyFAsgB2+e18cv9f2eH/W6lCwU6CkbDpz/x3x+rTlyr7e/IUDcdhAvYvaIQNL7+/9d47Ow/1xO0ShXKw2W6SSgzFWD+DPKJmMLgIwljH2IudcL7HkxIoNLZi5YhP69GjE0nUtkRUHAKFMGao87SKPeObfMqKCAyi9jBev3YzP3/OesrzIefeNxK59e+Dowdvh9tfDWUz+O2ERfvR0WXjfWdF/cZI/bt7a0PfDbx4GoKQtf27i4tBxz155HO71AopubO3AufeOxMifn4qeXRuwalMbpi/dgL8IcQ5ue20GbnttBp676jjsM6AXnpu4GOua27ChpSP4cfncXaW0m316NGL89WdgXXMbjrjlTewnmdKubW7H+7NWKf3LAOBfYxYE8R8A4LuPjweAyDkACEzyRU65421lvSqO+u2b+P0XD4psEzntzrfRv2dXo6JJ5MYXp0WUDrJ7xM+fjSqz/jwiHFfi+09NDGWduc+zTmgvcDwzfhEuOHwXTFuyQetq8NnfDcdHvzojopBpLxRx8I1vKI95fdoyfOef47G7FIhsSswK/qK1zVrl3tPjFuIN6eV+34jS2Nttux5YIAm3D743B7fExIiQlUYyvhtGHG0dxcAFZcXG1pBlwbDpy/HwyLmYsnj9/2/vzsOsqM48jn9fmkYRRXBBURDBBcUMIqKCuBNZ1OASY/RxRsfEIU501NHEJUQnZkZHHSc6Ppo4GeM8477gGqNRo6KoLAECiBGhWwUaUBqBZm2b5Z0/6txLdd/bDS23+3af/n2ep56ue+7S9VafvufUW6dO5a17mXp5wdE9STeFi6uqqdm4mZpNm1m5roZHJ9Xe/3X30/C732XMCX24cdQhtRJ7j09ZwLtzKzn2gN2znd8xjyS/c8bClayv2cT5v53IzIotf5v0nDRvztn65Vzpu/tMC4mmRyct4JyB+3L8nblz6Lw5Z2n2c58cM5gBPbsw4p7c2+3WvVtSJnn34aIqZi+q4rDUtayLV67nzPvf36YJmfe/4Q+8cPlQuu5Uyuj7cm/L/UVVNaf85/jswc6Anl2489z+/Or1LX/X5XVuIXz/2+U8N30Rxx24R85zjfHlqmoWLl/HoP1zDzqkbRt72qFcO/zgnOT/wXvtkm0Hu+YbyRF+bsu4i4P32rnW//N1Iw7hkmN71+r3ZEZyZEZITP7ZMAw4+rY3geQa8MbaM3XQecnQ/XPONhowOJx8ueLkA7nv7TIO7d45e3B8ev/uzP1yDVeccmB23qB+3TvzzurK7IiOuk7q242JqURyRmZUCGw5y1p3pMH0m06lY2kJFSvWcWq4VXjmAKi0pF29SaOMKWOHsWvHUh6blP9Ex/bcznRrl6sAPPWjIbwwYxEvzVicd86XtEuP682D7+Xvt6bVd2Y+O6w/PE732TJh1j25nClPT4qZvnPEsEO68fPUpJd1nTeoBzvvkJtcyJxoycztkNaj604ctk9nPlq8is3uDM4zsmlrbjv7b7j3rXncMvowrh3el3275CbmDu3emavDLT+hdrIsPT9DPkf26pptY+uTaX/67NmJTyuTtjzfcUs+HTuUsE+Xjkz+WXKSLpMIqO9SKSCbjMvI1Lt7LzgiezI0n225XKW0pB3pfEVJO+MnYfRPQzLfCzuFk7+n9tur1l0Dt9XcW0cByZ2kPlq8qtbohq6dOvDMZcnom+tHHsIPjutNaUk7Bv7rG9x69re48JheXDduZna7Mzq0b8fcfxuV9+TKjqUlPHbp4JzyjIYmBc7c+aY+Zw3Yl6N77563Tu5YWsKUnw1r8O9cLLa9t3JsicxsCPALdx8RHt8I4O7/Xt97Bg0a5FOnFmYUafWGTRxy0x8L8lkiIiKNced3+3PeUbkd8W/KzKa5e+44dWkSheyPNMacL1Yx8p4JXDnsIK459WBgy4i6Fy8fyuENnOmD5EBy+doaeuW5zWHGXxev4rR7J/D0j4bUupXkjIUrWVO9keMO2vrtRDNOvms8w/vtxYXH9MpOKj7zX4aza8dSnpm6kJ+OS85qv/vTk9kvJM0/X7aWk+4az4Hdds57CcED75QzbloFz/34WCbMXcbp/fNPFFqfzOcDfHrbadkDpsxBSd1RAyffNZ7Plq1l6s+/3eC1/w39rpeuGEr/Hl2YVbGS0fe9z8D9uvDcj+ufJLYhT/15Adc/+yHzbh2VM/dJXRs2bWaze4OjZqH+2NPu/OMcfh1Oau3eqQNfra3h89tP5+KHpvDO3Mpa+zLj6akLuW7cLC4a0ouHJ85n1Lf25jfhEpnyyjXss2vH7O0rR9/3HrMqqnjz2hM5YM9tu0tQY930wmwemTSfe74/gLOO2Dc7wvqFy4fWmoT3m6hvH66u3sCy6jsGkQAADN5JREFUNTX03qNT9v/3qP27Zg+g06rWb6C8cg3nhInaH/+HY+i2yw4c2G1L4uhvH5zMe2XLmHnzcA7/5etcduIBOXcrquu8/57IlM+W86drTtzmOzDVp3L119z9p7n84juH5R2xcvOLs3l44nxever47EjO7fXAO+Xc/uocdu/UgWk3nVrruaWrqtmtUwcOHPsqkCRRj+zVlTP6559zBZK/Vc/dOmZvs7uqegMflH3V6JEOo/5rAh8vWcWjPzwm53tx7PMf8pcFK3nlqm2/FA2SUWszK1ayQ/uSei9Pauka0x+JNclxLjDS3S8Nj/8OOMbdr6jzujHAGID99tvvyPnz5+d81jexrmYj/W5+rSCfJSIi0hh3f/9wzj6i4bPBjaEkR/MqVpIDkku/DtizU61J7NbXbMoeLLZUFSvW0b5du1pnnWcuXMk+XTrWmq/C3bn91Tl8b1DP7T4ga2hbps1fUesWua999AVLV1XnXMZRsWIdv5+5hMtO7FOQCT2fnrqQk/t2y5mjo5imL1jB1xs251zKnLZx02bKK9fSd+9dqN6wifU1m+jaqQPrazaxpGo9ffIkJjZvdn4/azFn9N+H98uSS6XrS8wsXVXNx1+srnUJRKGt/Xoj979dxpXDDtrq7Ucba/aiKkpL2tF37/yjjyGp278eX853B/ZocPTF6uoNfFD+FSMOyz3orlq/gU8r13DEfi3zALhm42amL1jxjUbKNGT2oir23nXHehON879aS+Xqr7dplOTC5evo3LE076VGjfFB+TImln+Vvb2uJJTkMPseMKJOkuNod/+n+t5TzE6FiIhIS6UkR/NSf0RERCRXY/ojsd5dpQJIj9XtAWiGNhEREREREZGIxZrk+DNwkJn1NrMOwPnAS0XeJhERERERERFpQlHeXcXdN5rZFcBrJLeQfcjdPyryZomIiIiIiIhIE4oyyQHg7q8ArxR7O0RERERERESkecR6uYqIiIiIiIiItDFKcoiIiIiIiIhIFJTkEBEREREREZEoKMkhIiIiIiIiIlFQkkNEREREREREoqAkh4iIiIiIiIhEQUkOEREREREREYmCkhwiIiIiIiIiEgUlOUREREREREQkCubuxd6GFsHMKoH5Bf7YPYBlBf7MliLm2EDxtWYxxwZxxxdzbNB64+vl7nsWeyPaCvVHmo32SS7tk1zaJ7m0T3Jpn+RX6P2yzf0RJTmakJlNdfdBxd6OphBzbKD4WrOYY4O444s5Nog/Pmm5VPdyaZ/k0j7JpX2SS/skl/ZJfsXcL7pcRURERERERESioCSHiIiIiIiIiERBSY6m9dtib0ATijk2UHytWcyxQdzxxRwbxB+ftFyqe7m0T3Jpn+TSPsmlfZJL+yS/ou0XzckhIiIiIiIiIlHQSA4RERERERERiYKSHE3AzEaa2SdmVmZmNxR7expiZg+Z2VIzm50q283M3jCzeeFn11BuZnZviGuWmQ1Mvefi8Pp5ZnZxqvxIM/swvOdeM7NmjK2nmb1tZh+b2UdmdlVk8e1oZlPMbGaI75ZQ3tvMJodtfcrMOoTyHcLjsvD8/qnPujGUf2JmI1LlRa3LZlZiZn8xs5cjjO3zUHdmmNnUUBZL3exiZuPMbE74/xsSUWx9w98ss6wys6tjiU/iUuzvuWKxArb/sSlEuxqTQrVXsTGzfw7/O7PN7AlL+pxtqq5YEx8jtUb17JP/CP8/s8zseTPrknqueH1wd9dSwAUoAcqBPkAHYCbQr9jb1cD2ngAMBGanyu4EbgjrNwB3hPXTgFcBAwYDk0P5bsCn4WfXsN41PDcFGBLe8yowqhlj6w4MDOu7AHOBfhHFZ8DOYb0UmBy2+2ng/FD+APCPYf3HwANh/XzgqbDeL9TTHYDeof6WtIS6DFwDPA68HB7HFNvnwB51ymKpm/8HXBrWOwBdYomtTpwlwBdArxjj09K6l5bwPVfE2AvS/se4sJ3tamxLIdqr2BZgX+AzoGOqjvx9W6srNPExUmtc6tknw4H2Yf2O1D4pah9cIzkK72igzN0/dfca4EngzCJvU73c/V1geZ3iM0m+9Ak/z0qVP+yJSUAXM+sOjADecPfl7r4CeAMYGZ7r7O4TPantD6c+q8m5+xJ3nx7WVwMfk3xxxxKfu/ua8LA0LA6cAowL5XXjy8Q9DhgWzhCfCTzp7l+7+2dAGUk9LmpdNrMewOnAg+GxEUlsDWj1ddPMOpM0gr8DcPcad18ZQ2x5DAPK3X0+ccYnrVtL/Z5rcgVs/6NSoHY1GgVsr2LUHuhoZu2BnYAltLG60pTHSE2/9U0j3z5x99fdfWN4OAnoEdaL2gdXkqPw9gUWph5XhLLWZC93XwJJRwHoFsrri62h8oo85c0uDJ07gmS0QzTxhWGnM4ClJF+c5cDK1JdNepuycYTnq4DdaXzczeUe4Dpgc3i8O/HEBklC6nUzm2ZmY0JZDHWzD1AJ/G8YEv2gmXUijtjqOh94IqzHGJ+0bi3he67otrP9j00h2tWYFKq9ioq7LwLuAhaQJDeqgGm07bqSUai2PlY/IBnRAkXeJ0pyFF6+zGUst7CpL7bGljcrM9sZeBa42t1XNfTSPGUtOj533+TuA0iypkcDhzawTa0mPjM7A1jq7tPSxQ1sT6uJLWWouw8ERgGXm9kJDby2NcXXnmQo42/c/QhgLcmQzvq0ptiywrXIo4FntvbSPGUtPj6JQpuvSwVo/6NRwHY1JoVqr6IS5pk4k+QSg32ATiR9lbraUl3ZmjbfppvZWGAj8FimKM/Lmm2fKMlReBVAz9TjHsDiIm3LN/VlZvhd+Lk0lNcXW0PlPfKUNxszKyXp4Dzm7s+F4mjiywjDK8eTXAfYJQwvrLtN2TjC87uSDDlrbNzNYSgw2sw+JxnGdgrJGagYYgPA3ReHn0uB50mSVDHUzQqgwt0nh8fjSDqRMcSWNgqY7u5fhsexxSetX9G/54qpQO1/TArVrsakUO1VbL4NfObule6+AXgOOJa2XVcyCtXWRyVMqHoGcGG41BaKvE+U5Ci8PwMHhRmIO5AMZ36pyNvUWC8Bmdl/LwZeTJVfFGYQHgxUhaFarwHDzaxryP4OB14Lz602s8Hh2ryLUp/V5MLv/B3wsbv/KvVULPHtmZnB2Mw6kjRKHwNvA+fWE18m7nOBt8IX0UvA+ZbMjt0bOIhk4sOi1WV3v9Hde7j7/uH3vuXuFxJBbABm1snMdsmsk9Sp2URQN939C2ChmfUNRcOAv8YQWx0XsOVSFYgvPmn9YuiPfCMFbP+jUcB2NRoFbK9iswAYbGY7hf+lzH5ps3UlpSBtfXNvdFMys5HA9cBod1+Xeqq4fXBvATO1xraQzLA7l2R+hLHF3p6tbOsTJNfbbSDJrP2Q5Dq6N4F54edu4bUG3B/i+hAYlPqcH5BMKFMGXJIqH0Ry8FYO3AdYM8Z2HMnwp1nAjLCcFlF8/YG/hPhmAzeH8j4kXyJlJEPpdwjlO4bHZeH5PqnPGhti+ITUnRxaQl0GTmLLLPBRxBbimBmWjzK/P6K6OQCYGurmCyQzikcRW/j9OwFfAbumyqKJT0s8S0v4Di9S3AVr/2Nc2M52NaalUO1VbAtwCzAntEWPkNwho03VFZr4GKk1LvXskzKSOTYy37UPpF5ftD64hV8kIiIiIiIiItKq6XIVEREREREREYmCkhwiIiIiIiIiEgUlOUREREREREQkCkpyiIiIiIiIiEgUlOQQERERERERkSgoySEirZKZnWRmLxd7O0RERKTtUn9EpOVRkkNEWgUzKyn2NoiIiEjbpv6ISMunJIeINDkzu87Mrgzrd5vZW2F9mJk9amYXmNmHZjbbzO5IvW+Nmf3SzCYDQ8xspJnNMbP3gHOKE42IiIi0RuqPiLQNSnKISHN4Fzg+rA8CdjazUuA4YB5wB3AKMAA4yszOCq/tBMx292OAqcD/AN8Jn7V3822+iIiIRED9EZE2QEkOEWkO04AjzWwX4GtgIknn4nhgJTDe3SvdfSPwGHBCeN8m4NmwfgjwmbvPc3cHHm3OAERERKTVU39EpA1QkkNEmpy7bwA+By4BPgAmACcDBwALGnhrtbtvSn9UU22jiIiIxE39EZG2QUkOEWku7wI/CT8nAJcBM4BJwIlmtkeYzOsC4J08758D9DazA8LjC5p+k0VERCQy6o+IRE5JDhFpLhOA7sBEd/8SqAYmuPsS4EbgbWAmMN3dX6z7ZnevBsYAfwgTfc1vti0XERGRWKg/IhI5Sy4lExERERERERFp3TSSQ0RERERERESioCSHiIiIiIiIiERBSQ4RERERERERiYKSHCIiIiIiIiISBSU5RERERERERCQKSnKIiIiIiIiISBSU5BARERERERGRKCjJISIiIiIiIiJR+H87K2zlg8sOXwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1296x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(18,6))\n",
    "# figure 1\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(counts)\n",
    "plt.xlabel(\"word\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.title(\"Frequency of words\")\n",
    "print(\"features\", len(counts), \"mean\", counts.mean(), \"median\", np.median(counts))\n",
    "\n",
    "# figure 2\n",
    "plt.subplot(1, 2, 2)\n",
    "top = 4000\n",
    "bottom = 400\n",
    "counts_filtered_top = counts[counts<top]\n",
    "counts_filtered = counts_filtered_top[counts_filtered_top>bottom]\n",
    "print(\"features\", len(counts_filtered), \"mean\", counts_filtered.mean(), \"median\", np.median(counts_filtered))\n",
    "plt.plot(counts_filtered)\n",
    "plt.xlabel(\"word\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.title(\"Frequency of filtered words. (bottom={},top={})\".format(bottom, top))\n",
    "\n",
    "# possible values for min 3, 50, 100, 400\n",
    "# possible values for max 400, 4000, 40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'whiskers': [<matplotlib.lines.Line2D at 0x1a26256c50>,\n",
       "  <matplotlib.lines.Line2D at 0x1a26256f98>,\n",
       "  <matplotlib.lines.Line2D at 0x1a261fc080>,\n",
       "  <matplotlib.lines.Line2D at 0x1a261fc3c8>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x1a26247320>,\n",
       "  <matplotlib.lines.Line2D at 0x1a26247668>,\n",
       "  <matplotlib.lines.Line2D at 0x1a261fc710>,\n",
       "  <matplotlib.lines.Line2D at 0x1a261fca58>],\n",
       " 'boxes': [<matplotlib.lines.Line2D at 0x1a26256b00>,\n",
       "  <matplotlib.lines.Line2D at 0x1a26247cf8>],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x1a262479b0>,\n",
       "  <matplotlib.lines.Line2D at 0x1a261fcda0>],\n",
       " 'fliers': [],\n",
       " 'means': []}"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_counts = [counts, counts_filtered_top]\n",
    "plt.boxplot(data_counts, showfliers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(18,6))  # follows a normal distribution? \n",
    "plt.subplot(1,2,1)\n",
    "\n",
    "sns.distplot(counts_filtered_top, fit=norm) \n",
    "plt.subplot(1,2,2)\n",
    "res = stats.probplot(counts_filtered_top, plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2386 9 227.142\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1.000e+00, 1.300e+01, 4.500e+01, 1.690e+02, 4.180e+02, 5.140e+02,\n",
       "        4.940e+02, 4.580e+02, 4.770e+02, 5.330e+02, 9.590e+02, 1.695e+03,\n",
       "        1.802e+03, 1.515e+03, 1.326e+03, 1.141e+03, 1.017e+03, 9.060e+02,\n",
       "        8.040e+02, 6.880e+02, 6.570e+02, 5.450e+02, 5.600e+02, 5.060e+02,\n",
       "        4.810e+02, 4.260e+02, 3.610e+02, 3.390e+02, 3.390e+02, 3.180e+02,\n",
       "        3.090e+02, 2.960e+02, 2.420e+02, 2.400e+02, 2.350e+02, 2.530e+02,\n",
       "        2.140e+02, 1.560e+02, 2.070e+02]),\n",
       " array([  0,  10,  20,  30,  40,  50,  60,  70,  80,  90, 100, 110, 120,\n",
       "        130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250,\n",
       "        260, 270, 280, 290, 300, 310, 320, 330, 340, 350, 360, 370, 380,\n",
       "        390]),\n",
       " <a list of 39 Patch objects>)"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAE/tJREFUeJzt3X+w5XV93/HnK6uSNJoCsjoblu2Cs6YFJ93gLWHG6tCYKmBGtGPSZTKRGGdWLczEaTsV6kylZpghadTWqcGucYu0yo9ICIzBKkEN0xkRF1xhEZEFNuGyOyyBBsmYoQXf/eN8Lpws99eec+85d/fzfMycOd/v5/s95/u+n7t3X/fz+X7P96aqkCT16SemXYAkaXoMAUnqmCEgSR0zBCSpY4aAJHXMEJCkjhkCktQxQ0CSOmYISFLHXjLtApZywgkn1ObNm6ddhiQdMe68886/qqr1y9l3zYfA5s2b2bVr17TLkKQjRpK/WO6+TgdJUscMAUnqmCEgSR0zBCSpY4aAJHXMEJCkjhkCktQxQ0CSOmYISFLH1vwnhnVk2Xzxny66fd/lb5tQJZKWw5GAJHVsyRBIsjPJwSR7htquTbK7PfYl2d3aNyf526Ftnx56zeuT3JNkb5JPJsnqfEmSpOVaznTQlcB/Ba6aa6iqfzm3nORjwFND+z9YVVvneZ8rgO3A7cDNwNnAlw+/ZEnSSllyJFBVtwFPzret/Tb/a8DVi71Hkg3Az1TVN6uqGATKOw6/XEnSShr3nMAbgceq6oGhtpOTfCfJnyd5Y2s7EZgd2me2tUmSpmjcq4PO5++OAg4Am6rqiSSvB/4kyWnAfPP/tdCbJtnOYOqITZs2jVmiJGkhI48EkrwE+BfAtXNtVfVMVT3Rlu8EHgRey+A3/41DL98I7F/ovatqR1XNVNXM+vXL+uM4kqQRjDMd9MvA96vq+WmeJOuTrGvLpwBbgIeq6gDwdJIz23mEdwM3jnFsSdIKWM4lolcD3wR+Lslskve2Tdt48QnhNwF3J/ku8EXg/VU1d1L5A8AfAnsZjBC8MkiSpmzJcwJVdf4C7b85T9v1wPUL7L8LeN1h1idJWkV+YliSOua9g3TYlro/kKQjhyMBSeqYISBJHTMEJKljhoAkdcwQkKSOGQKS1DFDQJI6ZghIUscMAUnqmCEgSR0zBCSpY4aAJHXMEJCkjhkCktQxQ0CSOmYISFLHDAFJ6pghIEkdWzIEkuxMcjDJnqG2S5M8mmR3e5w7tO2SJHuT3J/krUPtZ7e2vUkuXvkvRZJ0uJYzErgSOHue9k9U1db2uBkgyanANuC09po/SLIuyTrgU8A5wKnA+W1fSdIULfmH5qvqtiSbl/l+5wHXVNUzwMNJ9gJntG17q+ohgCTXtH2/d9gV64i21B+p33f52yZUiSQY75zARUnubtNFx7W2E4FHhvaZbW0Ltc8ryfYku5Lsevzxx8coUZK0mFFD4ArgNcBW4ADwsdaeefatRdrnVVU7qmqmqmbWr18/YomSpKUsOR00n6p6bG45yWeAL7XVWeCkoV03Avvb8kLtkqQpGWkkkGTD0Oo7gbkrh24CtiU5JsnJwBbgDuDbwJYkJyd5GYOTxzeNXrYkaSUsORJIcjVwFnBCklngI8BZSbYymNLZB7wPoKruTXIdgxO+zwIXVtVz7X0uAr4CrAN2VtW9K/7VSJIOy3KuDjp/nubPLrL/ZcBl87TfDNx8WNVJklaVnxiWpI4ZApLUMUNAkjpmCEhSxwwBSeqYISBJHTMEJKljhoAkdcwQkKSOGQKS1DFDQJI6ZghIUscMAUnqmCEgSR0b6S+LSavFP0QvTZYjAUnqmCEgSR0zBCSpY4aAJHVsyRBIsjPJwSR7htr+U5LvJ7k7yQ1Jjm3tm5P8bZLd7fHpode8Psk9SfYm+WSSrM6XJElaruWMBK4Ezj6k7RbgdVX188APgEuGtj1YVVvb4/1D7VcA24Et7XHoe0qSJmzJEKiq24AnD2n7alU921ZvBzYu9h5JNgA/U1XfrKoCrgLeMVrJkqSVshLnBH4L+PLQ+slJvpPkz5O8sbWdCMwO7TPb2iRJUzTWh8WSfBh4Fvh8azoAbKqqJ5K8HviTJKcB883/1yLvu53B1BGbNm0ap0RJ0iJGHgkkuQD4FeDX2xQPVfVMVT3Rlu8EHgRey+A3/+Epo43A/oXeu6p2VNVMVc2sX79+1BIlSUsYKQSSnA18CHh7Vf1oqH19knVt+RQGJ4AfqqoDwNNJzmxXBb0buHHs6iVJY1lyOijJ1cBZwAlJZoGPMLga6Bjglnal5+3tSqA3AR9N8izwHPD+qpo7qfwBBlca/RSDcwjD5xEkSVOwZAhU1fnzNH92gX2vB65fYNsu4HWHVZ0kaVX5iWFJ6pghIEkdMwQkqWOGgCR1zBCQpI4ZApLUMUNAkjpmCEhSx8a6gZw0aZsv/tNFt++7/G0TqkQ6OjgSkKSOGQKS1DFDQJI6ZghIUscMAUnqmCEgSR0zBCSpY4aAJHXMEJCkjhkCktQxQ0CSOrasEEiyM8nBJHuG2o5PckuSB9rzca09ST6ZZG+Su5OcPvSaC9r+DyS5YOW/HEnS4VjuSOBK4OxD2i4Gbq2qLcCtbR3gHGBLe2wHroBBaAAfAX4ROAP4yFxwSJKmY1l3Ea2q25JsPqT5POCstvw54BvAh1r7VVVVwO1Jjk2yoe17S1U9CZDkFgbBcvVYX4E0ZLG7jHqHUenFxjkn8OqqOgDQnl/V2k8EHhnab7a1LdQuSZqS1TgxnHnaapH2F79Bsj3JriS7Hn/88RUtTpL0gnFC4LE2zUN7PtjaZ4GThvbbCOxfpP1FqmpHVc1U1cz69evHKFGStJhxQuAmYO4KnwuAG4fa392uEjoTeKpNF30FeEuS49oJ4be0NknSlCzrxHCSqxmc2D0hySyDq3wuB65L8l7gL4FfbbvfDJwL7AV+BLwHoKqeTPI7wLfbfh+dO0ksSZqO5V4ddP4Cm948z74FXLjA++wEdi67OknSqvIPzetFlvpj7pKOHt42QpI6ZghIUscMAUnqmCEgSR0zBCSpY14dpG4sddWTN5hTjxwJSFLHDAFJ6pghIEkdMwQkqWOGgCR1zBCQpI4ZApLUMUNAkjpmCEhSxwwBSeqYt42QGm8roR45EpCkjhkCktSxkUMgyc8l2T30+GGSDya5NMmjQ+3nDr3mkiR7k9yf5K0r8yVIkkY18jmBqrof2AqQZB3wKHAD8B7gE1X1+8P7JzkV2AacBvws8GdJXltVz41agyRpPCs1HfRm4MGq+otF9jkPuKaqnqmqh4G9wBkrdHxJ0ghWKgS2AVcPrV+U5O4kO5Mc19pOBB4Z2me2tUmSpmTsEEjyMuDtwB+1piuA1zCYKjoAfGxu13leXgu85/Yku5Lsevzxx8ctUZK0gJUYCZwD3FVVjwFU1WNV9VxV/Rj4DC9M+cwCJw29biOwf743rKodVTVTVTPr169fgRIlSfNZiRA4n6GpoCQbhra9E9jTlm8CtiU5JsnJwBbgjhU4viRpRGN9YjjJ3wP+OfC+oebfS7KVwVTPvrltVXVvkuuA7wHPAhd6ZZAkTddYIVBVPwJeeUjbbyyy/2XAZeMcU5K0cvzEsCR1zBCQpI55F1FpmbzLqI5GjgQkqWOGgCR1zOkgaYU4XaQjkSMBSeqYISBJHTMEJKljhoAkdcwQkKSOGQKS1DFDQJI6ZghIUscMAUnqmCEgSR3zthHSGuAtJzQtjgQkqWOOBKQJWeq3fWkaHAlIUsfGHgkk2Qc8DTwHPFtVM0mOB64FNgP7gF+rqv+TJMB/Ac4FfgT8ZlXdNW4NOjz+RippzkqNBP5ZVW2tqpm2fjFwa1VtAW5t6wDnAFvaYztwxQodX5I0gtU6J3AecFZb/hzwDeBDrf2qqirg9iTHJtlQVQdWqY41y6tBJK0FKxECBXw1SQH/rap2AK+e+4+9qg4keVXb90TgkaHXzra2vxMCSbYzGCmwadOmFShROrL5S4NWy0qEwBuqan/7j/6WJN9fZN/M01YvahgEyQ6AmZmZF23vwWI/9P7AS1opY58TqKr97fkgcANwBvBYkg0A7flg230WOGno5RuB/ePWIEkazVgjgSQ/DfxEVT3dlt8CfBS4CbgAuLw939hechNwUZJrgF8EnurxfMBq8+qf/jhdpFGNOx30auCGwZWfvAT4QlX9ryTfBq5L8l7gL4FfbfvfzODy0L0MLhF9z5jHlySNYawQqKqHgH88T/sTwJvnaS/gwnGOKX/Tl7Ry/MSwJHXMEJCkjhkCktQx7yIqdcCrh7QQQ2CVePJW0pHA6SBJ6pghIEkdMwQkqWOGgCR1zBCQpI55dZAkLyHtmCMBSeqYISBJHXM6SNKSxvnwo1NJa5sjAUnqmCMBSavKk85rmyEgaaoMielyOkiSOuZIQNKa5khhdY08EkhyUpKvJ7kvyb1Jfru1X5rk0SS72+PcoddckmRvkvuTvHUlvgBJ0ujGGQk8C/ybqrorySuAO5Pc0rZ9oqp+f3jnJKcC24DTgJ8F/izJa6vquTFqkCSNYeSRQFUdqKq72vLTwH3AiYu85Dzgmqp6pqoeBvYCZ4x6fEnS+FbkxHCSzcAvAN9qTRcluTvJziTHtbYTgUeGXjbL4qEhSVplY58YTvJy4Hrgg1X1wyRXAL8DVHv+GPBbQOZ5eS3wntuB7QCbNm0at0RJGkkPJ6XHCoEkL2UQAJ+vqj8GqKrHhrZ/BvhSW50FThp6+UZg/3zvW1U7gB0AMzMz8waFJIG3tBjXyCGQJMBngfuq6uND7Ruq6kBbfSewpy3fBHwhyccZnBjeAtwx6vElaVzjBMjRYpyRwBuA3wDuSbK7tf174PwkWxlM9ewD3gdQVfcmuQ74HoMriy70yiBJmq6RQ6Cq/jfzz/PfvMhrLgMuG/WYkrSWHA3nDPzEsCRNwVoJEO8dJEkdMwQkqWOGgCR1zHMCI/LSMklHA0cCktQxRwKStEqOhBkDRwKS1DFDQJI6ZghIUscMAUnqmCEgSR0zBCSpY4aAJHXMEJCkjhkCktQxQ0CSOmYISFLHDAFJ6pg3kFvAkXDjJ0ka18RHAknOTnJ/kr1JLp708SVJL5hoCCRZB3wKOAc4FTg/yamTrEGS9IJJTwedAeytqocAklwDnAd8b8J1AE75SNKkp4NOBB4ZWp9tbZKkKZj0SCDztNWLdkq2A9vb6t8kuX/E450A/NWIr11t1jYaaxuNtY1marXld5fcZbHa/sFyjzPpEJgFThpa3wjsP3SnqtoB7Bj3YEl2VdXMuO+zGqxtNNY2GmsbTQ+1TXo66NvAliQnJ3kZsA24acI1SJKaiY4EqurZJBcBXwHWATur6t5J1iBJesHEPyxWVTcDN0/ocGNPKa0iaxuNtY3G2kZz1NeWqhedl5UkdcJ7B0lSx47KEFhrt6ZIsi/JPUl2J9nV2o5PckuSB9rzcROsZ2eSg0n2DLXNW08GPtn68u4kp0+4rkuTPNr6bneSc4e2XdLquj/JW1errnask5J8Pcl9Se5N8tutfS3020K1Tb3vkvxkkjuSfLfV9h9b+8lJvtX67dp2oQhJjmnre9v2zVOo7cokDw/129bWPrHv6VCN65J8J8mX2vrK91tVHVUPBiecHwROAV4GfBc4dco17QNOOKTt94CL2/LFwO9OsJ43AacDe5aqBzgX+DKDz3icCXxrwnVdCvzbefY9tX1vjwFObt/zdatY2wbg9Lb8CuAHrYa10G8L1Tb1vmtf/8vb8kuBb7X+uA7Y1to/DXygLf8r4NNteRtw7Sr220K1XQm8a579J/Y9HTrmvwa+AHypra94vx2NI4Hnb01RVf8XmLs1xVpzHvC5tvw54B2TOnBV3QY8ucx6zgOuqoHbgWOTbJhgXQs5D7imqp6pqoeBvQy+96uiqg5U1V1t+WngPgafdl8L/bZQbQuZWN+1r/9v2upL26OAXwK+2NoP7be5/vwi8OYk833IdDVrW8jEvqcASTYCbwP+sK2HVei3ozEE1uKtKQr4apI7M/g0NMCrq+oADH6IgVdNrbrF61kL/XlRG37vHJo2m1pdbaj9Cwx+c1xT/XZIbbAG+q5NaewGDgK3MBh5/HVVPTvP8Z+vrW1/CnjlpGqrqrl+u6z12yeSHHNobfPUvRr+M/DvgB+39VeyCv12NIbAsm5NMWFvqKrTGdw99cIkb5pyPYdj2v15BfAaYCtwAPhYa59KXUleDlwPfLCqfrjYrvO0rWp989S2Jvquqp6rqq0M7hBwBvCPFjn+VGtL8jrgEuAfAv8EOB740KRrS/IrwMGqunO4eZHjj1zb0RgCy7o1xSRV1f72fBC4gcEPwmNzQ8n2fHB6FcIi9Uy1P6vqsfaD+mPgM7wwbTHxupK8lMF/sp+vqj9uzWui3+arbS31Xavnr4FvMJhPPzbJ3OeUho//fG1t+99n+VOEK1Hb2W16rarqGeC/M51+ewPw9iT7GExp/xKDkcGK99vRGAJr6tYUSX46ySvmloG3AHtaTRe03S4AbpxOhc9bqJ6bgHe3KyPOBJ6am/6YhEPmXN/JoO/m6trWroo4GdgC3LGKdQT4LHBfVX18aNPU+22h2tZC3yVZn+TYtvxTwC8zOGfxdeBdbbdD+22uP98FfK3a2c4J1fb9oVAPgzn34X6byPe0qi6pqo1VtZnB/2Ffq6pfZzX6bbXPbk/jweAs/g8YzD1+eMq1nMLgSozvAvfO1cNgvu5W4IH2fPwEa7qawfTA/2PwG8R7F6qHwTDzU60v7wFmJlzX/2jHvbv9Q98wtP+HW133A+escp/9UwbD67uB3e1x7hrpt4Vqm3rfAT8PfKfVsAf4D0M/F3cwOCn9R8Axrf0n2/retv2UKdT2tdZve4D/yQtXEE3se3pInWfxwtVBK95vfmJYkjp2NE4HSZKWyRCQpI4ZApLUMUNAkjpmCEhSxwwBSeqYISBJHTMEJKlj/x/2+vjfeZ5zOAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts_review = np.ravel(data.sum(axis=1))\n",
    "print(counts_review.max(), counts_review.min(), counts_review.mean())\n",
    "plt.hist(counts_review, bins = np.arange(0,400,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering & Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = 'f1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering considering TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5251.9633893966675,\n",
       " {'vec__stem': False,\n",
       "  'vec__ngram_range': (1, 2),\n",
       "  'vec__min_df': 100,\n",
       "  'vec__max_df': 4000,\n",
       "  'tfidf__smooth_idf': False,\n",
       "  'tfidf__norm': 'l2',\n",
       "  'clf__fit_intercept': False,\n",
       "  'clf__C': 2.5},\n",
       " 0.8953462918411281)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_regression_tfidf = {\n",
    "    'vec__min_df': [3, 50, 100, 400, 1],\n",
    "    'vec__max_df': [4000, 40000, 1.0],\n",
    "    'vec__stem': [True, False],\n",
    "    'vec__ngram_range':[(1, 1), (1, 2), (2, 2)],\n",
    "    'tfidf__norm': ['l1', 'l2'],\n",
    "    'tfidf__smooth_idf': [True, False],\n",
    "    'clf__fit_intercept': [True, False], \n",
    "    'clf__C': [.5, 1, 2, 2.5, 3], \n",
    "}\n",
    "\n",
    "pipeline_regression_tfidf = Pipeline([\n",
    "    ('vec', LemmaCountVectorizer(strip_accents='unicode', stop_words=None, binary=False)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression(solver='saga', penalty='l2'))\n",
    "])\n",
    "                  \n",
    "rs_regression_tfidf = RandomizedSearchCV(pipeline_regression_tfidf, parameters_regression_tfidf, \n",
    "                                   cv=10, scoring=score, n_jobs=-1, verbose=0, random_state=62, n_iter=20, \n",
    "                                         return_train_score=True)\n",
    "start = time.time()\n",
    "rs_regression_tfidf.fit(X_train, y)\n",
    "time.time() - start, rs_regression_tfidf.best_params_, rs_regression_tfidf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_vec__stem</th>\n",
       "      <th>param_vec__ngram_range</th>\n",
       "      <th>param_vec__min_df</th>\n",
       "      <th>param_vec__max_df</th>\n",
       "      <th>param_tfidf__smooth_idf</th>\n",
       "      <th>param_tfidf__norm</th>\n",
       "      <th>param_clf__fit_intercept</th>\n",
       "      <th>param_clf__C</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>split5_train_score</th>\n",
       "      <th>split6_train_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>split8_train_score</th>\n",
       "      <th>split9_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>31.785872</td>\n",
       "      <td>0.286845</td>\n",
       "      <td>1.997381</td>\n",
       "      <td>0.067683</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>100</td>\n",
       "      <td>4000</td>\n",
       "      <td>False</td>\n",
       "      <td>l2</td>\n",
       "      <td>False</td>\n",
       "      <td>2.5</td>\n",
       "      <td>{'vec__stem': False, 'vec__ngram_range': (1, 2...</td>\n",
       "      <td>0.901186</td>\n",
       "      <td>0.906613</td>\n",
       "      <td>0.895817</td>\n",
       "      <td>0.892320</td>\n",
       "      <td>0.894548</td>\n",
       "      <td>0.885635</td>\n",
       "      <td>0.883117</td>\n",
       "      <td>0.903251</td>\n",
       "      <td>0.892332</td>\n",
       "      <td>0.898643</td>\n",
       "      <td>0.895346</td>\n",
       "      <td>0.007052</td>\n",
       "      <td>1</td>\n",
       "      <td>0.948263</td>\n",
       "      <td>0.949046</td>\n",
       "      <td>0.948518</td>\n",
       "      <td>0.948519</td>\n",
       "      <td>0.948216</td>\n",
       "      <td>0.948183</td>\n",
       "      <td>0.949060</td>\n",
       "      <td>0.947201</td>\n",
       "      <td>0.948262</td>\n",
       "      <td>0.948207</td>\n",
       "      <td>0.948348</td>\n",
       "      <td>0.000495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>252.530001</td>\n",
       "      <td>1.001709</td>\n",
       "      <td>26.464286</td>\n",
       "      <td>0.475152</td>\n",
       "      <td>True</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>l2</td>\n",
       "      <td>True</td>\n",
       "      <td>2.5</td>\n",
       "      <td>{'vec__stem': True, 'vec__ngram_range': (2, 2)...</td>\n",
       "      <td>0.895924</td>\n",
       "      <td>0.897943</td>\n",
       "      <td>0.881383</td>\n",
       "      <td>0.887658</td>\n",
       "      <td>0.884446</td>\n",
       "      <td>0.891989</td>\n",
       "      <td>0.881064</td>\n",
       "      <td>0.892758</td>\n",
       "      <td>0.895110</td>\n",
       "      <td>0.898447</td>\n",
       "      <td>0.890672</td>\n",
       "      <td>0.006270</td>\n",
       "      <td>2</td>\n",
       "      <td>0.987338</td>\n",
       "      <td>0.988704</td>\n",
       "      <td>0.987783</td>\n",
       "      <td>0.988128</td>\n",
       "      <td>0.987906</td>\n",
       "      <td>0.987913</td>\n",
       "      <td>0.987604</td>\n",
       "      <td>0.987150</td>\n",
       "      <td>0.988095</td>\n",
       "      <td>0.987423</td>\n",
       "      <td>0.987804</td>\n",
       "      <td>0.000430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>11.857570</td>\n",
       "      <td>0.060254</td>\n",
       "      <td>1.119470</td>\n",
       "      <td>0.017520</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>50</td>\n",
       "      <td>40000</td>\n",
       "      <td>False</td>\n",
       "      <td>l2</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>{'vec__stem': False, 'vec__ngram_range': (1, 1...</td>\n",
       "      <td>0.898930</td>\n",
       "      <td>0.894042</td>\n",
       "      <td>0.890205</td>\n",
       "      <td>0.888801</td>\n",
       "      <td>0.887735</td>\n",
       "      <td>0.883353</td>\n",
       "      <td>0.876582</td>\n",
       "      <td>0.886680</td>\n",
       "      <td>0.882797</td>\n",
       "      <td>0.893012</td>\n",
       "      <td>0.888214</td>\n",
       "      <td>0.006058</td>\n",
       "      <td>3</td>\n",
       "      <td>0.934394</td>\n",
       "      <td>0.934399</td>\n",
       "      <td>0.935233</td>\n",
       "      <td>0.935303</td>\n",
       "      <td>0.934011</td>\n",
       "      <td>0.935261</td>\n",
       "      <td>0.934873</td>\n",
       "      <td>0.935120</td>\n",
       "      <td>0.935144</td>\n",
       "      <td>0.934095</td>\n",
       "      <td>0.934783</td>\n",
       "      <td>0.000482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>235.351550</td>\n",
       "      <td>5.165870</td>\n",
       "      <td>25.773658</td>\n",
       "      <td>0.816844</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>50</td>\n",
       "      <td>40000</td>\n",
       "      <td>True</td>\n",
       "      <td>l2</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>{'vec__stem': True, 'vec__ngram_range': (1, 1)...</td>\n",
       "      <td>0.892928</td>\n",
       "      <td>0.889509</td>\n",
       "      <td>0.888976</td>\n",
       "      <td>0.885624</td>\n",
       "      <td>0.884075</td>\n",
       "      <td>0.874162</td>\n",
       "      <td>0.874507</td>\n",
       "      <td>0.886867</td>\n",
       "      <td>0.887569</td>\n",
       "      <td>0.894966</td>\n",
       "      <td>0.885918</td>\n",
       "      <td>0.006548</td>\n",
       "      <td>4</td>\n",
       "      <td>0.926562</td>\n",
       "      <td>0.928436</td>\n",
       "      <td>0.927657</td>\n",
       "      <td>0.928455</td>\n",
       "      <td>0.928174</td>\n",
       "      <td>0.929196</td>\n",
       "      <td>0.930041</td>\n",
       "      <td>0.928278</td>\n",
       "      <td>0.928935</td>\n",
       "      <td>0.926838</td>\n",
       "      <td>0.928257</td>\n",
       "      <td>0.000992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>32.017736</td>\n",
       "      <td>0.424411</td>\n",
       "      <td>2.087793</td>\n",
       "      <td>0.033162</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>l2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'vec__stem': False, 'vec__ngram_range': (1, 2...</td>\n",
       "      <td>0.892758</td>\n",
       "      <td>0.896026</td>\n",
       "      <td>0.879086</td>\n",
       "      <td>0.885805</td>\n",
       "      <td>0.887828</td>\n",
       "      <td>0.878416</td>\n",
       "      <td>0.872399</td>\n",
       "      <td>0.888536</td>\n",
       "      <td>0.883241</td>\n",
       "      <td>0.881543</td>\n",
       "      <td>0.884564</td>\n",
       "      <td>0.006732</td>\n",
       "      <td>5</td>\n",
       "      <td>0.909443</td>\n",
       "      <td>0.907923</td>\n",
       "      <td>0.909866</td>\n",
       "      <td>0.909635</td>\n",
       "      <td>0.909555</td>\n",
       "      <td>0.910429</td>\n",
       "      <td>0.909019</td>\n",
       "      <td>0.907810</td>\n",
       "      <td>0.909788</td>\n",
       "      <td>0.908067</td>\n",
       "      <td>0.909153</td>\n",
       "      <td>0.000868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>227.387620</td>\n",
       "      <td>35.132097</td>\n",
       "      <td>21.169703</td>\n",
       "      <td>7.060336</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>400</td>\n",
       "      <td>40000</td>\n",
       "      <td>True</td>\n",
       "      <td>l2</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>{'vec__stem': True, 'vec__ngram_range': (1, 2)...</td>\n",
       "      <td>0.879874</td>\n",
       "      <td>0.881342</td>\n",
       "      <td>0.871614</td>\n",
       "      <td>0.877373</td>\n",
       "      <td>0.875650</td>\n",
       "      <td>0.864715</td>\n",
       "      <td>0.867821</td>\n",
       "      <td>0.881933</td>\n",
       "      <td>0.873849</td>\n",
       "      <td>0.874803</td>\n",
       "      <td>0.874897</td>\n",
       "      <td>0.005368</td>\n",
       "      <td>6</td>\n",
       "      <td>0.895291</td>\n",
       "      <td>0.893376</td>\n",
       "      <td>0.894767</td>\n",
       "      <td>0.894157</td>\n",
       "      <td>0.894333</td>\n",
       "      <td>0.894665</td>\n",
       "      <td>0.895187</td>\n",
       "      <td>0.894739</td>\n",
       "      <td>0.895877</td>\n",
       "      <td>0.894746</td>\n",
       "      <td>0.894714</td>\n",
       "      <td>0.000645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31.758895</td>\n",
       "      <td>0.151905</td>\n",
       "      <td>2.151341</td>\n",
       "      <td>0.038139</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>50</td>\n",
       "      <td>4000</td>\n",
       "      <td>False</td>\n",
       "      <td>l1</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>{'vec__stem': False, 'vec__ngram_range': (1, 2...</td>\n",
       "      <td>0.873072</td>\n",
       "      <td>0.875693</td>\n",
       "      <td>0.868072</td>\n",
       "      <td>0.868599</td>\n",
       "      <td>0.873317</td>\n",
       "      <td>0.866878</td>\n",
       "      <td>0.857592</td>\n",
       "      <td>0.879365</td>\n",
       "      <td>0.871146</td>\n",
       "      <td>0.868944</td>\n",
       "      <td>0.870268</td>\n",
       "      <td>0.005583</td>\n",
       "      <td>7</td>\n",
       "      <td>0.885394</td>\n",
       "      <td>0.884994</td>\n",
       "      <td>0.885767</td>\n",
       "      <td>0.886080</td>\n",
       "      <td>0.885455</td>\n",
       "      <td>0.884806</td>\n",
       "      <td>0.885861</td>\n",
       "      <td>0.883866</td>\n",
       "      <td>0.884769</td>\n",
       "      <td>0.884162</td>\n",
       "      <td>0.885116</td>\n",
       "      <td>0.000693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>252.265632</td>\n",
       "      <td>1.684677</td>\n",
       "      <td>26.957509</td>\n",
       "      <td>0.342728</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>400</td>\n",
       "      <td>4000</td>\n",
       "      <td>True</td>\n",
       "      <td>l2</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>{'vec__stem': True, 'vec__ngram_range': (1, 2)...</td>\n",
       "      <td>0.874016</td>\n",
       "      <td>0.877221</td>\n",
       "      <td>0.869667</td>\n",
       "      <td>0.864158</td>\n",
       "      <td>0.877543</td>\n",
       "      <td>0.859621</td>\n",
       "      <td>0.859621</td>\n",
       "      <td>0.872713</td>\n",
       "      <td>0.874601</td>\n",
       "      <td>0.873072</td>\n",
       "      <td>0.870223</td>\n",
       "      <td>0.006421</td>\n",
       "      <td>8</td>\n",
       "      <td>0.893104</td>\n",
       "      <td>0.893063</td>\n",
       "      <td>0.894436</td>\n",
       "      <td>0.894980</td>\n",
       "      <td>0.894086</td>\n",
       "      <td>0.894450</td>\n",
       "      <td>0.893813</td>\n",
       "      <td>0.892558</td>\n",
       "      <td>0.894301</td>\n",
       "      <td>0.893376</td>\n",
       "      <td>0.893817</td>\n",
       "      <td>0.000728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.121524</td>\n",
       "      <td>0.353529</td>\n",
       "      <td>2.430551</td>\n",
       "      <td>0.059831</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>3</td>\n",
       "      <td>4000</td>\n",
       "      <td>True</td>\n",
       "      <td>l1</td>\n",
       "      <td>True</td>\n",
       "      <td>2.5</td>\n",
       "      <td>{'vec__stem': False, 'vec__ngram_range': (1, 2...</td>\n",
       "      <td>0.872868</td>\n",
       "      <td>0.874660</td>\n",
       "      <td>0.867101</td>\n",
       "      <td>0.867545</td>\n",
       "      <td>0.876596</td>\n",
       "      <td>0.862822</td>\n",
       "      <td>0.854839</td>\n",
       "      <td>0.876947</td>\n",
       "      <td>0.872671</td>\n",
       "      <td>0.869024</td>\n",
       "      <td>0.869507</td>\n",
       "      <td>0.006505</td>\n",
       "      <td>9</td>\n",
       "      <td>0.892433</td>\n",
       "      <td>0.893998</td>\n",
       "      <td>0.894150</td>\n",
       "      <td>0.893688</td>\n",
       "      <td>0.893267</td>\n",
       "      <td>0.893915</td>\n",
       "      <td>0.893108</td>\n",
       "      <td>0.891587</td>\n",
       "      <td>0.892304</td>\n",
       "      <td>0.892833</td>\n",
       "      <td>0.893128</td>\n",
       "      <td>0.000799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>25.720951</td>\n",
       "      <td>0.083602</td>\n",
       "      <td>1.559441</td>\n",
       "      <td>0.017758</td>\n",
       "      <td>False</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>l2</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>{'vec__stem': False, 'vec__ngram_range': (2, 2...</td>\n",
       "      <td>0.867460</td>\n",
       "      <td>0.873453</td>\n",
       "      <td>0.859144</td>\n",
       "      <td>0.860649</td>\n",
       "      <td>0.860567</td>\n",
       "      <td>0.867894</td>\n",
       "      <td>0.861961</td>\n",
       "      <td>0.876898</td>\n",
       "      <td>0.859635</td>\n",
       "      <td>0.869148</td>\n",
       "      <td>0.865681</td>\n",
       "      <td>0.005924</td>\n",
       "      <td>10</td>\n",
       "      <td>0.938387</td>\n",
       "      <td>0.936569</td>\n",
       "      <td>0.937459</td>\n",
       "      <td>0.937939</td>\n",
       "      <td>0.937975</td>\n",
       "      <td>0.936563</td>\n",
       "      <td>0.937149</td>\n",
       "      <td>0.937276</td>\n",
       "      <td>0.938145</td>\n",
       "      <td>0.937840</td>\n",
       "      <td>0.937530</td>\n",
       "      <td>0.000604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>256.893795</td>\n",
       "      <td>1.417640</td>\n",
       "      <td>27.027909</td>\n",
       "      <td>0.554721</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>3</td>\n",
       "      <td>4000</td>\n",
       "      <td>True</td>\n",
       "      <td>l1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>{'vec__stem': True, 'vec__ngram_range': (1, 2)...</td>\n",
       "      <td>0.874025</td>\n",
       "      <td>0.871458</td>\n",
       "      <td>0.864558</td>\n",
       "      <td>0.860916</td>\n",
       "      <td>0.865512</td>\n",
       "      <td>0.860197</td>\n",
       "      <td>0.844664</td>\n",
       "      <td>0.869205</td>\n",
       "      <td>0.867184</td>\n",
       "      <td>0.855964</td>\n",
       "      <td>0.863368</td>\n",
       "      <td>0.008089</td>\n",
       "      <td>11</td>\n",
       "      <td>0.885483</td>\n",
       "      <td>0.886482</td>\n",
       "      <td>0.886941</td>\n",
       "      <td>0.887506</td>\n",
       "      <td>0.886144</td>\n",
       "      <td>0.886329</td>\n",
       "      <td>0.886271</td>\n",
       "      <td>0.884889</td>\n",
       "      <td>0.886570</td>\n",
       "      <td>0.886135</td>\n",
       "      <td>0.886275</td>\n",
       "      <td>0.000684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.569095</td>\n",
       "      <td>0.088059</td>\n",
       "      <td>1.080440</td>\n",
       "      <td>0.017520</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>100</td>\n",
       "      <td>4000</td>\n",
       "      <td>True</td>\n",
       "      <td>l1</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>{'vec__stem': False, 'vec__ngram_range': (1, 1...</td>\n",
       "      <td>0.870283</td>\n",
       "      <td>0.868400</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.851618</td>\n",
       "      <td>0.866745</td>\n",
       "      <td>0.852317</td>\n",
       "      <td>0.846245</td>\n",
       "      <td>0.864886</td>\n",
       "      <td>0.866588</td>\n",
       "      <td>0.867687</td>\n",
       "      <td>0.861523</td>\n",
       "      <td>0.008023</td>\n",
       "      <td>12</td>\n",
       "      <td>0.873370</td>\n",
       "      <td>0.874133</td>\n",
       "      <td>0.874891</td>\n",
       "      <td>0.875524</td>\n",
       "      <td>0.874580</td>\n",
       "      <td>0.875437</td>\n",
       "      <td>0.876675</td>\n",
       "      <td>0.873629</td>\n",
       "      <td>0.872422</td>\n",
       "      <td>0.873142</td>\n",
       "      <td>0.874380</td>\n",
       "      <td>0.001225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>28.849953</td>\n",
       "      <td>0.202854</td>\n",
       "      <td>1.745739</td>\n",
       "      <td>0.042130</td>\n",
       "      <td>False</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>3</td>\n",
       "      <td>4000</td>\n",
       "      <td>True</td>\n",
       "      <td>l1</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>{'vec__stem': False, 'vec__ngram_range': (2, 2...</td>\n",
       "      <td>0.851047</td>\n",
       "      <td>0.866641</td>\n",
       "      <td>0.848672</td>\n",
       "      <td>0.853592</td>\n",
       "      <td>0.854920</td>\n",
       "      <td>0.858142</td>\n",
       "      <td>0.844615</td>\n",
       "      <td>0.864717</td>\n",
       "      <td>0.855810</td>\n",
       "      <td>0.851708</td>\n",
       "      <td>0.854987</td>\n",
       "      <td>0.006456</td>\n",
       "      <td>13</td>\n",
       "      <td>0.894580</td>\n",
       "      <td>0.893817</td>\n",
       "      <td>0.895324</td>\n",
       "      <td>0.894775</td>\n",
       "      <td>0.894024</td>\n",
       "      <td>0.894074</td>\n",
       "      <td>0.893159</td>\n",
       "      <td>0.892481</td>\n",
       "      <td>0.894805</td>\n",
       "      <td>0.894111</td>\n",
       "      <td>0.894115</td>\n",
       "      <td>0.000792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>265.263951</td>\n",
       "      <td>13.972791</td>\n",
       "      <td>27.285560</td>\n",
       "      <td>0.624821</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>1</td>\n",
       "      <td>4000</td>\n",
       "      <td>False</td>\n",
       "      <td>l1</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>{'vec__stem': True, 'vec__ngram_range': (1, 1)...</td>\n",
       "      <td>0.865303</td>\n",
       "      <td>0.860662</td>\n",
       "      <td>0.852807</td>\n",
       "      <td>0.847994</td>\n",
       "      <td>0.860331</td>\n",
       "      <td>0.845798</td>\n",
       "      <td>0.840613</td>\n",
       "      <td>0.858810</td>\n",
       "      <td>0.861989</td>\n",
       "      <td>0.852611</td>\n",
       "      <td>0.854692</td>\n",
       "      <td>0.007618</td>\n",
       "      <td>14</td>\n",
       "      <td>0.868340</td>\n",
       "      <td>0.869127</td>\n",
       "      <td>0.869714</td>\n",
       "      <td>0.872280</td>\n",
       "      <td>0.869223</td>\n",
       "      <td>0.870105</td>\n",
       "      <td>0.870473</td>\n",
       "      <td>0.869230</td>\n",
       "      <td>0.868977</td>\n",
       "      <td>0.868914</td>\n",
       "      <td>0.869638</td>\n",
       "      <td>0.001056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10.883897</td>\n",
       "      <td>0.051391</td>\n",
       "      <td>1.111758</td>\n",
       "      <td>0.020170</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>50</td>\n",
       "      <td>4000</td>\n",
       "      <td>False</td>\n",
       "      <td>l1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'vec__stem': False, 'vec__ngram_range': (1, 1...</td>\n",
       "      <td>0.856481</td>\n",
       "      <td>0.851080</td>\n",
       "      <td>0.849315</td>\n",
       "      <td>0.846569</td>\n",
       "      <td>0.851568</td>\n",
       "      <td>0.838536</td>\n",
       "      <td>0.833909</td>\n",
       "      <td>0.853044</td>\n",
       "      <td>0.857364</td>\n",
       "      <td>0.848111</td>\n",
       "      <td>0.848598</td>\n",
       "      <td>0.007040</td>\n",
       "      <td>15</td>\n",
       "      <td>0.857155</td>\n",
       "      <td>0.857949</td>\n",
       "      <td>0.858231</td>\n",
       "      <td>0.858832</td>\n",
       "      <td>0.860366</td>\n",
       "      <td>0.858047</td>\n",
       "      <td>0.861208</td>\n",
       "      <td>0.857791</td>\n",
       "      <td>0.856286</td>\n",
       "      <td>0.857744</td>\n",
       "      <td>0.858361</td>\n",
       "      <td>0.001382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25.326829</td>\n",
       "      <td>0.283038</td>\n",
       "      <td>1.507646</td>\n",
       "      <td>0.063554</td>\n",
       "      <td>False</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>100</td>\n",
       "      <td>40000</td>\n",
       "      <td>True</td>\n",
       "      <td>l2</td>\n",
       "      <td>True</td>\n",
       "      <td>2.5</td>\n",
       "      <td>{'vec__stem': False, 'vec__ngram_range': (2, 2...</td>\n",
       "      <td>0.850475</td>\n",
       "      <td>0.858167</td>\n",
       "      <td>0.848057</td>\n",
       "      <td>0.844869</td>\n",
       "      <td>0.836422</td>\n",
       "      <td>0.845229</td>\n",
       "      <td>0.843383</td>\n",
       "      <td>0.858404</td>\n",
       "      <td>0.850337</td>\n",
       "      <td>0.844711</td>\n",
       "      <td>0.848005</td>\n",
       "      <td>0.006379</td>\n",
       "      <td>16</td>\n",
       "      <td>0.907862</td>\n",
       "      <td>0.906082</td>\n",
       "      <td>0.907545</td>\n",
       "      <td>0.908448</td>\n",
       "      <td>0.907614</td>\n",
       "      <td>0.906573</td>\n",
       "      <td>0.906986</td>\n",
       "      <td>0.906558</td>\n",
       "      <td>0.907590</td>\n",
       "      <td>0.905464</td>\n",
       "      <td>0.907072</td>\n",
       "      <td>0.000860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>11.551335</td>\n",
       "      <td>0.101160</td>\n",
       "      <td>1.127901</td>\n",
       "      <td>0.084802</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>l1</td>\n",
       "      <td>False</td>\n",
       "      <td>2.5</td>\n",
       "      <td>{'vec__stem': False, 'vec__ngram_range': (1, 1...</td>\n",
       "      <td>0.840062</td>\n",
       "      <td>0.845732</td>\n",
       "      <td>0.825952</td>\n",
       "      <td>0.841449</td>\n",
       "      <td>0.834877</td>\n",
       "      <td>0.826138</td>\n",
       "      <td>0.824750</td>\n",
       "      <td>0.830975</td>\n",
       "      <td>0.832241</td>\n",
       "      <td>0.837607</td>\n",
       "      <td>0.833978</td>\n",
       "      <td>0.006841</td>\n",
       "      <td>17</td>\n",
       "      <td>0.843728</td>\n",
       "      <td>0.844757</td>\n",
       "      <td>0.846277</td>\n",
       "      <td>0.844758</td>\n",
       "      <td>0.845107</td>\n",
       "      <td>0.845388</td>\n",
       "      <td>0.845085</td>\n",
       "      <td>0.844892</td>\n",
       "      <td>0.845258</td>\n",
       "      <td>0.844321</td>\n",
       "      <td>0.844957</td>\n",
       "      <td>0.000637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>24.985783</td>\n",
       "      <td>0.168056</td>\n",
       "      <td>1.534324</td>\n",
       "      <td>0.083687</td>\n",
       "      <td>False</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>100</td>\n",
       "      <td>4000</td>\n",
       "      <td>True</td>\n",
       "      <td>l1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'vec__stem': False, 'vec__ngram_range': (2, 2...</td>\n",
       "      <td>0.820087</td>\n",
       "      <td>0.839731</td>\n",
       "      <td>0.824214</td>\n",
       "      <td>0.820047</td>\n",
       "      <td>0.821019</td>\n",
       "      <td>0.829753</td>\n",
       "      <td>0.816199</td>\n",
       "      <td>0.831816</td>\n",
       "      <td>0.822319</td>\n",
       "      <td>0.816310</td>\n",
       "      <td>0.824150</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>18</td>\n",
       "      <td>0.838986</td>\n",
       "      <td>0.837016</td>\n",
       "      <td>0.837943</td>\n",
       "      <td>0.837507</td>\n",
       "      <td>0.838164</td>\n",
       "      <td>0.836942</td>\n",
       "      <td>0.838334</td>\n",
       "      <td>0.837199</td>\n",
       "      <td>0.837860</td>\n",
       "      <td>0.838859</td>\n",
       "      <td>0.837881</td>\n",
       "      <td>0.000686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45.815221</td>\n",
       "      <td>0.998658</td>\n",
       "      <td>2.750689</td>\n",
       "      <td>0.063005</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>l1</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>{'vec__stem': False, 'vec__ngram_range': (1, 2...</td>\n",
       "      <td>0.810117</td>\n",
       "      <td>0.830469</td>\n",
       "      <td>0.802185</td>\n",
       "      <td>0.823391</td>\n",
       "      <td>0.809119</td>\n",
       "      <td>0.811313</td>\n",
       "      <td>0.801237</td>\n",
       "      <td>0.815987</td>\n",
       "      <td>0.809375</td>\n",
       "      <td>0.808112</td>\n",
       "      <td>0.812131</td>\n",
       "      <td>0.008562</td>\n",
       "      <td>19</td>\n",
       "      <td>0.834398</td>\n",
       "      <td>0.833092</td>\n",
       "      <td>0.834002</td>\n",
       "      <td>0.834786</td>\n",
       "      <td>0.834562</td>\n",
       "      <td>0.834173</td>\n",
       "      <td>0.832723</td>\n",
       "      <td>0.832978</td>\n",
       "      <td>0.835115</td>\n",
       "      <td>0.832921</td>\n",
       "      <td>0.833875</td>\n",
       "      <td>0.000829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>232.212803</td>\n",
       "      <td>2.107050</td>\n",
       "      <td>25.703045</td>\n",
       "      <td>0.415816</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>1</td>\n",
       "      <td>40000</td>\n",
       "      <td>True</td>\n",
       "      <td>l1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'vec__stem': True, 'vec__ngram_range': (1, 1)...</td>\n",
       "      <td>0.814873</td>\n",
       "      <td>0.827532</td>\n",
       "      <td>0.801407</td>\n",
       "      <td>0.820614</td>\n",
       "      <td>0.806907</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.793899</td>\n",
       "      <td>0.808971</td>\n",
       "      <td>0.809878</td>\n",
       "      <td>0.810345</td>\n",
       "      <td>0.810144</td>\n",
       "      <td>0.008923</td>\n",
       "      <td>20</td>\n",
       "      <td>0.816483</td>\n",
       "      <td>0.816394</td>\n",
       "      <td>0.817275</td>\n",
       "      <td>0.817017</td>\n",
       "      <td>0.818972</td>\n",
       "      <td>0.817725</td>\n",
       "      <td>0.817123</td>\n",
       "      <td>0.816743</td>\n",
       "      <td>0.818582</td>\n",
       "      <td>0.816303</td>\n",
       "      <td>0.817262</td>\n",
       "      <td>0.000867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "14      31.785872      0.286845         1.997381        0.067683   \n",
       "8      252.530001      1.001709        26.464286        0.475152   \n",
       "13      11.857570      0.060254         1.119470        0.017520   \n",
       "1      235.351550      5.165870        25.773658        0.816844   \n",
       "15      32.017736      0.424411         2.087793        0.033162   \n",
       "18     227.387620     35.132097        21.169703        7.060336   \n",
       "4       31.758895      0.151905         2.151341        0.038139   \n",
       "16     252.265632      1.684677        26.957509        0.342728   \n",
       "3       37.121524      0.353529         2.430551        0.059831   \n",
       "12      25.720951      0.083602         1.559441        0.017758   \n",
       "5      256.893795      1.417640        27.027909        0.554721   \n",
       "6       10.569095      0.088059         1.080440        0.017520   \n",
       "10      28.849953      0.202854         1.745739        0.042130   \n",
       "0      265.263951     13.972791        27.285560        0.624821   \n",
       "11      10.883897      0.051391         1.111758        0.020170   \n",
       "17      25.326829      0.283038         1.507646        0.063554   \n",
       "19      11.551335      0.101160         1.127901        0.084802   \n",
       "7       24.985783      0.168056         1.534324        0.083687   \n",
       "2       45.815221      0.998658         2.750689        0.063005   \n",
       "9      232.212803      2.107050        25.703045        0.415816   \n",
       "\n",
       "   param_vec__stem param_vec__ngram_range param_vec__min_df param_vec__max_df  \\\n",
       "14           False                 (1, 2)               100              4000   \n",
       "8             True                 (2, 2)                 3                 1   \n",
       "13           False                 (1, 1)                50             40000   \n",
       "1             True                 (1, 1)                50             40000   \n",
       "15           False                 (1, 2)               100                 1   \n",
       "18            True                 (1, 2)               400             40000   \n",
       "4            False                 (1, 2)                50              4000   \n",
       "16            True                 (1, 2)               400              4000   \n",
       "3            False                 (1, 2)                 3              4000   \n",
       "12           False                 (2, 2)                50                 1   \n",
       "5             True                 (1, 2)                 3              4000   \n",
       "6            False                 (1, 1)               100              4000   \n",
       "10           False                 (2, 2)                 3              4000   \n",
       "0             True                 (1, 1)                 1              4000   \n",
       "11           False                 (1, 1)                50              4000   \n",
       "17           False                 (2, 2)               100             40000   \n",
       "19           False                 (1, 1)                 3                 1   \n",
       "7            False                 (2, 2)               100              4000   \n",
       "2            False                 (1, 2)                 1                 1   \n",
       "9             True                 (1, 1)                 1             40000   \n",
       "\n",
       "   param_tfidf__smooth_idf param_tfidf__norm param_clf__fit_intercept  \\\n",
       "14                   False                l2                    False   \n",
       "8                     True                l2                     True   \n",
       "13                   False                l2                     True   \n",
       "1                     True                l2                     True   \n",
       "15                   False                l2                    False   \n",
       "18                    True                l2                    False   \n",
       "4                    False                l1                    False   \n",
       "16                    True                l2                    False   \n",
       "3                     True                l1                     True   \n",
       "12                   False                l2                    False   \n",
       "5                     True                l1                    False   \n",
       "6                     True                l1                    False   \n",
       "10                    True                l1                     True   \n",
       "0                    False                l1                     True   \n",
       "11                   False                l1                     True   \n",
       "17                    True                l2                     True   \n",
       "19                    True                l1                    False   \n",
       "7                     True                l1                     True   \n",
       "2                    False                l1                     True   \n",
       "9                     True                l1                    False   \n",
       "\n",
       "   param_clf__C                                             params  \\\n",
       "14          2.5  {'vec__stem': False, 'vec__ngram_range': (1, 2...   \n",
       "8           2.5  {'vec__stem': True, 'vec__ngram_range': (2, 2)...   \n",
       "13            3  {'vec__stem': False, 'vec__ngram_range': (1, 1...   \n",
       "1             3  {'vec__stem': True, 'vec__ngram_range': (1, 1)...   \n",
       "15          0.5  {'vec__stem': False, 'vec__ngram_range': (1, 2...   \n",
       "18            1  {'vec__stem': True, 'vec__ngram_range': (1, 2)...   \n",
       "4             3  {'vec__stem': False, 'vec__ngram_range': (1, 2...   \n",
       "16            1  {'vec__stem': True, 'vec__ngram_range': (1, 2)...   \n",
       "3           2.5  {'vec__stem': False, 'vec__ngram_range': (1, 2...   \n",
       "12            2  {'vec__stem': False, 'vec__ngram_range': (2, 2...   \n",
       "5             1  {'vec__stem': True, 'vec__ngram_range': (1, 2)...   \n",
       "6             3  {'vec__stem': False, 'vec__ngram_range': (1, 1...   \n",
       "10            2  {'vec__stem': False, 'vec__ngram_range': (2, 2...   \n",
       "0             2  {'vec__stem': True, 'vec__ngram_range': (1, 1)...   \n",
       "11          0.5  {'vec__stem': False, 'vec__ngram_range': (1, 1...   \n",
       "17          2.5  {'vec__stem': False, 'vec__ngram_range': (2, 2...   \n",
       "19          2.5  {'vec__stem': False, 'vec__ngram_range': (1, 1...   \n",
       "7           0.5  {'vec__stem': False, 'vec__ngram_range': (2, 2...   \n",
       "2             2  {'vec__stem': False, 'vec__ngram_range': (1, 2...   \n",
       "9           0.5  {'vec__stem': True, 'vec__ngram_range': (1, 1)...   \n",
       "\n",
       "    split0_test_score  split1_test_score  split2_test_score  \\\n",
       "14           0.901186           0.906613           0.895817   \n",
       "8            0.895924           0.897943           0.881383   \n",
       "13           0.898930           0.894042           0.890205   \n",
       "1            0.892928           0.889509           0.888976   \n",
       "15           0.892758           0.896026           0.879086   \n",
       "18           0.879874           0.881342           0.871614   \n",
       "4            0.873072           0.875693           0.868072   \n",
       "16           0.874016           0.877221           0.869667   \n",
       "3            0.872868           0.874660           0.867101   \n",
       "12           0.867460           0.873453           0.859144   \n",
       "5            0.874025           0.871458           0.864558   \n",
       "6            0.870283           0.868400           0.860465   \n",
       "10           0.851047           0.866641           0.848672   \n",
       "0            0.865303           0.860662           0.852807   \n",
       "11           0.856481           0.851080           0.849315   \n",
       "17           0.850475           0.858167           0.848057   \n",
       "19           0.840062           0.845732           0.825952   \n",
       "7            0.820087           0.839731           0.824214   \n",
       "2            0.810117           0.830469           0.802185   \n",
       "9            0.814873           0.827532           0.801407   \n",
       "\n",
       "    split3_test_score  split4_test_score  split5_test_score  \\\n",
       "14           0.892320           0.894548           0.885635   \n",
       "8            0.887658           0.884446           0.891989   \n",
       "13           0.888801           0.887735           0.883353   \n",
       "1            0.885624           0.884075           0.874162   \n",
       "15           0.885805           0.887828           0.878416   \n",
       "18           0.877373           0.875650           0.864715   \n",
       "4            0.868599           0.873317           0.866878   \n",
       "16           0.864158           0.877543           0.859621   \n",
       "3            0.867545           0.876596           0.862822   \n",
       "12           0.860649           0.860567           0.867894   \n",
       "5            0.860916           0.865512           0.860197   \n",
       "6            0.851618           0.866745           0.852317   \n",
       "10           0.853592           0.854920           0.858142   \n",
       "0            0.847994           0.860331           0.845798   \n",
       "11           0.846569           0.851568           0.838536   \n",
       "17           0.844869           0.836422           0.845229   \n",
       "19           0.841449           0.834877           0.826138   \n",
       "7            0.820047           0.821019           0.829753   \n",
       "2            0.823391           0.809119           0.811313   \n",
       "9            0.820614           0.806907           0.807018   \n",
       "\n",
       "    split6_test_score  split7_test_score  split8_test_score  \\\n",
       "14           0.883117           0.903251           0.892332   \n",
       "8            0.881064           0.892758           0.895110   \n",
       "13           0.876582           0.886680           0.882797   \n",
       "1            0.874507           0.886867           0.887569   \n",
       "15           0.872399           0.888536           0.883241   \n",
       "18           0.867821           0.881933           0.873849   \n",
       "4            0.857592           0.879365           0.871146   \n",
       "16           0.859621           0.872713           0.874601   \n",
       "3            0.854839           0.876947           0.872671   \n",
       "12           0.861961           0.876898           0.859635   \n",
       "5            0.844664           0.869205           0.867184   \n",
       "6            0.846245           0.864886           0.866588   \n",
       "10           0.844615           0.864717           0.855810   \n",
       "0            0.840613           0.858810           0.861989   \n",
       "11           0.833909           0.853044           0.857364   \n",
       "17           0.843383           0.858404           0.850337   \n",
       "19           0.824750           0.830975           0.832241   \n",
       "7            0.816199           0.831816           0.822319   \n",
       "2            0.801237           0.815987           0.809375   \n",
       "9            0.793899           0.808971           0.809878   \n",
       "\n",
       "    split9_test_score  mean_test_score  std_test_score  rank_test_score  \\\n",
       "14           0.898643         0.895346        0.007052                1   \n",
       "8            0.898447         0.890672        0.006270                2   \n",
       "13           0.893012         0.888214        0.006058                3   \n",
       "1            0.894966         0.885918        0.006548                4   \n",
       "15           0.881543         0.884564        0.006732                5   \n",
       "18           0.874803         0.874897        0.005368                6   \n",
       "4            0.868944         0.870268        0.005583                7   \n",
       "16           0.873072         0.870223        0.006421                8   \n",
       "3            0.869024         0.869507        0.006505                9   \n",
       "12           0.869148         0.865681        0.005924               10   \n",
       "5            0.855964         0.863368        0.008089               11   \n",
       "6            0.867687         0.861523        0.008023               12   \n",
       "10           0.851708         0.854987        0.006456               13   \n",
       "0            0.852611         0.854692        0.007618               14   \n",
       "11           0.848111         0.848598        0.007040               15   \n",
       "17           0.844711         0.848005        0.006379               16   \n",
       "19           0.837607         0.833978        0.006841               17   \n",
       "7            0.816310         0.824150        0.007100               18   \n",
       "2            0.808112         0.812131        0.008562               19   \n",
       "9            0.810345         0.810144        0.008923               20   \n",
       "\n",
       "    split0_train_score  split1_train_score  split2_train_score  \\\n",
       "14            0.948263            0.949046            0.948518   \n",
       "8             0.987338            0.988704            0.987783   \n",
       "13            0.934394            0.934399            0.935233   \n",
       "1             0.926562            0.928436            0.927657   \n",
       "15            0.909443            0.907923            0.909866   \n",
       "18            0.895291            0.893376            0.894767   \n",
       "4             0.885394            0.884994            0.885767   \n",
       "16            0.893104            0.893063            0.894436   \n",
       "3             0.892433            0.893998            0.894150   \n",
       "12            0.938387            0.936569            0.937459   \n",
       "5             0.885483            0.886482            0.886941   \n",
       "6             0.873370            0.874133            0.874891   \n",
       "10            0.894580            0.893817            0.895324   \n",
       "0             0.868340            0.869127            0.869714   \n",
       "11            0.857155            0.857949            0.858231   \n",
       "17            0.907862            0.906082            0.907545   \n",
       "19            0.843728            0.844757            0.846277   \n",
       "7             0.838986            0.837016            0.837943   \n",
       "2             0.834398            0.833092            0.834002   \n",
       "9             0.816483            0.816394            0.817275   \n",
       "\n",
       "    split3_train_score  split4_train_score  split5_train_score  \\\n",
       "14            0.948519            0.948216            0.948183   \n",
       "8             0.988128            0.987906            0.987913   \n",
       "13            0.935303            0.934011            0.935261   \n",
       "1             0.928455            0.928174            0.929196   \n",
       "15            0.909635            0.909555            0.910429   \n",
       "18            0.894157            0.894333            0.894665   \n",
       "4             0.886080            0.885455            0.884806   \n",
       "16            0.894980            0.894086            0.894450   \n",
       "3             0.893688            0.893267            0.893915   \n",
       "12            0.937939            0.937975            0.936563   \n",
       "5             0.887506            0.886144            0.886329   \n",
       "6             0.875524            0.874580            0.875437   \n",
       "10            0.894775            0.894024            0.894074   \n",
       "0             0.872280            0.869223            0.870105   \n",
       "11            0.858832            0.860366            0.858047   \n",
       "17            0.908448            0.907614            0.906573   \n",
       "19            0.844758            0.845107            0.845388   \n",
       "7             0.837507            0.838164            0.836942   \n",
       "2             0.834786            0.834562            0.834173   \n",
       "9             0.817017            0.818972            0.817725   \n",
       "\n",
       "    split6_train_score  split7_train_score  split8_train_score  \\\n",
       "14            0.949060            0.947201            0.948262   \n",
       "8             0.987604            0.987150            0.988095   \n",
       "13            0.934873            0.935120            0.935144   \n",
       "1             0.930041            0.928278            0.928935   \n",
       "15            0.909019            0.907810            0.909788   \n",
       "18            0.895187            0.894739            0.895877   \n",
       "4             0.885861            0.883866            0.884769   \n",
       "16            0.893813            0.892558            0.894301   \n",
       "3             0.893108            0.891587            0.892304   \n",
       "12            0.937149            0.937276            0.938145   \n",
       "5             0.886271            0.884889            0.886570   \n",
       "6             0.876675            0.873629            0.872422   \n",
       "10            0.893159            0.892481            0.894805   \n",
       "0             0.870473            0.869230            0.868977   \n",
       "11            0.861208            0.857791            0.856286   \n",
       "17            0.906986            0.906558            0.907590   \n",
       "19            0.845085            0.844892            0.845258   \n",
       "7             0.838334            0.837199            0.837860   \n",
       "2             0.832723            0.832978            0.835115   \n",
       "9             0.817123            0.816743            0.818582   \n",
       "\n",
       "    split9_train_score  mean_train_score  std_train_score  \n",
       "14            0.948207          0.948348         0.000495  \n",
       "8             0.987423          0.987804         0.000430  \n",
       "13            0.934095          0.934783         0.000482  \n",
       "1             0.926838          0.928257         0.000992  \n",
       "15            0.908067          0.909153         0.000868  \n",
       "18            0.894746          0.894714         0.000645  \n",
       "4             0.884162          0.885116         0.000693  \n",
       "16            0.893376          0.893817         0.000728  \n",
       "3             0.892833          0.893128         0.000799  \n",
       "12            0.937840          0.937530         0.000604  \n",
       "5             0.886135          0.886275         0.000684  \n",
       "6             0.873142          0.874380         0.001225  \n",
       "10            0.894111          0.894115         0.000792  \n",
       "0             0.868914          0.869638         0.001056  \n",
       "11            0.857744          0.858361         0.001382  \n",
       "17            0.905464          0.907072         0.000860  \n",
       "19            0.844321          0.844957         0.000637  \n",
       "7             0.838859          0.837881         0.000686  \n",
       "2             0.832921          0.833875         0.000829  \n",
       "9             0.816303          0.817262         0.000867  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(rs_regression_tfidf.cv_results_).sort_values(by=['mean_test_score'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cuent/anaconda/envs/comp551-proj2/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9610.997161865234,\n",
       " {'vec__stem': False,\n",
       "  'vec__ngram_range': (1, 2),\n",
       "  'vec__min_df': 1,\n",
       "  'vec__max_df': 400,\n",
       "  'clf__fit_intercept': False,\n",
       "  'clf__C': 0.5},\n",
       " 0.9040914014287277)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_regression_bow = {\n",
    "    'vec__min_df': [3, 50, 100, 400, 1],\n",
    "    'vec__max_df': [400, 4000, 40000, 1.0],\n",
    "    'vec__stem': [True, False],\n",
    "    'vec__ngram_range':[(1, 1), (1, 2), (2, 2)],\n",
    "    'clf__fit_intercept': [True, False], \n",
    "    'clf__C': [.5, 1, 2, 2.5, 3], \n",
    "}\n",
    "\n",
    "pipeline_regression_bow = Pipeline([\n",
    "    ('vec', LemmaCountVectorizer(strip_accents='unicode', stop_words=None)),\n",
    "    ('clf', LogisticRegression(solver='saga', penalty='l2', max_iter=200))\n",
    "])\n",
    "                  \n",
    "rs_regression_bow = RandomizedSearchCV(pipeline_regression_bow, parameters_regression_bow, \n",
    "                                   cv=15, scoring=score, n_jobs=-1, verbose=0, random_state=62, n_iter=20,\n",
    "                                      return_train_score=True)\n",
    "start = time.time()\n",
    "rs_regression_bow.fit(X_train, y)\n",
    "time.time() - start, rs_regression_bow.best_params_, rs_regression_bow.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_vec__stem</th>\n",
       "      <th>param_vec__ngram_range</th>\n",
       "      <th>param_vec__min_df</th>\n",
       "      <th>param_vec__max_df</th>\n",
       "      <th>param_clf__fit_intercept</th>\n",
       "      <th>param_clf__C</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>split10_test_score</th>\n",
       "      <th>split11_test_score</th>\n",
       "      <th>split12_test_score</th>\n",
       "      <th>split13_test_score</th>\n",
       "      <th>split14_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>split5_train_score</th>\n",
       "      <th>split6_train_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>split8_train_score</th>\n",
       "      <th>split9_train_score</th>\n",
       "      <th>split10_train_score</th>\n",
       "      <th>split11_train_score</th>\n",
       "      <th>split12_train_score</th>\n",
       "      <th>split13_train_score</th>\n",
       "      <th>split14_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>150.317218</td>\n",
       "      <td>5.914168</td>\n",
       "      <td>1.549478</td>\n",
       "      <td>0.046928</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'vec__stem': False, 'vec__ngram_range': (1, 2...</td>\n",
       "      <td>0.908218</td>\n",
       "      <td>0.907484</td>\n",
       "      <td>0.903529</td>\n",
       "      <td>0.907046</td>\n",
       "      <td>0.889152</td>\n",
       "      <td>0.918404</td>\n",
       "      <td>0.902728</td>\n",
       "      <td>0.905592</td>\n",
       "      <td>0.903730</td>\n",
       "      <td>0.896469</td>\n",
       "      <td>0.888758</td>\n",
       "      <td>0.905136</td>\n",
       "      <td>0.912552</td>\n",
       "      <td>0.896510</td>\n",
       "      <td>0.916067</td>\n",
       "      <td>0.904091</td>\n",
       "      <td>0.008313</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998199</td>\n",
       "      <td>0.998328</td>\n",
       "      <td>0.999614</td>\n",
       "      <td>0.998114</td>\n",
       "      <td>0.998199</td>\n",
       "      <td>0.997985</td>\n",
       "      <td>0.998027</td>\n",
       "      <td>0.998199</td>\n",
       "      <td>0.997942</td>\n",
       "      <td>0.998242</td>\n",
       "      <td>0.998199</td>\n",
       "      <td>0.998157</td>\n",
       "      <td>0.998242</td>\n",
       "      <td>0.998285</td>\n",
       "      <td>0.998156</td>\n",
       "      <td>0.998259</td>\n",
       "      <td>0.000377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>305.350994</td>\n",
       "      <td>2.414326</td>\n",
       "      <td>18.169974</td>\n",
       "      <td>0.259414</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>True</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'vec__stem': True, 'vec__ngram_range': (1, 2)...</td>\n",
       "      <td>0.901561</td>\n",
       "      <td>0.899822</td>\n",
       "      <td>0.886136</td>\n",
       "      <td>0.899115</td>\n",
       "      <td>0.887972</td>\n",
       "      <td>0.903651</td>\n",
       "      <td>0.894923</td>\n",
       "      <td>0.898325</td>\n",
       "      <td>0.899522</td>\n",
       "      <td>0.895238</td>\n",
       "      <td>0.890468</td>\n",
       "      <td>0.902483</td>\n",
       "      <td>0.909846</td>\n",
       "      <td>0.894484</td>\n",
       "      <td>0.911712</td>\n",
       "      <td>0.898349</td>\n",
       "      <td>0.006956</td>\n",
       "      <td>2</td>\n",
       "      <td>0.996614</td>\n",
       "      <td>0.996956</td>\n",
       "      <td>0.998971</td>\n",
       "      <td>0.996871</td>\n",
       "      <td>0.996058</td>\n",
       "      <td>0.996613</td>\n",
       "      <td>0.996313</td>\n",
       "      <td>0.996527</td>\n",
       "      <td>0.996312</td>\n",
       "      <td>0.996613</td>\n",
       "      <td>0.996742</td>\n",
       "      <td>0.996570</td>\n",
       "      <td>0.996400</td>\n",
       "      <td>0.996870</td>\n",
       "      <td>0.996570</td>\n",
       "      <td>0.996733</td>\n",
       "      <td>0.000641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>304.270236</td>\n",
       "      <td>36.747755</td>\n",
       "      <td>16.246507</td>\n",
       "      <td>2.493766</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'vec__stem': True, 'vec__ngram_range': (1, 2)...</td>\n",
       "      <td>0.897983</td>\n",
       "      <td>0.895452</td>\n",
       "      <td>0.902031</td>\n",
       "      <td>0.881899</td>\n",
       "      <td>0.884706</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>0.894454</td>\n",
       "      <td>0.888352</td>\n",
       "      <td>0.878951</td>\n",
       "      <td>0.887433</td>\n",
       "      <td>0.879483</td>\n",
       "      <td>0.899584</td>\n",
       "      <td>0.893491</td>\n",
       "      <td>0.890476</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.891390</td>\n",
       "      <td>0.007357</td>\n",
       "      <td>3</td>\n",
       "      <td>0.939167</td>\n",
       "      <td>0.939461</td>\n",
       "      <td>0.945124</td>\n",
       "      <td>0.939978</td>\n",
       "      <td>0.939601</td>\n",
       "      <td>0.939974</td>\n",
       "      <td>0.939602</td>\n",
       "      <td>0.939257</td>\n",
       "      <td>0.940207</td>\n",
       "      <td>0.938970</td>\n",
       "      <td>0.939465</td>\n",
       "      <td>0.938558</td>\n",
       "      <td>0.940242</td>\n",
       "      <td>0.939562</td>\n",
       "      <td>0.938836</td>\n",
       "      <td>0.939867</td>\n",
       "      <td>0.001481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>296.678784</td>\n",
       "      <td>2.133247</td>\n",
       "      <td>17.782592</td>\n",
       "      <td>0.311741</td>\n",
       "      <td>True</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>3</td>\n",
       "      <td>40000</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>{'vec__stem': True, 'vec__ngram_range': (2, 2)...</td>\n",
       "      <td>0.886512</td>\n",
       "      <td>0.881556</td>\n",
       "      <td>0.899170</td>\n",
       "      <td>0.880615</td>\n",
       "      <td>0.882111</td>\n",
       "      <td>0.885324</td>\n",
       "      <td>0.874925</td>\n",
       "      <td>0.881928</td>\n",
       "      <td>0.881965</td>\n",
       "      <td>0.883194</td>\n",
       "      <td>0.872491</td>\n",
       "      <td>0.895147</td>\n",
       "      <td>0.881936</td>\n",
       "      <td>0.884638</td>\n",
       "      <td>0.897638</td>\n",
       "      <td>0.884611</td>\n",
       "      <td>0.007268</td>\n",
       "      <td>4</td>\n",
       "      <td>0.997600</td>\n",
       "      <td>0.997086</td>\n",
       "      <td>0.997172</td>\n",
       "      <td>0.997300</td>\n",
       "      <td>0.998071</td>\n",
       "      <td>0.997043</td>\n",
       "      <td>0.997042</td>\n",
       "      <td>0.997301</td>\n",
       "      <td>0.997515</td>\n",
       "      <td>0.997258</td>\n",
       "      <td>0.997215</td>\n",
       "      <td>0.997300</td>\n",
       "      <td>0.997172</td>\n",
       "      <td>0.997301</td>\n",
       "      <td>0.997214</td>\n",
       "      <td>0.997306</td>\n",
       "      <td>0.000253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>121.177887</td>\n",
       "      <td>14.089188</td>\n",
       "      <td>1.175479</td>\n",
       "      <td>0.039654</td>\n",
       "      <td>False</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>1</td>\n",
       "      <td>4000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'vec__stem': False, 'vec__ngram_range': (2, 2...</td>\n",
       "      <td>0.879290</td>\n",
       "      <td>0.882701</td>\n",
       "      <td>0.893668</td>\n",
       "      <td>0.870337</td>\n",
       "      <td>0.882147</td>\n",
       "      <td>0.884638</td>\n",
       "      <td>0.880526</td>\n",
       "      <td>0.881579</td>\n",
       "      <td>0.876499</td>\n",
       "      <td>0.880995</td>\n",
       "      <td>0.878592</td>\n",
       "      <td>0.895397</td>\n",
       "      <td>0.878538</td>\n",
       "      <td>0.875895</td>\n",
       "      <td>0.892512</td>\n",
       "      <td>0.882221</td>\n",
       "      <td>0.006687</td>\n",
       "      <td>5</td>\n",
       "      <td>0.999057</td>\n",
       "      <td>0.999271</td>\n",
       "      <td>0.999657</td>\n",
       "      <td>0.999100</td>\n",
       "      <td>0.999057</td>\n",
       "      <td>0.999100</td>\n",
       "      <td>0.999271</td>\n",
       "      <td>0.999143</td>\n",
       "      <td>0.999057</td>\n",
       "      <td>0.999057</td>\n",
       "      <td>0.999143</td>\n",
       "      <td>0.999143</td>\n",
       "      <td>0.999272</td>\n",
       "      <td>0.999229</td>\n",
       "      <td>0.999272</td>\n",
       "      <td>0.999189</td>\n",
       "      <td>0.000150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>262.217608</td>\n",
       "      <td>3.135966</td>\n",
       "      <td>17.414064</td>\n",
       "      <td>0.351692</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>{'vec__stem': True, 'vec__ngram_range': (1, 1)...</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.891124</td>\n",
       "      <td>0.882601</td>\n",
       "      <td>0.874484</td>\n",
       "      <td>0.880047</td>\n",
       "      <td>0.892985</td>\n",
       "      <td>0.893642</td>\n",
       "      <td>0.870192</td>\n",
       "      <td>0.852184</td>\n",
       "      <td>0.882775</td>\n",
       "      <td>0.872066</td>\n",
       "      <td>0.882562</td>\n",
       "      <td>0.881416</td>\n",
       "      <td>0.877381</td>\n",
       "      <td>0.884198</td>\n",
       "      <td>0.879908</td>\n",
       "      <td>0.009965</td>\n",
       "      <td>6</td>\n",
       "      <td>0.905072</td>\n",
       "      <td>0.904086</td>\n",
       "      <td>0.908658</td>\n",
       "      <td>0.904742</td>\n",
       "      <td>0.904594</td>\n",
       "      <td>0.904632</td>\n",
       "      <td>0.904501</td>\n",
       "      <td>0.904930</td>\n",
       "      <td>0.906285</td>\n",
       "      <td>0.905566</td>\n",
       "      <td>0.905458</td>\n",
       "      <td>0.904432</td>\n",
       "      <td>0.905630</td>\n",
       "      <td>0.905112</td>\n",
       "      <td>0.904034</td>\n",
       "      <td>0.905182</td>\n",
       "      <td>0.001101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>256.497799</td>\n",
       "      <td>2.358328</td>\n",
       "      <td>17.330120</td>\n",
       "      <td>0.256446</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>1</td>\n",
       "      <td>4000</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>{'vec__stem': True, 'vec__ngram_range': (1, 1)...</td>\n",
       "      <td>0.891136</td>\n",
       "      <td>0.889284</td>\n",
       "      <td>0.879290</td>\n",
       "      <td>0.887441</td>\n",
       "      <td>0.869718</td>\n",
       "      <td>0.884410</td>\n",
       "      <td>0.879762</td>\n",
       "      <td>0.866505</td>\n",
       "      <td>0.857992</td>\n",
       "      <td>0.877698</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.877872</td>\n",
       "      <td>0.876258</td>\n",
       "      <td>0.869617</td>\n",
       "      <td>0.895522</td>\n",
       "      <td>0.877312</td>\n",
       "      <td>0.011088</td>\n",
       "      <td>7</td>\n",
       "      <td>0.942773</td>\n",
       "      <td>0.942597</td>\n",
       "      <td>0.963045</td>\n",
       "      <td>0.942578</td>\n",
       "      <td>0.942336</td>\n",
       "      <td>0.944243</td>\n",
       "      <td>0.942552</td>\n",
       "      <td>0.942782</td>\n",
       "      <td>0.943366</td>\n",
       "      <td>0.943341</td>\n",
       "      <td>0.943918</td>\n",
       "      <td>0.942868</td>\n",
       "      <td>0.942653</td>\n",
       "      <td>0.944122</td>\n",
       "      <td>0.942517</td>\n",
       "      <td>0.944380</td>\n",
       "      <td>0.005024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>25.090355</td>\n",
       "      <td>0.428465</td>\n",
       "      <td>0.741297</td>\n",
       "      <td>0.023155</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>100</td>\n",
       "      <td>40000</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>{'vec__stem': False, 'vec__ngram_range': (1, 1...</td>\n",
       "      <td>0.882562</td>\n",
       "      <td>0.887307</td>\n",
       "      <td>0.879713</td>\n",
       "      <td>0.871674</td>\n",
       "      <td>0.875222</td>\n",
       "      <td>0.877297</td>\n",
       "      <td>0.892069</td>\n",
       "      <td>0.872315</td>\n",
       "      <td>0.859012</td>\n",
       "      <td>0.876941</td>\n",
       "      <td>0.860834</td>\n",
       "      <td>0.881094</td>\n",
       "      <td>0.876033</td>\n",
       "      <td>0.867024</td>\n",
       "      <td>0.891136</td>\n",
       "      <td>0.876683</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>8</td>\n",
       "      <td>0.896227</td>\n",
       "      <td>0.895533</td>\n",
       "      <td>0.898689</td>\n",
       "      <td>0.897070</td>\n",
       "      <td>0.896845</td>\n",
       "      <td>0.895905</td>\n",
       "      <td>0.894788</td>\n",
       "      <td>0.895430</td>\n",
       "      <td>0.898084</td>\n",
       "      <td>0.896473</td>\n",
       "      <td>0.897056</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>0.896786</td>\n",
       "      <td>0.895840</td>\n",
       "      <td>0.894994</td>\n",
       "      <td>0.896418</td>\n",
       "      <td>0.001039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>256.005480</td>\n",
       "      <td>2.648494</td>\n",
       "      <td>17.132976</td>\n",
       "      <td>0.332700</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>100</td>\n",
       "      <td>40000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'vec__stem': True, 'vec__ngram_range': (1, 1)...</td>\n",
       "      <td>0.879382</td>\n",
       "      <td>0.885866</td>\n",
       "      <td>0.878460</td>\n",
       "      <td>0.868901</td>\n",
       "      <td>0.874927</td>\n",
       "      <td>0.884935</td>\n",
       "      <td>0.886525</td>\n",
       "      <td>0.867427</td>\n",
       "      <td>0.847943</td>\n",
       "      <td>0.877758</td>\n",
       "      <td>0.866706</td>\n",
       "      <td>0.881235</td>\n",
       "      <td>0.879056</td>\n",
       "      <td>0.875669</td>\n",
       "      <td>0.880967</td>\n",
       "      <td>0.875718</td>\n",
       "      <td>0.009547</td>\n",
       "      <td>9</td>\n",
       "      <td>0.895488</td>\n",
       "      <td>0.895003</td>\n",
       "      <td>0.897472</td>\n",
       "      <td>0.896499</td>\n",
       "      <td>0.895980</td>\n",
       "      <td>0.896066</td>\n",
       "      <td>0.895448</td>\n",
       "      <td>0.896321</td>\n",
       "      <td>0.897503</td>\n",
       "      <td>0.895913</td>\n",
       "      <td>0.896704</td>\n",
       "      <td>0.895005</td>\n",
       "      <td>0.896394</td>\n",
       "      <td>0.895606</td>\n",
       "      <td>0.895483</td>\n",
       "      <td>0.896059</td>\n",
       "      <td>0.000747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>47.017657</td>\n",
       "      <td>0.184964</td>\n",
       "      <td>1.333746</td>\n",
       "      <td>0.033409</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>400</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>{'vec__stem': False, 'vec__ngram_range': (1, 2...</td>\n",
       "      <td>0.872278</td>\n",
       "      <td>0.872216</td>\n",
       "      <td>0.878951</td>\n",
       "      <td>0.856637</td>\n",
       "      <td>0.869667</td>\n",
       "      <td>0.870184</td>\n",
       "      <td>0.877527</td>\n",
       "      <td>0.866387</td>\n",
       "      <td>0.853906</td>\n",
       "      <td>0.863962</td>\n",
       "      <td>0.853428</td>\n",
       "      <td>0.874109</td>\n",
       "      <td>0.869410</td>\n",
       "      <td>0.859356</td>\n",
       "      <td>0.874479</td>\n",
       "      <td>0.867501</td>\n",
       "      <td>0.008023</td>\n",
       "      <td>10</td>\n",
       "      <td>0.885253</td>\n",
       "      <td>0.886023</td>\n",
       "      <td>0.887117</td>\n",
       "      <td>0.886702</td>\n",
       "      <td>0.886446</td>\n",
       "      <td>0.885637</td>\n",
       "      <td>0.885062</td>\n",
       "      <td>0.885508</td>\n",
       "      <td>0.886561</td>\n",
       "      <td>0.887428</td>\n",
       "      <td>0.886354</td>\n",
       "      <td>0.886242</td>\n",
       "      <td>0.887277</td>\n",
       "      <td>0.886948</td>\n",
       "      <td>0.885264</td>\n",
       "      <td>0.886255</td>\n",
       "      <td>0.000748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>38.032737</td>\n",
       "      <td>0.238129</td>\n",
       "      <td>1.030999</td>\n",
       "      <td>0.032324</td>\n",
       "      <td>False</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>50</td>\n",
       "      <td>40000</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>{'vec__stem': False, 'vec__ngram_range': (2, 2...</td>\n",
       "      <td>0.869462</td>\n",
       "      <td>0.849821</td>\n",
       "      <td>0.875075</td>\n",
       "      <td>0.853746</td>\n",
       "      <td>0.850588</td>\n",
       "      <td>0.862582</td>\n",
       "      <td>0.852321</td>\n",
       "      <td>0.858177</td>\n",
       "      <td>0.855596</td>\n",
       "      <td>0.854944</td>\n",
       "      <td>0.857814</td>\n",
       "      <td>0.875303</td>\n",
       "      <td>0.853919</td>\n",
       "      <td>0.851830</td>\n",
       "      <td>0.864897</td>\n",
       "      <td>0.859072</td>\n",
       "      <td>0.008230</td>\n",
       "      <td>11</td>\n",
       "      <td>0.959206</td>\n",
       "      <td>0.959406</td>\n",
       "      <td>0.959135</td>\n",
       "      <td>0.960164</td>\n",
       "      <td>0.967538</td>\n",
       "      <td>0.959030</td>\n",
       "      <td>0.959532</td>\n",
       "      <td>0.958476</td>\n",
       "      <td>0.958814</td>\n",
       "      <td>0.960082</td>\n",
       "      <td>0.959098</td>\n",
       "      <td>0.958780</td>\n",
       "      <td>0.958410</td>\n",
       "      <td>0.959051</td>\n",
       "      <td>0.957620</td>\n",
       "      <td>0.959623</td>\n",
       "      <td>0.002202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>268.788099</td>\n",
       "      <td>1.290587</td>\n",
       "      <td>17.479432</td>\n",
       "      <td>0.345357</td>\n",
       "      <td>True</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>50</td>\n",
       "      <td>4000</td>\n",
       "      <td>True</td>\n",
       "      <td>2.5</td>\n",
       "      <td>{'vec__stem': True, 'vec__ngram_range': (2, 2)...</td>\n",
       "      <td>0.862028</td>\n",
       "      <td>0.851391</td>\n",
       "      <td>0.867700</td>\n",
       "      <td>0.854762</td>\n",
       "      <td>0.851039</td>\n",
       "      <td>0.856629</td>\n",
       "      <td>0.851449</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.853892</td>\n",
       "      <td>0.856805</td>\n",
       "      <td>0.857649</td>\n",
       "      <td>0.873341</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.845411</td>\n",
       "      <td>0.870987</td>\n",
       "      <td>0.858117</td>\n",
       "      <td>0.007535</td>\n",
       "      <td>12</td>\n",
       "      <td>0.978126</td>\n",
       "      <td>0.979784</td>\n",
       "      <td>0.989374</td>\n",
       "      <td>0.979616</td>\n",
       "      <td>0.978842</td>\n",
       "      <td>0.978709</td>\n",
       "      <td>0.978627</td>\n",
       "      <td>0.978195</td>\n",
       "      <td>0.978616</td>\n",
       "      <td>0.978702</td>\n",
       "      <td>0.978231</td>\n",
       "      <td>0.979029</td>\n",
       "      <td>0.978056</td>\n",
       "      <td>0.978804</td>\n",
       "      <td>0.978640</td>\n",
       "      <td>0.979423</td>\n",
       "      <td>0.002701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>34.886632</td>\n",
       "      <td>0.315058</td>\n",
       "      <td>0.994251</td>\n",
       "      <td>0.035305</td>\n",
       "      <td>False</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>100</td>\n",
       "      <td>40000</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>{'vec__stem': False, 'vec__ngram_range': (2, 2...</td>\n",
       "      <td>0.846199</td>\n",
       "      <td>0.833432</td>\n",
       "      <td>0.852695</td>\n",
       "      <td>0.834231</td>\n",
       "      <td>0.832254</td>\n",
       "      <td>0.840787</td>\n",
       "      <td>0.839297</td>\n",
       "      <td>0.838438</td>\n",
       "      <td>0.843693</td>\n",
       "      <td>0.833235</td>\n",
       "      <td>0.838366</td>\n",
       "      <td>0.856287</td>\n",
       "      <td>0.838978</td>\n",
       "      <td>0.832838</td>\n",
       "      <td>0.839759</td>\n",
       "      <td>0.840033</td>\n",
       "      <td>0.006925</td>\n",
       "      <td>13</td>\n",
       "      <td>0.917819</td>\n",
       "      <td>0.917161</td>\n",
       "      <td>0.918033</td>\n",
       "      <td>0.918147</td>\n",
       "      <td>0.923688</td>\n",
       "      <td>0.917634</td>\n",
       "      <td>0.917246</td>\n",
       "      <td>0.917906</td>\n",
       "      <td>0.916443</td>\n",
       "      <td>0.917199</td>\n",
       "      <td>0.918647</td>\n",
       "      <td>0.917743</td>\n",
       "      <td>0.916763</td>\n",
       "      <td>0.918677</td>\n",
       "      <td>0.918311</td>\n",
       "      <td>0.918094</td>\n",
       "      <td>0.001619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.851386</td>\n",
       "      <td>0.097297</td>\n",
       "      <td>0.724146</td>\n",
       "      <td>0.021940</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'vec__stem': False, 'vec__ngram_range': (1, 1...</td>\n",
       "      <td>0.841537</td>\n",
       "      <td>0.854265</td>\n",
       "      <td>0.856119</td>\n",
       "      <td>0.840528</td>\n",
       "      <td>0.818832</td>\n",
       "      <td>0.833534</td>\n",
       "      <td>0.837182</td>\n",
       "      <td>0.842797</td>\n",
       "      <td>0.841346</td>\n",
       "      <td>0.835033</td>\n",
       "      <td>0.839202</td>\n",
       "      <td>0.831216</td>\n",
       "      <td>0.846473</td>\n",
       "      <td>0.832335</td>\n",
       "      <td>0.842362</td>\n",
       "      <td>0.839519</td>\n",
       "      <td>0.008867</td>\n",
       "      <td>14</td>\n",
       "      <td>0.938953</td>\n",
       "      <td>0.939581</td>\n",
       "      <td>0.961992</td>\n",
       "      <td>0.939815</td>\n",
       "      <td>0.939838</td>\n",
       "      <td>0.940241</td>\n",
       "      <td>0.938953</td>\n",
       "      <td>0.940369</td>\n",
       "      <td>0.939348</td>\n",
       "      <td>0.939965</td>\n",
       "      <td>0.940777</td>\n",
       "      <td>0.939080</td>\n",
       "      <td>0.938707</td>\n",
       "      <td>0.941474</td>\n",
       "      <td>0.939757</td>\n",
       "      <td>0.941257</td>\n",
       "      <td>0.005588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>18.700022</td>\n",
       "      <td>0.223215</td>\n",
       "      <td>0.723858</td>\n",
       "      <td>0.023737</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>{'vec__stem': False, 'vec__ngram_range': (1, 1...</td>\n",
       "      <td>0.839136</td>\n",
       "      <td>0.850207</td>\n",
       "      <td>0.852537</td>\n",
       "      <td>0.840528</td>\n",
       "      <td>0.821002</td>\n",
       "      <td>0.829003</td>\n",
       "      <td>0.838174</td>\n",
       "      <td>0.840719</td>\n",
       "      <td>0.841346</td>\n",
       "      <td>0.836735</td>\n",
       "      <td>0.836171</td>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.841855</td>\n",
       "      <td>0.833533</td>\n",
       "      <td>0.840951</td>\n",
       "      <td>0.838100</td>\n",
       "      <td>0.007642</td>\n",
       "      <td>15</td>\n",
       "      <td>0.935796</td>\n",
       "      <td>0.936347</td>\n",
       "      <td>0.958630</td>\n",
       "      <td>0.937404</td>\n",
       "      <td>0.937693</td>\n",
       "      <td>0.938464</td>\n",
       "      <td>0.936265</td>\n",
       "      <td>0.937747</td>\n",
       "      <td>0.936167</td>\n",
       "      <td>0.936615</td>\n",
       "      <td>0.936882</td>\n",
       "      <td>0.935410</td>\n",
       "      <td>0.935319</td>\n",
       "      <td>0.938644</td>\n",
       "      <td>0.936567</td>\n",
       "      <td>0.938263</td>\n",
       "      <td>0.005530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34.454538</td>\n",
       "      <td>0.178746</td>\n",
       "      <td>1.042673</td>\n",
       "      <td>0.032303</td>\n",
       "      <td>False</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>100</td>\n",
       "      <td>4000</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>{'vec__stem': False, 'vec__ngram_range': (2, 2...</td>\n",
       "      <td>0.835113</td>\n",
       "      <td>0.819964</td>\n",
       "      <td>0.824024</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.817595</td>\n",
       "      <td>0.837321</td>\n",
       "      <td>0.821192</td>\n",
       "      <td>0.828537</td>\n",
       "      <td>0.826586</td>\n",
       "      <td>0.821934</td>\n",
       "      <td>0.824779</td>\n",
       "      <td>0.838554</td>\n",
       "      <td>0.843917</td>\n",
       "      <td>0.829181</td>\n",
       "      <td>0.829387</td>\n",
       "      <td>0.827966</td>\n",
       "      <td>0.007453</td>\n",
       "      <td>16</td>\n",
       "      <td>0.928156</td>\n",
       "      <td>0.929160</td>\n",
       "      <td>0.932736</td>\n",
       "      <td>0.932116</td>\n",
       "      <td>0.930260</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.930105</td>\n",
       "      <td>0.929681</td>\n",
       "      <td>0.929742</td>\n",
       "      <td>0.929831</td>\n",
       "      <td>0.929965</td>\n",
       "      <td>0.929728</td>\n",
       "      <td>0.928919</td>\n",
       "      <td>0.929961</td>\n",
       "      <td>0.930822</td>\n",
       "      <td>0.930079</td>\n",
       "      <td>0.001101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>247.061529</td>\n",
       "      <td>1.837971</td>\n",
       "      <td>17.207929</td>\n",
       "      <td>0.289540</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>50</td>\n",
       "      <td>400</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>{'vec__stem': True, 'vec__ngram_range': (1, 1)...</td>\n",
       "      <td>0.789410</td>\n",
       "      <td>0.786477</td>\n",
       "      <td>0.792676</td>\n",
       "      <td>0.788278</td>\n",
       "      <td>0.774387</td>\n",
       "      <td>0.771290</td>\n",
       "      <td>0.770983</td>\n",
       "      <td>0.773731</td>\n",
       "      <td>0.773058</td>\n",
       "      <td>0.782556</td>\n",
       "      <td>0.764218</td>\n",
       "      <td>0.784901</td>\n",
       "      <td>0.775194</td>\n",
       "      <td>0.792793</td>\n",
       "      <td>0.782713</td>\n",
       "      <td>0.780180</td>\n",
       "      <td>0.008599</td>\n",
       "      <td>17</td>\n",
       "      <td>0.853264</td>\n",
       "      <td>0.852673</td>\n",
       "      <td>0.855506</td>\n",
       "      <td>0.853280</td>\n",
       "      <td>0.854653</td>\n",
       "      <td>0.854699</td>\n",
       "      <td>0.852730</td>\n",
       "      <td>0.855294</td>\n",
       "      <td>0.852786</td>\n",
       "      <td>0.853561</td>\n",
       "      <td>0.854437</td>\n",
       "      <td>0.854647</td>\n",
       "      <td>0.856312</td>\n",
       "      <td>0.854607</td>\n",
       "      <td>0.855455</td>\n",
       "      <td>0.854260</td>\n",
       "      <td>0.001107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>28.916911</td>\n",
       "      <td>0.535701</td>\n",
       "      <td>0.963547</td>\n",
       "      <td>0.047587</td>\n",
       "      <td>False</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>400</td>\n",
       "      <td>4000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'vec__stem': False, 'vec__ngram_range': (2, 2...</td>\n",
       "      <td>0.780226</td>\n",
       "      <td>0.772137</td>\n",
       "      <td>0.806812</td>\n",
       "      <td>0.768957</td>\n",
       "      <td>0.768696</td>\n",
       "      <td>0.784010</td>\n",
       "      <td>0.774081</td>\n",
       "      <td>0.773697</td>\n",
       "      <td>0.769506</td>\n",
       "      <td>0.768342</td>\n",
       "      <td>0.775701</td>\n",
       "      <td>0.792697</td>\n",
       "      <td>0.768502</td>\n",
       "      <td>0.754695</td>\n",
       "      <td>0.762802</td>\n",
       "      <td>0.774726</td>\n",
       "      <td>0.012106</td>\n",
       "      <td>18</td>\n",
       "      <td>0.797191</td>\n",
       "      <td>0.796197</td>\n",
       "      <td>0.796685</td>\n",
       "      <td>0.797324</td>\n",
       "      <td>0.797953</td>\n",
       "      <td>0.796368</td>\n",
       "      <td>0.799092</td>\n",
       "      <td>0.798487</td>\n",
       "      <td>0.798218</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.797997</td>\n",
       "      <td>0.796422</td>\n",
       "      <td>0.797142</td>\n",
       "      <td>0.798688</td>\n",
       "      <td>0.797576</td>\n",
       "      <td>0.797555</td>\n",
       "      <td>0.000862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>30.454910</td>\n",
       "      <td>0.277203</td>\n",
       "      <td>1.252359</td>\n",
       "      <td>0.054745</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>400</td>\n",
       "      <td>400</td>\n",
       "      <td>False</td>\n",
       "      <td>2.5</td>\n",
       "      <td>{'vec__stem': False, 'vec__ngram_range': (1, 2...</td>\n",
       "      <td>0.092715</td>\n",
       "      <td>0.176166</td>\n",
       "      <td>0.035006</td>\n",
       "      <td>0.126294</td>\n",
       "      <td>0.054115</td>\n",
       "      <td>0.028037</td>\n",
       "      <td>0.118033</td>\n",
       "      <td>0.035211</td>\n",
       "      <td>0.113413</td>\n",
       "      <td>0.065537</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.162839</td>\n",
       "      <td>0.018824</td>\n",
       "      <td>0.114101</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>0.084761</td>\n",
       "      <td>0.049486</td>\n",
       "      <td>19</td>\n",
       "      <td>0.112691</td>\n",
       "      <td>0.157634</td>\n",
       "      <td>0.036840</td>\n",
       "      <td>0.144051</td>\n",
       "      <td>0.064626</td>\n",
       "      <td>0.034971</td>\n",
       "      <td>0.117152</td>\n",
       "      <td>0.048243</td>\n",
       "      <td>0.106168</td>\n",
       "      <td>0.075756</td>\n",
       "      <td>0.108521</td>\n",
       "      <td>0.149042</td>\n",
       "      <td>0.034225</td>\n",
       "      <td>0.124117</td>\n",
       "      <td>0.032387</td>\n",
       "      <td>0.089762</td>\n",
       "      <td>0.043876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.081504</td>\n",
       "      <td>0.155217</td>\n",
       "      <td>0.653664</td>\n",
       "      <td>0.018860</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>400</td>\n",
       "      <td>400</td>\n",
       "      <td>False</td>\n",
       "      <td>2.5</td>\n",
       "      <td>{'vec__stem': False, 'vec__ngram_range': (1, 1...</td>\n",
       "      <td>0.069741</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028037</td>\n",
       "      <td>0.118033</td>\n",
       "      <td>0.035211</td>\n",
       "      <td>0.048443</td>\n",
       "      <td>0.041812</td>\n",
       "      <td>0.034884</td>\n",
       "      <td>0.043931</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031744</td>\n",
       "      <td>0.031111</td>\n",
       "      <td>20</td>\n",
       "      <td>0.083320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034337</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034971</td>\n",
       "      <td>0.117152</td>\n",
       "      <td>0.048243</td>\n",
       "      <td>0.034323</td>\n",
       "      <td>0.041435</td>\n",
       "      <td>0.038643</td>\n",
       "      <td>0.035003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034334</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033451</td>\n",
       "      <td>0.032046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "7      150.317218      5.914168         1.549478        0.046928   \n",
       "4      305.350994      2.414326        18.169974        0.259414   \n",
       "19     304.270236     36.747755        16.246507        2.493766   \n",
       "9      296.678784      2.133247        17.782592        0.311741   \n",
       "5      121.177887     14.089188         1.175479        0.039654   \n",
       "8      262.217608      3.135966        17.414064        0.351692   \n",
       "0      256.497799      2.358328        17.330120        0.256446   \n",
       "10      25.090355      0.428465         0.741297        0.023155   \n",
       "1      256.005480      2.648494        17.132976        0.332700   \n",
       "11      47.017657      0.184964         1.333746        0.033409   \n",
       "12      38.032737      0.238129         1.030999        0.032324   \n",
       "18     268.788099      1.290587        17.479432        0.345357   \n",
       "16      34.886632      0.315058         0.994251        0.035305   \n",
       "2       19.851386      0.097297         0.724146        0.021940   \n",
       "15      18.700022      0.223215         0.723858        0.023737   \n",
       "3       34.454538      0.178746         1.042673        0.032303   \n",
       "13     247.061529      1.837971        17.207929        0.289540   \n",
       "17      28.916911      0.535701         0.963547        0.047587   \n",
       "14      30.454910      0.277203         1.252359        0.054745   \n",
       "6       10.081504      0.155217         0.653664        0.018860   \n",
       "\n",
       "   param_vec__stem param_vec__ngram_range param_vec__min_df param_vec__max_df  \\\n",
       "7            False                 (1, 2)                 1               400   \n",
       "4             True                 (1, 2)                 3               400   \n",
       "19            True                 (1, 2)                 3                 1   \n",
       "9             True                 (2, 2)                 3             40000   \n",
       "5            False                 (2, 2)                 1              4000   \n",
       "8             True                 (1, 1)                 3                 1   \n",
       "0             True                 (1, 1)                 1              4000   \n",
       "10           False                 (1, 1)               100             40000   \n",
       "1             True                 (1, 1)               100             40000   \n",
       "11           False                 (1, 2)               400                 1   \n",
       "12           False                 (2, 2)                50             40000   \n",
       "18            True                 (2, 2)                50              4000   \n",
       "16           False                 (2, 2)               100             40000   \n",
       "2            False                 (1, 1)                 1               400   \n",
       "15           False                 (1, 1)                 3               400   \n",
       "3            False                 (2, 2)               100              4000   \n",
       "13            True                 (1, 1)                50               400   \n",
       "17           False                 (2, 2)               400              4000   \n",
       "14           False                 (1, 2)               400               400   \n",
       "6            False                 (1, 1)               400               400   \n",
       "\n",
       "   param_clf__fit_intercept param_clf__C  \\\n",
       "7                     False          0.5   \n",
       "4                      True          0.5   \n",
       "19                    False          0.5   \n",
       "9                      True            2   \n",
       "5                      True          0.5   \n",
       "8                      True            3   \n",
       "0                     False            2   \n",
       "10                     True            1   \n",
       "1                      True          0.5   \n",
       "11                     True            1   \n",
       "12                    False            1   \n",
       "18                     True          2.5   \n",
       "16                     True            1   \n",
       "2                     False          0.5   \n",
       "15                    False            1   \n",
       "3                      True            3   \n",
       "13                    False            3   \n",
       "17                     True          0.5   \n",
       "14                    False          2.5   \n",
       "6                     False          2.5   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "7   {'vec__stem': False, 'vec__ngram_range': (1, 2...           0.908218   \n",
       "4   {'vec__stem': True, 'vec__ngram_range': (1, 2)...           0.901561   \n",
       "19  {'vec__stem': True, 'vec__ngram_range': (1, 2)...           0.897983   \n",
       "9   {'vec__stem': True, 'vec__ngram_range': (2, 2)...           0.886512   \n",
       "5   {'vec__stem': False, 'vec__ngram_range': (2, 2...           0.879290   \n",
       "8   {'vec__stem': True, 'vec__ngram_range': (1, 1)...           0.880952   \n",
       "0   {'vec__stem': True, 'vec__ngram_range': (1, 1)...           0.891136   \n",
       "10  {'vec__stem': False, 'vec__ngram_range': (1, 1...           0.882562   \n",
       "1   {'vec__stem': True, 'vec__ngram_range': (1, 1)...           0.879382   \n",
       "11  {'vec__stem': False, 'vec__ngram_range': (1, 2...           0.872278   \n",
       "12  {'vec__stem': False, 'vec__ngram_range': (2, 2...           0.869462   \n",
       "18  {'vec__stem': True, 'vec__ngram_range': (2, 2)...           0.862028   \n",
       "16  {'vec__stem': False, 'vec__ngram_range': (2, 2...           0.846199   \n",
       "2   {'vec__stem': False, 'vec__ngram_range': (1, 1...           0.841537   \n",
       "15  {'vec__stem': False, 'vec__ngram_range': (1, 1...           0.839136   \n",
       "3   {'vec__stem': False, 'vec__ngram_range': (2, 2...           0.835113   \n",
       "13  {'vec__stem': True, 'vec__ngram_range': (1, 1)...           0.789410   \n",
       "17  {'vec__stem': False, 'vec__ngram_range': (2, 2...           0.780226   \n",
       "14  {'vec__stem': False, 'vec__ngram_range': (1, 2...           0.092715   \n",
       "6   {'vec__stem': False, 'vec__ngram_range': (1, 1...           0.069741   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "7            0.907484           0.903529           0.907046   \n",
       "4            0.899822           0.886136           0.899115   \n",
       "19           0.895452           0.902031           0.881899   \n",
       "9            0.881556           0.899170           0.880615   \n",
       "5            0.882701           0.893668           0.870337   \n",
       "8            0.891124           0.882601           0.874484   \n",
       "0            0.889284           0.879290           0.887441   \n",
       "10           0.887307           0.879713           0.871674   \n",
       "1            0.885866           0.878460           0.868901   \n",
       "11           0.872216           0.878951           0.856637   \n",
       "12           0.849821           0.875075           0.853746   \n",
       "18           0.851391           0.867700           0.854762   \n",
       "16           0.833432           0.852695           0.834231   \n",
       "2            0.854265           0.856119           0.840528   \n",
       "15           0.850207           0.852537           0.840528   \n",
       "3            0.819964           0.824024           0.821429   \n",
       "13           0.786477           0.792676           0.788278   \n",
       "17           0.772137           0.806812           0.768957   \n",
       "14           0.176166           0.035006           0.126294   \n",
       "6            0.000000           0.000000           0.028005   \n",
       "\n",
       "    split4_test_score  split5_test_score  split6_test_score  \\\n",
       "7            0.889152           0.918404           0.902728   \n",
       "4            0.887972           0.903651           0.894923   \n",
       "19           0.884706           0.896552           0.894454   \n",
       "9            0.882111           0.885324           0.874925   \n",
       "5            0.882147           0.884638           0.880526   \n",
       "8            0.880047           0.892985           0.893642   \n",
       "0            0.869718           0.884410           0.879762   \n",
       "10           0.875222           0.877297           0.892069   \n",
       "1            0.874927           0.884935           0.886525   \n",
       "11           0.869667           0.870184           0.877527   \n",
       "12           0.850588           0.862582           0.852321   \n",
       "18           0.851039           0.856629           0.851449   \n",
       "16           0.832254           0.840787           0.839297   \n",
       "2            0.818832           0.833534           0.837182   \n",
       "15           0.821002           0.829003           0.838174   \n",
       "3            0.817595           0.837321           0.821192   \n",
       "13           0.774387           0.771290           0.770983   \n",
       "17           0.768696           0.784010           0.774081   \n",
       "14           0.054115           0.028037           0.118033   \n",
       "6            0.000000           0.028037           0.118033   \n",
       "\n",
       "    split7_test_score  split8_test_score  split9_test_score  \\\n",
       "7            0.905592           0.903730           0.896469   \n",
       "4            0.898325           0.899522           0.895238   \n",
       "19           0.888352           0.878951           0.887433   \n",
       "9            0.881928           0.881965           0.883194   \n",
       "5            0.881579           0.876499           0.880995   \n",
       "8            0.870192           0.852184           0.882775   \n",
       "0            0.866505           0.857992           0.877698   \n",
       "10           0.872315           0.859012           0.876941   \n",
       "1            0.867427           0.847943           0.877758   \n",
       "11           0.866387           0.853906           0.863962   \n",
       "12           0.858177           0.855596           0.854944   \n",
       "18           0.857143           0.853892           0.856805   \n",
       "16           0.838438           0.843693           0.833235   \n",
       "2            0.842797           0.841346           0.835033   \n",
       "15           0.840719           0.841346           0.836735   \n",
       "3            0.828537           0.826586           0.821934   \n",
       "13           0.773731           0.773058           0.782556   \n",
       "17           0.773697           0.769506           0.768342   \n",
       "14           0.035211           0.113413           0.065537   \n",
       "6            0.035211           0.048443           0.041812   \n",
       "\n",
       "    split10_test_score  split11_test_score  split12_test_score  \\\n",
       "7             0.888758            0.905136            0.912552   \n",
       "4             0.890468            0.902483            0.909846   \n",
       "19            0.879483            0.899584            0.893491   \n",
       "9             0.872491            0.895147            0.881936   \n",
       "5             0.878592            0.895397            0.878538   \n",
       "8             0.872066            0.882562            0.881416   \n",
       "0             0.857143            0.877872            0.876258   \n",
       "10            0.860834            0.881094            0.876033   \n",
       "1             0.866706            0.881235            0.879056   \n",
       "11            0.853428            0.874109            0.869410   \n",
       "12            0.857814            0.875303            0.853919   \n",
       "18            0.857649            0.873341            0.861538   \n",
       "16            0.838366            0.856287            0.838978   \n",
       "2             0.839202            0.831216            0.846473   \n",
       "15            0.836171            0.829594            0.841855   \n",
       "3             0.824779            0.838554            0.843917   \n",
       "13            0.764218            0.784901            0.775194   \n",
       "17            0.775701            0.792697            0.768502   \n",
       "14            0.105263            0.162839            0.018824   \n",
       "6             0.034884            0.043931            0.000000   \n",
       "\n",
       "    split13_test_score  split14_test_score  mean_test_score  std_test_score  \\\n",
       "7             0.896510            0.916067         0.904091        0.008313   \n",
       "4             0.894484            0.911712         0.898349        0.006956   \n",
       "19            0.890476            0.900000         0.891390        0.007357   \n",
       "9             0.884638            0.897638         0.884611        0.007268   \n",
       "5             0.875895            0.892512         0.882221        0.006687   \n",
       "8             0.877381            0.884198         0.879908        0.009965   \n",
       "0             0.869617            0.895522         0.877312        0.011088   \n",
       "10            0.867024            0.891136         0.876683        0.009400   \n",
       "1             0.875669            0.880967         0.875718        0.009547   \n",
       "11            0.859356            0.874479         0.867501        0.008023   \n",
       "12            0.851830            0.864897         0.859072        0.008230   \n",
       "18            0.845411            0.870987         0.858117        0.007535   \n",
       "16            0.832838            0.839759         0.840033        0.006925   \n",
       "2             0.832335            0.842362         0.839519        0.008867   \n",
       "15            0.833533            0.840951         0.838100        0.007642   \n",
       "3             0.829181            0.829387         0.827966        0.007453   \n",
       "13            0.792793            0.782713         0.780180        0.008599   \n",
       "17            0.754695            0.762802         0.774726        0.012106   \n",
       "14            0.114101            0.025791         0.084761        0.049486   \n",
       "6             0.028136            0.000000         0.031744        0.031111   \n",
       "\n",
       "    rank_test_score  split0_train_score  split1_train_score  \\\n",
       "7                 1            0.998199            0.998328   \n",
       "4                 2            0.996614            0.996956   \n",
       "19                3            0.939167            0.939461   \n",
       "9                 4            0.997600            0.997086   \n",
       "5                 5            0.999057            0.999271   \n",
       "8                 6            0.905072            0.904086   \n",
       "0                 7            0.942773            0.942597   \n",
       "10                8            0.896227            0.895533   \n",
       "1                 9            0.895488            0.895003   \n",
       "11               10            0.885253            0.886023   \n",
       "12               11            0.959206            0.959406   \n",
       "18               12            0.978126            0.979784   \n",
       "16               13            0.917819            0.917161   \n",
       "2                14            0.938953            0.939581   \n",
       "15               15            0.935796            0.936347   \n",
       "3                16            0.928156            0.929160   \n",
       "13               17            0.853264            0.852673   \n",
       "17               18            0.797191            0.796197   \n",
       "14               19            0.112691            0.157634   \n",
       "6                20            0.083320            0.000000   \n",
       "\n",
       "    split2_train_score  split3_train_score  split4_train_score  \\\n",
       "7             0.999614            0.998114            0.998199   \n",
       "4             0.998971            0.996871            0.996058   \n",
       "19            0.945124            0.939978            0.939601   \n",
       "9             0.997172            0.997300            0.998071   \n",
       "5             0.999657            0.999100            0.999057   \n",
       "8             0.908658            0.904742            0.904594   \n",
       "0             0.963045            0.942578            0.942336   \n",
       "10            0.898689            0.897070            0.896845   \n",
       "1             0.897472            0.896499            0.895980   \n",
       "11            0.887117            0.886702            0.886446   \n",
       "12            0.959135            0.960164            0.967538   \n",
       "18            0.989374            0.979616            0.978842   \n",
       "16            0.918033            0.918147            0.923688   \n",
       "2             0.961992            0.939815            0.939838   \n",
       "15            0.958630            0.937404            0.937693   \n",
       "3             0.932736            0.932116            0.930260   \n",
       "13            0.855506            0.853280            0.854653   \n",
       "17            0.796685            0.797324            0.797953   \n",
       "14            0.036840            0.144051            0.064626   \n",
       "6             0.000000            0.034337            0.000000   \n",
       "\n",
       "    split5_train_score  split6_train_score  split7_train_score  \\\n",
       "7             0.997985            0.998027            0.998199   \n",
       "4             0.996613            0.996313            0.996527   \n",
       "19            0.939974            0.939602            0.939257   \n",
       "9             0.997043            0.997042            0.997301   \n",
       "5             0.999100            0.999271            0.999143   \n",
       "8             0.904632            0.904501            0.904930   \n",
       "0             0.944243            0.942552            0.942782   \n",
       "10            0.895905            0.894788            0.895430   \n",
       "1             0.896066            0.895448            0.896321   \n",
       "11            0.885637            0.885062            0.885508   \n",
       "12            0.959030            0.959532            0.958476   \n",
       "18            0.978709            0.978627            0.978195   \n",
       "16            0.917634            0.917246            0.917906   \n",
       "2             0.940241            0.938953            0.940369   \n",
       "15            0.938464            0.936265            0.937747   \n",
       "3             0.930000            0.930105            0.929681   \n",
       "13            0.854699            0.852730            0.855294   \n",
       "17            0.796368            0.799092            0.798487   \n",
       "14            0.034971            0.117152            0.048243   \n",
       "6             0.034971            0.117152            0.048243   \n",
       "\n",
       "    split8_train_score  split9_train_score  split10_train_score  \\\n",
       "7             0.997942            0.998242             0.998199   \n",
       "4             0.996312            0.996613             0.996742   \n",
       "19            0.940207            0.938970             0.939465   \n",
       "9             0.997515            0.997258             0.997215   \n",
       "5             0.999057            0.999057             0.999143   \n",
       "8             0.906285            0.905566             0.905458   \n",
       "0             0.943366            0.943341             0.943918   \n",
       "10            0.898084            0.896473             0.897056   \n",
       "1             0.897503            0.895913             0.896704   \n",
       "11            0.886561            0.887428             0.886354   \n",
       "12            0.958814            0.960082             0.959098   \n",
       "18            0.978616            0.978702             0.978231   \n",
       "16            0.916443            0.917199             0.918647   \n",
       "2             0.939348            0.939965             0.940777   \n",
       "15            0.936167            0.936615             0.936882   \n",
       "3             0.929742            0.929831             0.929965   \n",
       "13            0.852786            0.853561             0.854437   \n",
       "17            0.798218            0.797980             0.797997   \n",
       "14            0.106168            0.075756             0.108521   \n",
       "6             0.034323            0.041435             0.038643   \n",
       "\n",
       "    split11_train_score  split12_train_score  split13_train_score  \\\n",
       "7              0.998157             0.998242             0.998285   \n",
       "4              0.996570             0.996400             0.996870   \n",
       "19             0.938558             0.940242             0.939562   \n",
       "9              0.997300             0.997172             0.997301   \n",
       "5              0.999143             0.999272             0.999229   \n",
       "8              0.904432             0.905630             0.905112   \n",
       "0              0.942868             0.942653             0.944122   \n",
       "10             0.896552             0.896786             0.895840   \n",
       "1              0.895005             0.896394             0.895606   \n",
       "11             0.886242             0.887277             0.886948   \n",
       "12             0.958780             0.958410             0.959051   \n",
       "18             0.979029             0.978056             0.978804   \n",
       "16             0.917743             0.916763             0.918677   \n",
       "2              0.939080             0.938707             0.941474   \n",
       "15             0.935410             0.935319             0.938644   \n",
       "3              0.929728             0.928919             0.929961   \n",
       "13             0.854647             0.856312             0.854607   \n",
       "17             0.796422             0.797142             0.798688   \n",
       "14             0.149042             0.034225             0.124117   \n",
       "6              0.035003             0.000000             0.034334   \n",
       "\n",
       "    split14_train_score  mean_train_score  std_train_score  \n",
       "7              0.998156          0.998259         0.000377  \n",
       "4              0.996570          0.996733         0.000641  \n",
       "19             0.938836          0.939867         0.001481  \n",
       "9              0.997214          0.997306         0.000253  \n",
       "5              0.999272          0.999189         0.000150  \n",
       "8              0.904034          0.905182         0.001101  \n",
       "0              0.942517          0.944380         0.005024  \n",
       "10             0.894994          0.896418         0.001039  \n",
       "1              0.895483          0.896059         0.000747  \n",
       "11             0.885264          0.886255         0.000748  \n",
       "12             0.957620          0.959623         0.002202  \n",
       "18             0.978640          0.979423         0.002701  \n",
       "16             0.918311          0.918094         0.001619  \n",
       "2              0.939757          0.941257         0.005588  \n",
       "15             0.936567          0.938263         0.005530  \n",
       "3              0.930822          0.930079         0.001101  \n",
       "13             0.855455          0.854260         0.001107  \n",
       "17             0.797576          0.797555         0.000862  \n",
       "14             0.032387          0.089762         0.043876  \n",
       "6              0.000000          0.033451         0.032046  "
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(rs_regression_bow.cv_results_).sort_values(by=['mean_test_score'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6781.202300071716, {'clf__C': 0.5}, 0.8958046812134007)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_regression_bin = {\n",
    "#     'vec__stem': [True, False],\n",
    "#     'clf__fit_intercept': [True, False], \n",
    "    'clf__C': [.5, 1, 2, 2.5, 3], \n",
    "}\n",
    "\n",
    "pipeline_regression_bin = Pipeline([\n",
    "    ('vec', LemmaCountVectorizer(strip_accents='unicode', stop_words=None, binary=True, max_df=400, min_df=3,\n",
    "                                ngram_range=(1,2), stem=True)),\n",
    "    ('clf', LogisticRegression(solver='saga', penalty='l2', fit_intercept=True, max_iter=1000))\n",
    "])\n",
    "                  \n",
    "rs_regression_bin = GridSearchCV(pipeline_regression_bin, parameters_regression_bin, \n",
    "                                   cv=15, scoring=score, n_jobs=-1, verbose=0, return_train_score=True)\n",
    "                            #, random_state=62, n_iter=20)\n",
    "start = time.time()\n",
    "rs_regression_bin.fit(X_train, y)\n",
    "time.time() - start, rs_regression_bin.best_params_, rs_regression_bin.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_clf__C</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>split10_test_score</th>\n",
       "      <th>split11_test_score</th>\n",
       "      <th>split12_test_score</th>\n",
       "      <th>split13_test_score</th>\n",
       "      <th>split14_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>split5_train_score</th>\n",
       "      <th>split6_train_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>split8_train_score</th>\n",
       "      <th>split9_train_score</th>\n",
       "      <th>split10_train_score</th>\n",
       "      <th>split11_train_score</th>\n",
       "      <th>split12_train_score</th>\n",
       "      <th>split13_train_score</th>\n",
       "      <th>split14_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>361.707683</td>\n",
       "      <td>5.105309</td>\n",
       "      <td>18.154699</td>\n",
       "      <td>0.469170</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'clf__C': 0.5}</td>\n",
       "      <td>0.904648</td>\n",
       "      <td>0.899353</td>\n",
       "      <td>0.889801</td>\n",
       "      <td>0.897527</td>\n",
       "      <td>0.882701</td>\n",
       "      <td>0.897421</td>\n",
       "      <td>0.898516</td>\n",
       "      <td>0.895021</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.893007</td>\n",
       "      <td>0.890215</td>\n",
       "      <td>0.897343</td>\n",
       "      <td>0.902007</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>0.904077</td>\n",
       "      <td>0.895805</td>\n",
       "      <td>0.005825</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>402.473715</td>\n",
       "      <td>8.110607</td>\n",
       "      <td>18.290383</td>\n",
       "      <td>0.515732</td>\n",
       "      <td>1</td>\n",
       "      <td>{'clf__C': 1}</td>\n",
       "      <td>0.905301</td>\n",
       "      <td>0.902007</td>\n",
       "      <td>0.891101</td>\n",
       "      <td>0.897647</td>\n",
       "      <td>0.883225</td>\n",
       "      <td>0.898082</td>\n",
       "      <td>0.894299</td>\n",
       "      <td>0.896345</td>\n",
       "      <td>0.889022</td>\n",
       "      <td>0.890746</td>\n",
       "      <td>0.888358</td>\n",
       "      <td>0.896426</td>\n",
       "      <td>0.900826</td>\n",
       "      <td>0.895468</td>\n",
       "      <td>0.902102</td>\n",
       "      <td>0.895397</td>\n",
       "      <td>0.005832</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>457.334456</td>\n",
       "      <td>11.801718</td>\n",
       "      <td>18.518091</td>\n",
       "      <td>0.374315</td>\n",
       "      <td>2.5</td>\n",
       "      <td>{'clf__C': 2.5}</td>\n",
       "      <td>0.904224</td>\n",
       "      <td>0.903721</td>\n",
       "      <td>0.889930</td>\n",
       "      <td>0.895698</td>\n",
       "      <td>0.882423</td>\n",
       "      <td>0.896882</td>\n",
       "      <td>0.894580</td>\n",
       "      <td>0.896345</td>\n",
       "      <td>0.889556</td>\n",
       "      <td>0.889952</td>\n",
       "      <td>0.889021</td>\n",
       "      <td>0.897514</td>\n",
       "      <td>0.900059</td>\n",
       "      <td>0.894260</td>\n",
       "      <td>0.898376</td>\n",
       "      <td>0.894836</td>\n",
       "      <td>0.005690</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>446.756659</td>\n",
       "      <td>13.720502</td>\n",
       "      <td>18.276270</td>\n",
       "      <td>0.426953</td>\n",
       "      <td>2</td>\n",
       "      <td>{'clf__C': 2}</td>\n",
       "      <td>0.905301</td>\n",
       "      <td>0.901891</td>\n",
       "      <td>0.890451</td>\n",
       "      <td>0.894768</td>\n",
       "      <td>0.883225</td>\n",
       "      <td>0.897543</td>\n",
       "      <td>0.893516</td>\n",
       "      <td>0.895397</td>\n",
       "      <td>0.889022</td>\n",
       "      <td>0.889952</td>\n",
       "      <td>0.888358</td>\n",
       "      <td>0.897514</td>\n",
       "      <td>0.901891</td>\n",
       "      <td>0.894260</td>\n",
       "      <td>0.899038</td>\n",
       "      <td>0.894809</td>\n",
       "      <td>0.005759</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>423.694898</td>\n",
       "      <td>90.241086</td>\n",
       "      <td>16.276376</td>\n",
       "      <td>4.121535</td>\n",
       "      <td>3</td>\n",
       "      <td>{'clf__C': 3}</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.901359</td>\n",
       "      <td>0.890579</td>\n",
       "      <td>0.895698</td>\n",
       "      <td>0.882423</td>\n",
       "      <td>0.898082</td>\n",
       "      <td>0.893389</td>\n",
       "      <td>0.897421</td>\n",
       "      <td>0.887155</td>\n",
       "      <td>0.889952</td>\n",
       "      <td>0.889021</td>\n",
       "      <td>0.898058</td>\n",
       "      <td>0.898876</td>\n",
       "      <td>0.893591</td>\n",
       "      <td>0.896510</td>\n",
       "      <td>0.894459</td>\n",
       "      <td>0.005652</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_clf__C  \\\n",
       "0     361.707683      5.105309        18.154699        0.469170          0.5   \n",
       "1     402.473715      8.110607        18.290383        0.515732            1   \n",
       "3     457.334456     11.801718        18.518091        0.374315          2.5   \n",
       "2     446.756659     13.720502        18.276270        0.426953            2   \n",
       "4     423.694898     90.241086        16.276376        4.121535            3   \n",
       "\n",
       "            params  split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0  {'clf__C': 0.5}           0.904648           0.899353           0.889801   \n",
       "1    {'clf__C': 1}           0.905301           0.902007           0.891101   \n",
       "3  {'clf__C': 2.5}           0.904224           0.903721           0.889930   \n",
       "2    {'clf__C': 2}           0.905301           0.901891           0.890451   \n",
       "4    {'clf__C': 3}           0.904762           0.901359           0.890579   \n",
       "\n",
       "   split3_test_score  split4_test_score  split5_test_score  split6_test_score  \\\n",
       "0           0.897527           0.882701           0.897421           0.898516   \n",
       "1           0.897647           0.883225           0.898082           0.894299   \n",
       "3           0.895698           0.882423           0.896882           0.894580   \n",
       "2           0.894768           0.883225           0.897543           0.893516   \n",
       "4           0.895698           0.882423           0.898082           0.893389   \n",
       "\n",
       "   split7_test_score  split8_test_score  split9_test_score  \\\n",
       "0           0.895021           0.888889           0.893007   \n",
       "1           0.896345           0.889022           0.890746   \n",
       "3           0.896345           0.889556           0.889952   \n",
       "2           0.895397           0.889022           0.889952   \n",
       "4           0.897421           0.887155           0.889952   \n",
       "\n",
       "   split10_test_score  split11_test_score  split12_test_score  \\\n",
       "0            0.890215            0.897343            0.902007   \n",
       "1            0.888358            0.896426            0.900826   \n",
       "3            0.889021            0.897514            0.900059   \n",
       "2            0.888358            0.897514            0.901891   \n",
       "4            0.889021            0.898058            0.898876   \n",
       "\n",
       "   split13_test_score  split14_test_score  mean_test_score  std_test_score  \\\n",
       "0            0.896552            0.904077         0.895805        0.005825   \n",
       "1            0.895468            0.902102         0.895397        0.005832   \n",
       "3            0.894260            0.898376         0.894836        0.005690   \n",
       "2            0.894260            0.899038         0.894809        0.005759   \n",
       "4            0.893591            0.896510         0.894459        0.005652   \n",
       "\n",
       "   rank_test_score  split0_train_score  split1_train_score  \\\n",
       "0                1                 1.0                 1.0   \n",
       "1                2                 1.0                 1.0   \n",
       "3                3                 1.0                 1.0   \n",
       "2                4                 1.0                 1.0   \n",
       "4                5                 1.0                 1.0   \n",
       "\n",
       "   split2_train_score  split3_train_score  split4_train_score  \\\n",
       "0                 1.0                 1.0                 1.0   \n",
       "1                 1.0                 1.0                 1.0   \n",
       "3                 1.0                 1.0                 1.0   \n",
       "2                 1.0                 1.0                 1.0   \n",
       "4                 1.0                 1.0                 1.0   \n",
       "\n",
       "   split5_train_score  split6_train_score  split7_train_score  \\\n",
       "0                 1.0                 1.0                 1.0   \n",
       "1                 1.0                 1.0                 1.0   \n",
       "3                 1.0                 1.0                 1.0   \n",
       "2                 1.0                 1.0                 1.0   \n",
       "4                 1.0                 1.0                 1.0   \n",
       "\n",
       "   split8_train_score  split9_train_score  split10_train_score  \\\n",
       "0                 1.0                 1.0                  1.0   \n",
       "1                 1.0                 1.0                  1.0   \n",
       "3                 1.0                 1.0                  1.0   \n",
       "2                 1.0                 1.0                  1.0   \n",
       "4                 1.0                 1.0                  1.0   \n",
       "\n",
       "   split11_train_score  split12_train_score  split13_train_score  \\\n",
       "0                  1.0                  1.0                  1.0   \n",
       "1                  1.0                  1.0                  1.0   \n",
       "3                  1.0                  1.0                  1.0   \n",
       "2                  1.0                  1.0                  1.0   \n",
       "4                  1.0                  1.0                  1.0   \n",
       "\n",
       "   split14_train_score  mean_train_score  std_train_score  \n",
       "0                  1.0               1.0              0.0  \n",
       "1                  1.0               1.0              0.0  \n",
       "3                  1.0               1.0              0.0  \n",
       "2                  1.0               1.0              0.0  \n",
       "4                  1.0               1.0              0.0  "
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(rs_regression_bin.cv_results_).sort_values(by=['mean_test_score'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering considering BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19414.725648880005,\n",
       " {'vec__stem': False,\n",
       "  'vec__ngram_range': (2, 2),\n",
       "  'vec__min_df': 1,\n",
       "  'vec__max_df': 1.0,\n",
       "  'clf__alpha': 2},\n",
       " 0.8875362305239204)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_mnv_bow = {\n",
    "    'vec__min_df': [3, 50, 100, 400, 1],\n",
    "    'vec__max_df': [4000, 40000, 1.0],\n",
    "    'vec__stem': [True, False],\n",
    "    'vec__ngram_range':[(1, 2), (2, 2)],\n",
    "    'clf__alpha': [.5, .1, 2, 3], \n",
    "}\n",
    "\n",
    "pipeline_mnv_bow = Pipeline([\n",
    "    ('vec', LemmaCountVectorizer(strip_accents='unicode', stop_words=None, binary=False)),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "                  \n",
    "rs_mnv_bow = RandomizedSearchCV(pipeline_mnv_bow, parameters_mnv_bow, \n",
    "                                   cv=15, scoring=score, n_jobs=-1, verbose=0, random_state=62, n_iter=20\n",
    "                               , return_train_score=True)\n",
    "start = time.time()\n",
    "rs_mnv_bow.fit(X_train, y)\n",
    "time.time() - start, rs_mnv_bow.best_params_, rs_mnv_bow.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_vec__stem</th>\n",
       "      <th>param_vec__ngram_range</th>\n",
       "      <th>param_vec__min_df</th>\n",
       "      <th>param_vec__max_df</th>\n",
       "      <th>param_clf__alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>split10_test_score</th>\n",
       "      <th>split11_test_score</th>\n",
       "      <th>split12_test_score</th>\n",
       "      <th>split13_test_score</th>\n",
       "      <th>split14_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>split5_train_score</th>\n",
       "      <th>split6_train_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>split8_train_score</th>\n",
       "      <th>split9_train_score</th>\n",
       "      <th>split10_train_score</th>\n",
       "      <th>split11_train_score</th>\n",
       "      <th>split12_train_score</th>\n",
       "      <th>split13_train_score</th>\n",
       "      <th>split14_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>27.338482</td>\n",
       "      <td>0.264187</td>\n",
       "      <td>1.195799</td>\n",
       "      <td>0.035670</td>\n",
       "      <td>False</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>{'vec__stem': False, 'vec__ngram_range': (2, 2...</td>\n",
       "      <td>0.879511</td>\n",
       "      <td>0.900480</td>\n",
       "      <td>0.899094</td>\n",
       "      <td>0.883273</td>\n",
       "      <td>0.889831</td>\n",
       "      <td>0.888485</td>\n",
       "      <td>0.894231</td>\n",
       "      <td>0.893513</td>\n",
       "      <td>0.874156</td>\n",
       "      <td>0.879569</td>\n",
       "      <td>0.871299</td>\n",
       "      <td>0.889851</td>\n",
       "      <td>0.894417</td>\n",
       "      <td>0.889831</td>\n",
       "      <td>0.885487</td>\n",
       "      <td>0.887536</td>\n",
       "      <td>0.008322</td>\n",
       "      <td>1</td>\n",
       "      <td>0.994407</td>\n",
       "      <td>0.994103</td>\n",
       "      <td>0.994192</td>\n",
       "      <td>0.993845</td>\n",
       "      <td>0.994320</td>\n",
       "      <td>0.993931</td>\n",
       "      <td>0.993497</td>\n",
       "      <td>0.994017</td>\n",
       "      <td>0.993844</td>\n",
       "      <td>0.993931</td>\n",
       "      <td>0.993844</td>\n",
       "      <td>0.993628</td>\n",
       "      <td>0.993585</td>\n",
       "      <td>0.994105</td>\n",
       "      <td>0.993973</td>\n",
       "      <td>0.993948</td>\n",
       "      <td>0.000249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>256.710281</td>\n",
       "      <td>2.887174</td>\n",
       "      <td>17.397306</td>\n",
       "      <td>0.407629</td>\n",
       "      <td>True</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>{'vec__stem': True, 'vec__ngram_range': (2, 2)...</td>\n",
       "      <td>0.879708</td>\n",
       "      <td>0.894387</td>\n",
       "      <td>0.901391</td>\n",
       "      <td>0.879663</td>\n",
       "      <td>0.886623</td>\n",
       "      <td>0.888214</td>\n",
       "      <td>0.894103</td>\n",
       "      <td>0.886978</td>\n",
       "      <td>0.878287</td>\n",
       "      <td>0.871703</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.892175</td>\n",
       "      <td>0.895181</td>\n",
       "      <td>0.885801</td>\n",
       "      <td>0.886598</td>\n",
       "      <td>0.886026</td>\n",
       "      <td>0.008624</td>\n",
       "      <td>2</td>\n",
       "      <td>0.990813</td>\n",
       "      <td>0.990985</td>\n",
       "      <td>0.991289</td>\n",
       "      <td>0.990727</td>\n",
       "      <td>0.990725</td>\n",
       "      <td>0.990857</td>\n",
       "      <td>0.990682</td>\n",
       "      <td>0.990724</td>\n",
       "      <td>0.990813</td>\n",
       "      <td>0.990594</td>\n",
       "      <td>0.990724</td>\n",
       "      <td>0.990901</td>\n",
       "      <td>0.990725</td>\n",
       "      <td>0.990987</td>\n",
       "      <td>0.990684</td>\n",
       "      <td>0.990815</td>\n",
       "      <td>0.000167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>33.380934</td>\n",
       "      <td>0.351356</td>\n",
       "      <td>1.609731</td>\n",
       "      <td>0.032869</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1</td>\n",
       "      <td>4000</td>\n",
       "      <td>3</td>\n",
       "      <td>{'vec__stem': False, 'vec__ngram_range': (1, 2...</td>\n",
       "      <td>0.877724</td>\n",
       "      <td>0.897619</td>\n",
       "      <td>0.898082</td>\n",
       "      <td>0.883582</td>\n",
       "      <td>0.886499</td>\n",
       "      <td>0.882424</td>\n",
       "      <td>0.894358</td>\n",
       "      <td>0.892683</td>\n",
       "      <td>0.872881</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.870813</td>\n",
       "      <td>0.883436</td>\n",
       "      <td>0.893848</td>\n",
       "      <td>0.880866</td>\n",
       "      <td>0.890909</td>\n",
       "      <td>0.885114</td>\n",
       "      <td>0.009012</td>\n",
       "      <td>3</td>\n",
       "      <td>0.979375</td>\n",
       "      <td>0.979666</td>\n",
       "      <td>0.979627</td>\n",
       "      <td>0.979459</td>\n",
       "      <td>0.979678</td>\n",
       "      <td>0.979325</td>\n",
       "      <td>0.979219</td>\n",
       "      <td>0.979449</td>\n",
       "      <td>0.979458</td>\n",
       "      <td>0.979235</td>\n",
       "      <td>0.979654</td>\n",
       "      <td>0.979410</td>\n",
       "      <td>0.979263</td>\n",
       "      <td>0.979719</td>\n",
       "      <td>0.978902</td>\n",
       "      <td>0.979429</td>\n",
       "      <td>0.000215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>265.768027</td>\n",
       "      <td>9.286911</td>\n",
       "      <td>768.698651</td>\n",
       "      <td>1913.983053</td>\n",
       "      <td>True</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'vec__stem': True, 'vec__ngram_range': (2, 2)...</td>\n",
       "      <td>0.879081</td>\n",
       "      <td>0.896716</td>\n",
       "      <td>0.890877</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.881618</td>\n",
       "      <td>0.884988</td>\n",
       "      <td>0.882775</td>\n",
       "      <td>0.887668</td>\n",
       "      <td>0.882675</td>\n",
       "      <td>0.866469</td>\n",
       "      <td>0.868516</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.897129</td>\n",
       "      <td>0.885009</td>\n",
       "      <td>0.878460</td>\n",
       "      <td>0.883229</td>\n",
       "      <td>0.008423</td>\n",
       "      <td>4</td>\n",
       "      <td>0.982700</td>\n",
       "      <td>0.982733</td>\n",
       "      <td>0.982449</td>\n",
       "      <td>0.982185</td>\n",
       "      <td>0.982060</td>\n",
       "      <td>0.982231</td>\n",
       "      <td>0.982394</td>\n",
       "      <td>0.981748</td>\n",
       "      <td>0.982399</td>\n",
       "      <td>0.981300</td>\n",
       "      <td>0.982789</td>\n",
       "      <td>0.982533</td>\n",
       "      <td>0.981712</td>\n",
       "      <td>0.982362</td>\n",
       "      <td>0.982228</td>\n",
       "      <td>0.982255</td>\n",
       "      <td>0.000399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33.303984</td>\n",
       "      <td>0.316284</td>\n",
       "      <td>1.598839</td>\n",
       "      <td>0.052621</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1</td>\n",
       "      <td>40000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'vec__stem': False, 'vec__ngram_range': (1, 2...</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.894833</td>\n",
       "      <td>0.879854</td>\n",
       "      <td>0.885662</td>\n",
       "      <td>0.877839</td>\n",
       "      <td>0.888082</td>\n",
       "      <td>0.885609</td>\n",
       "      <td>0.868389</td>\n",
       "      <td>0.863388</td>\n",
       "      <td>0.862674</td>\n",
       "      <td>0.877085</td>\n",
       "      <td>0.891743</td>\n",
       "      <td>0.876144</td>\n",
       "      <td>0.877538</td>\n",
       "      <td>0.879833</td>\n",
       "      <td>0.009804</td>\n",
       "      <td>5</td>\n",
       "      <td>0.995441</td>\n",
       "      <td>0.995743</td>\n",
       "      <td>0.995527</td>\n",
       "      <td>0.995570</td>\n",
       "      <td>0.995571</td>\n",
       "      <td>0.995311</td>\n",
       "      <td>0.995440</td>\n",
       "      <td>0.995527</td>\n",
       "      <td>0.995441</td>\n",
       "      <td>0.995225</td>\n",
       "      <td>0.995829</td>\n",
       "      <td>0.995484</td>\n",
       "      <td>0.995440</td>\n",
       "      <td>0.995528</td>\n",
       "      <td>0.995398</td>\n",
       "      <td>0.995498</td>\n",
       "      <td>0.000145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>262.922652</td>\n",
       "      <td>1.083385</td>\n",
       "      <td>17.752333</td>\n",
       "      <td>0.420279</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'vec__stem': True, 'vec__ngram_range': (1, 2)...</td>\n",
       "      <td>0.870416</td>\n",
       "      <td>0.887009</td>\n",
       "      <td>0.889697</td>\n",
       "      <td>0.871981</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>0.875921</td>\n",
       "      <td>0.880823</td>\n",
       "      <td>0.880835</td>\n",
       "      <td>0.858716</td>\n",
       "      <td>0.856799</td>\n",
       "      <td>0.863005</td>\n",
       "      <td>0.877388</td>\n",
       "      <td>0.886336</td>\n",
       "      <td>0.874543</td>\n",
       "      <td>0.876144</td>\n",
       "      <td>0.875338</td>\n",
       "      <td>0.009532</td>\n",
       "      <td>6</td>\n",
       "      <td>0.953661</td>\n",
       "      <td>0.953838</td>\n",
       "      <td>0.953813</td>\n",
       "      <td>0.952546</td>\n",
       "      <td>0.953447</td>\n",
       "      <td>0.953566</td>\n",
       "      <td>0.953534</td>\n",
       "      <td>0.952996</td>\n",
       "      <td>0.953617</td>\n",
       "      <td>0.953166</td>\n",
       "      <td>0.953954</td>\n",
       "      <td>0.953371</td>\n",
       "      <td>0.952840</td>\n",
       "      <td>0.954670</td>\n",
       "      <td>0.953479</td>\n",
       "      <td>0.953500</td>\n",
       "      <td>0.000487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>264.902432</td>\n",
       "      <td>0.850924</td>\n",
       "      <td>17.942699</td>\n",
       "      <td>0.342192</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1</td>\n",
       "      <td>40000</td>\n",
       "      <td>3</td>\n",
       "      <td>{'vec__stem': True, 'vec__ngram_range': (1, 2)...</td>\n",
       "      <td>0.867764</td>\n",
       "      <td>0.881785</td>\n",
       "      <td>0.886473</td>\n",
       "      <td>0.869148</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.873394</td>\n",
       "      <td>0.883023</td>\n",
       "      <td>0.877689</td>\n",
       "      <td>0.861314</td>\n",
       "      <td>0.856802</td>\n",
       "      <td>0.859892</td>\n",
       "      <td>0.875616</td>\n",
       "      <td>0.882850</td>\n",
       "      <td>0.876812</td>\n",
       "      <td>0.870146</td>\n",
       "      <td>0.873483</td>\n",
       "      <td>0.008793</td>\n",
       "      <td>7</td>\n",
       "      <td>0.967574</td>\n",
       "      <td>0.968639</td>\n",
       "      <td>0.968081</td>\n",
       "      <td>0.967355</td>\n",
       "      <td>0.968305</td>\n",
       "      <td>0.967187</td>\n",
       "      <td>0.967574</td>\n",
       "      <td>0.967661</td>\n",
       "      <td>0.968036</td>\n",
       "      <td>0.967862</td>\n",
       "      <td>0.967585</td>\n",
       "      <td>0.968347</td>\n",
       "      <td>0.967001</td>\n",
       "      <td>0.968249</td>\n",
       "      <td>0.967347</td>\n",
       "      <td>0.967787</td>\n",
       "      <td>0.000460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>31.445339</td>\n",
       "      <td>0.172708</td>\n",
       "      <td>1.521602</td>\n",
       "      <td>0.033771</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>{'vec__stem': False, 'vec__ngram_range': (1, 2...</td>\n",
       "      <td>0.864167</td>\n",
       "      <td>0.885385</td>\n",
       "      <td>0.881995</td>\n",
       "      <td>0.873341</td>\n",
       "      <td>0.873716</td>\n",
       "      <td>0.874539</td>\n",
       "      <td>0.885784</td>\n",
       "      <td>0.877601</td>\n",
       "      <td>0.861709</td>\n",
       "      <td>0.856451</td>\n",
       "      <td>0.855407</td>\n",
       "      <td>0.868159</td>\n",
       "      <td>0.883037</td>\n",
       "      <td>0.869671</td>\n",
       "      <td>0.877689</td>\n",
       "      <td>0.872578</td>\n",
       "      <td>0.009540</td>\n",
       "      <td>8</td>\n",
       "      <td>0.940639</td>\n",
       "      <td>0.941156</td>\n",
       "      <td>0.942409</td>\n",
       "      <td>0.940951</td>\n",
       "      <td>0.941713</td>\n",
       "      <td>0.940915</td>\n",
       "      <td>0.941279</td>\n",
       "      <td>0.940281</td>\n",
       "      <td>0.941356</td>\n",
       "      <td>0.939967</td>\n",
       "      <td>0.941141</td>\n",
       "      <td>0.941320</td>\n",
       "      <td>0.940695</td>\n",
       "      <td>0.943111</td>\n",
       "      <td>0.940475</td>\n",
       "      <td>0.941161</td>\n",
       "      <td>0.000775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>267.644572</td>\n",
       "      <td>6.375577</td>\n",
       "      <td>18.188342</td>\n",
       "      <td>0.242856</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>3</td>\n",
       "      <td>40000</td>\n",
       "      <td>2</td>\n",
       "      <td>{'vec__stem': True, 'vec__ngram_range': (1, 2)...</td>\n",
       "      <td>0.865031</td>\n",
       "      <td>0.880923</td>\n",
       "      <td>0.886199</td>\n",
       "      <td>0.866989</td>\n",
       "      <td>0.872044</td>\n",
       "      <td>0.869458</td>\n",
       "      <td>0.881643</td>\n",
       "      <td>0.875616</td>\n",
       "      <td>0.858716</td>\n",
       "      <td>0.854732</td>\n",
       "      <td>0.856796</td>\n",
       "      <td>0.872908</td>\n",
       "      <td>0.883636</td>\n",
       "      <td>0.871420</td>\n",
       "      <td>0.872705</td>\n",
       "      <td>0.871255</td>\n",
       "      <td>0.009316</td>\n",
       "      <td>9</td>\n",
       "      <td>0.936261</td>\n",
       "      <td>0.936900</td>\n",
       "      <td>0.937296</td>\n",
       "      <td>0.936840</td>\n",
       "      <td>0.936677</td>\n",
       "      <td>0.936514</td>\n",
       "      <td>0.936953</td>\n",
       "      <td>0.936281</td>\n",
       "      <td>0.936524</td>\n",
       "      <td>0.934988</td>\n",
       "      <td>0.936725</td>\n",
       "      <td>0.936161</td>\n",
       "      <td>0.935628</td>\n",
       "      <td>0.938130</td>\n",
       "      <td>0.936057</td>\n",
       "      <td>0.936529</td>\n",
       "      <td>0.000698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>259.376821</td>\n",
       "      <td>2.669786</td>\n",
       "      <td>17.647668</td>\n",
       "      <td>0.405302</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>50</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'vec__stem': True, 'vec__ngram_range': (1, 2)...</td>\n",
       "      <td>0.861613</td>\n",
       "      <td>0.875224</td>\n",
       "      <td>0.868531</td>\n",
       "      <td>0.868687</td>\n",
       "      <td>0.865316</td>\n",
       "      <td>0.863719</td>\n",
       "      <td>0.875075</td>\n",
       "      <td>0.855596</td>\n",
       "      <td>0.861925</td>\n",
       "      <td>0.855450</td>\n",
       "      <td>0.848810</td>\n",
       "      <td>0.876364</td>\n",
       "      <td>0.881336</td>\n",
       "      <td>0.858525</td>\n",
       "      <td>0.872771</td>\n",
       "      <td>0.865930</td>\n",
       "      <td>0.008875</td>\n",
       "      <td>10</td>\n",
       "      <td>0.882878</td>\n",
       "      <td>0.883608</td>\n",
       "      <td>0.883612</td>\n",
       "      <td>0.883175</td>\n",
       "      <td>0.883993</td>\n",
       "      <td>0.883582</td>\n",
       "      <td>0.882718</td>\n",
       "      <td>0.884472</td>\n",
       "      <td>0.882768</td>\n",
       "      <td>0.882451</td>\n",
       "      <td>0.884230</td>\n",
       "      <td>0.883335</td>\n",
       "      <td>0.882906</td>\n",
       "      <td>0.884752</td>\n",
       "      <td>0.882114</td>\n",
       "      <td>0.883373</td>\n",
       "      <td>0.000736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1777.653417</td>\n",
       "      <td>2500.229758</td>\n",
       "      <td>17.303805</td>\n",
       "      <td>4.820780</td>\n",
       "      <td>True</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>50</td>\n",
       "      <td>4000</td>\n",
       "      <td>2</td>\n",
       "      <td>{'vec__stem': True, 'vec__ngram_range': (2, 2)...</td>\n",
       "      <td>0.848594</td>\n",
       "      <td>0.867969</td>\n",
       "      <td>0.866430</td>\n",
       "      <td>0.851742</td>\n",
       "      <td>0.850764</td>\n",
       "      <td>0.863905</td>\n",
       "      <td>0.866785</td>\n",
       "      <td>0.856971</td>\n",
       "      <td>0.850450</td>\n",
       "      <td>0.844626</td>\n",
       "      <td>0.839812</td>\n",
       "      <td>0.873728</td>\n",
       "      <td>0.872856</td>\n",
       "      <td>0.853919</td>\n",
       "      <td>0.864800</td>\n",
       "      <td>0.858223</td>\n",
       "      <td>0.010199</td>\n",
       "      <td>11</td>\n",
       "      <td>0.881586</td>\n",
       "      <td>0.881221</td>\n",
       "      <td>0.881912</td>\n",
       "      <td>0.881735</td>\n",
       "      <td>0.881665</td>\n",
       "      <td>0.881824</td>\n",
       "      <td>0.880872</td>\n",
       "      <td>0.881770</td>\n",
       "      <td>0.881498</td>\n",
       "      <td>0.880557</td>\n",
       "      <td>0.882345</td>\n",
       "      <td>0.880820</td>\n",
       "      <td>0.880427</td>\n",
       "      <td>0.882864</td>\n",
       "      <td>0.881635</td>\n",
       "      <td>0.881515</td>\n",
       "      <td>0.000633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>255.647084</td>\n",
       "      <td>1.375877</td>\n",
       "      <td>17.445716</td>\n",
       "      <td>0.374399</td>\n",
       "      <td>True</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>50</td>\n",
       "      <td>40000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'vec__stem': True, 'vec__ngram_range': (2, 2)...</td>\n",
       "      <td>0.847761</td>\n",
       "      <td>0.866469</td>\n",
       "      <td>0.863744</td>\n",
       "      <td>0.851567</td>\n",
       "      <td>0.851089</td>\n",
       "      <td>0.859504</td>\n",
       "      <td>0.864066</td>\n",
       "      <td>0.862627</td>\n",
       "      <td>0.849760</td>\n",
       "      <td>0.845481</td>\n",
       "      <td>0.837758</td>\n",
       "      <td>0.871856</td>\n",
       "      <td>0.872706</td>\n",
       "      <td>0.854093</td>\n",
       "      <td>0.861061</td>\n",
       "      <td>0.857302</td>\n",
       "      <td>0.009726</td>\n",
       "      <td>12</td>\n",
       "      <td>0.878961</td>\n",
       "      <td>0.878810</td>\n",
       "      <td>0.879573</td>\n",
       "      <td>0.879013</td>\n",
       "      <td>0.879925</td>\n",
       "      <td>0.878088</td>\n",
       "      <td>0.877208</td>\n",
       "      <td>0.880007</td>\n",
       "      <td>0.878382</td>\n",
       "      <td>0.877570</td>\n",
       "      <td>0.880662</td>\n",
       "      <td>0.878460</td>\n",
       "      <td>0.878051</td>\n",
       "      <td>0.879640</td>\n",
       "      <td>0.878113</td>\n",
       "      <td>0.878831</td>\n",
       "      <td>0.000946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>256.636996</td>\n",
       "      <td>2.613164</td>\n",
       "      <td>17.504603</td>\n",
       "      <td>0.388934</td>\n",
       "      <td>True</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>50</td>\n",
       "      <td>40000</td>\n",
       "      <td>2</td>\n",
       "      <td>{'vec__stem': True, 'vec__ngram_range': (2, 2)...</td>\n",
       "      <td>0.847761</td>\n",
       "      <td>0.865282</td>\n",
       "      <td>0.862722</td>\n",
       "      <td>0.851240</td>\n",
       "      <td>0.851089</td>\n",
       "      <td>0.859338</td>\n",
       "      <td>0.863394</td>\n",
       "      <td>0.862110</td>\n",
       "      <td>0.849249</td>\n",
       "      <td>0.844133</td>\n",
       "      <td>0.835979</td>\n",
       "      <td>0.872010</td>\n",
       "      <td>0.871674</td>\n",
       "      <td>0.852575</td>\n",
       "      <td>0.860382</td>\n",
       "      <td>0.856595</td>\n",
       "      <td>0.009878</td>\n",
       "      <td>13</td>\n",
       "      <td>0.878069</td>\n",
       "      <td>0.878127</td>\n",
       "      <td>0.878444</td>\n",
       "      <td>0.878460</td>\n",
       "      <td>0.878938</td>\n",
       "      <td>0.877467</td>\n",
       "      <td>0.876655</td>\n",
       "      <td>0.878671</td>\n",
       "      <td>0.877586</td>\n",
       "      <td>0.876497</td>\n",
       "      <td>0.879891</td>\n",
       "      <td>0.877606</td>\n",
       "      <td>0.877339</td>\n",
       "      <td>0.878690</td>\n",
       "      <td>0.877856</td>\n",
       "      <td>0.878020</td>\n",
       "      <td>0.000854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>260.098398</td>\n",
       "      <td>0.965629</td>\n",
       "      <td>17.717461</td>\n",
       "      <td>0.371146</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>50</td>\n",
       "      <td>40000</td>\n",
       "      <td>3</td>\n",
       "      <td>{'vec__stem': True, 'vec__ngram_range': (1, 2)...</td>\n",
       "      <td>0.847826</td>\n",
       "      <td>0.868990</td>\n",
       "      <td>0.867384</td>\n",
       "      <td>0.855271</td>\n",
       "      <td>0.855443</td>\n",
       "      <td>0.851582</td>\n",
       "      <td>0.859880</td>\n",
       "      <td>0.847498</td>\n",
       "      <td>0.838554</td>\n",
       "      <td>0.843044</td>\n",
       "      <td>0.841916</td>\n",
       "      <td>0.867764</td>\n",
       "      <td>0.863145</td>\n",
       "      <td>0.847682</td>\n",
       "      <td>0.855429</td>\n",
       "      <td>0.854096</td>\n",
       "      <td>0.009479</td>\n",
       "      <td>14</td>\n",
       "      <td>0.868471</td>\n",
       "      <td>0.869632</td>\n",
       "      <td>0.869185</td>\n",
       "      <td>0.869311</td>\n",
       "      <td>0.870304</td>\n",
       "      <td>0.868733</td>\n",
       "      <td>0.868210</td>\n",
       "      <td>0.870181</td>\n",
       "      <td>0.868628</td>\n",
       "      <td>0.867605</td>\n",
       "      <td>0.870316</td>\n",
       "      <td>0.868274</td>\n",
       "      <td>0.869050</td>\n",
       "      <td>0.871126</td>\n",
       "      <td>0.867996</td>\n",
       "      <td>0.869135</td>\n",
       "      <td>0.000973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>31.663341</td>\n",
       "      <td>0.489359</td>\n",
       "      <td>1.423488</td>\n",
       "      <td>0.086009</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>100</td>\n",
       "      <td>40000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'vec__stem': False, 'vec__ngram_range': (1, 2...</td>\n",
       "      <td>0.845969</td>\n",
       "      <td>0.866547</td>\n",
       "      <td>0.859206</td>\n",
       "      <td>0.846655</td>\n",
       "      <td>0.850088</td>\n",
       "      <td>0.846013</td>\n",
       "      <td>0.858164</td>\n",
       "      <td>0.842797</td>\n",
       "      <td>0.842676</td>\n",
       "      <td>0.833136</td>\n",
       "      <td>0.835113</td>\n",
       "      <td>0.854015</td>\n",
       "      <td>0.858164</td>\n",
       "      <td>0.836451</td>\n",
       "      <td>0.841471</td>\n",
       "      <td>0.847767</td>\n",
       "      <td>0.009467</td>\n",
       "      <td>15</td>\n",
       "      <td>0.857619</td>\n",
       "      <td>0.858559</td>\n",
       "      <td>0.858155</td>\n",
       "      <td>0.858427</td>\n",
       "      <td>0.859537</td>\n",
       "      <td>0.857558</td>\n",
       "      <td>0.857938</td>\n",
       "      <td>0.858645</td>\n",
       "      <td>0.858109</td>\n",
       "      <td>0.857694</td>\n",
       "      <td>0.859245</td>\n",
       "      <td>0.858657</td>\n",
       "      <td>0.859183</td>\n",
       "      <td>0.859758</td>\n",
       "      <td>0.858097</td>\n",
       "      <td>0.858479</td>\n",
       "      <td>0.000673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>25.231685</td>\n",
       "      <td>0.139000</td>\n",
       "      <td>1.013610</td>\n",
       "      <td>0.025277</td>\n",
       "      <td>False</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>100</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'vec__stem': False, 'vec__ngram_range': (2, 2...</td>\n",
       "      <td>0.830054</td>\n",
       "      <td>0.854810</td>\n",
       "      <td>0.847017</td>\n",
       "      <td>0.836152</td>\n",
       "      <td>0.835165</td>\n",
       "      <td>0.852246</td>\n",
       "      <td>0.846244</td>\n",
       "      <td>0.836710</td>\n",
       "      <td>0.846933</td>\n",
       "      <td>0.828341</td>\n",
       "      <td>0.834600</td>\n",
       "      <td>0.860548</td>\n",
       "      <td>0.842290</td>\n",
       "      <td>0.833931</td>\n",
       "      <td>0.841855</td>\n",
       "      <td>0.841793</td>\n",
       "      <td>0.009085</td>\n",
       "      <td>16</td>\n",
       "      <td>0.856001</td>\n",
       "      <td>0.857179</td>\n",
       "      <td>0.857215</td>\n",
       "      <td>0.857396</td>\n",
       "      <td>0.856938</td>\n",
       "      <td>0.855661</td>\n",
       "      <td>0.857299</td>\n",
       "      <td>0.856975</td>\n",
       "      <td>0.856542</td>\n",
       "      <td>0.855541</td>\n",
       "      <td>0.857372</td>\n",
       "      <td>0.855642</td>\n",
       "      <td>0.856409</td>\n",
       "      <td>0.857323</td>\n",
       "      <td>0.856686</td>\n",
       "      <td>0.856678</td>\n",
       "      <td>0.000655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>262.184039</td>\n",
       "      <td>0.948326</td>\n",
       "      <td>17.942864</td>\n",
       "      <td>0.326410</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>400</td>\n",
       "      <td>40000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'vec__stem': True, 'vec__ngram_range': (1, 2)...</td>\n",
       "      <td>0.816986</td>\n",
       "      <td>0.841727</td>\n",
       "      <td>0.837349</td>\n",
       "      <td>0.820482</td>\n",
       "      <td>0.815696</td>\n",
       "      <td>0.816351</td>\n",
       "      <td>0.827545</td>\n",
       "      <td>0.810188</td>\n",
       "      <td>0.807483</td>\n",
       "      <td>0.816181</td>\n",
       "      <td>0.817527</td>\n",
       "      <td>0.828818</td>\n",
       "      <td>0.821192</td>\n",
       "      <td>0.819594</td>\n",
       "      <td>0.813415</td>\n",
       "      <td>0.820704</td>\n",
       "      <td>0.009167</td>\n",
       "      <td>17</td>\n",
       "      <td>0.825331</td>\n",
       "      <td>0.824990</td>\n",
       "      <td>0.824625</td>\n",
       "      <td>0.825749</td>\n",
       "      <td>0.826585</td>\n",
       "      <td>0.823605</td>\n",
       "      <td>0.824127</td>\n",
       "      <td>0.826365</td>\n",
       "      <td>0.825705</td>\n",
       "      <td>0.823408</td>\n",
       "      <td>0.825129</td>\n",
       "      <td>0.825766</td>\n",
       "      <td>0.825245</td>\n",
       "      <td>0.825683</td>\n",
       "      <td>0.824197</td>\n",
       "      <td>0.825101</td>\n",
       "      <td>0.000915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>261.939638</td>\n",
       "      <td>0.887518</td>\n",
       "      <td>17.866828</td>\n",
       "      <td>0.298943</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>400</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'vec__stem': True, 'vec__ngram_range': (1, 2)...</td>\n",
       "      <td>0.816986</td>\n",
       "      <td>0.841727</td>\n",
       "      <td>0.837349</td>\n",
       "      <td>0.820482</td>\n",
       "      <td>0.815696</td>\n",
       "      <td>0.816351</td>\n",
       "      <td>0.827545</td>\n",
       "      <td>0.810188</td>\n",
       "      <td>0.807483</td>\n",
       "      <td>0.816181</td>\n",
       "      <td>0.817527</td>\n",
       "      <td>0.828818</td>\n",
       "      <td>0.821192</td>\n",
       "      <td>0.819594</td>\n",
       "      <td>0.813415</td>\n",
       "      <td>0.820704</td>\n",
       "      <td>0.009167</td>\n",
       "      <td>17</td>\n",
       "      <td>0.825331</td>\n",
       "      <td>0.824990</td>\n",
       "      <td>0.824625</td>\n",
       "      <td>0.825749</td>\n",
       "      <td>0.826585</td>\n",
       "      <td>0.823605</td>\n",
       "      <td>0.824127</td>\n",
       "      <td>0.826365</td>\n",
       "      <td>0.825705</td>\n",
       "      <td>0.823408</td>\n",
       "      <td>0.825129</td>\n",
       "      <td>0.825766</td>\n",
       "      <td>0.825245</td>\n",
       "      <td>0.825683</td>\n",
       "      <td>0.824197</td>\n",
       "      <td>0.825101</td>\n",
       "      <td>0.000915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>256.526303</td>\n",
       "      <td>1.066164</td>\n",
       "      <td>17.208825</td>\n",
       "      <td>0.401150</td>\n",
       "      <td>True</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>400</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>{'vec__stem': True, 'vec__ngram_range': (2, 2)...</td>\n",
       "      <td>0.766926</td>\n",
       "      <td>0.780546</td>\n",
       "      <td>0.786437</td>\n",
       "      <td>0.773574</td>\n",
       "      <td>0.767868</td>\n",
       "      <td>0.763062</td>\n",
       "      <td>0.786885</td>\n",
       "      <td>0.773881</td>\n",
       "      <td>0.769784</td>\n",
       "      <td>0.764497</td>\n",
       "      <td>0.760143</td>\n",
       "      <td>0.783234</td>\n",
       "      <td>0.766908</td>\n",
       "      <td>0.756853</td>\n",
       "      <td>0.770708</td>\n",
       "      <td>0.771422</td>\n",
       "      <td>0.008987</td>\n",
       "      <td>19</td>\n",
       "      <td>0.779268</td>\n",
       "      <td>0.779612</td>\n",
       "      <td>0.778029</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.778621</td>\n",
       "      <td>0.778655</td>\n",
       "      <td>0.777265</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.778342</td>\n",
       "      <td>0.777408</td>\n",
       "      <td>0.779105</td>\n",
       "      <td>0.778692</td>\n",
       "      <td>0.779528</td>\n",
       "      <td>0.780332</td>\n",
       "      <td>0.778171</td>\n",
       "      <td>0.778692</td>\n",
       "      <td>0.000851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>24.837714</td>\n",
       "      <td>0.167324</td>\n",
       "      <td>0.963338</td>\n",
       "      <td>0.028127</td>\n",
       "      <td>False</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>400</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'vec__stem': False, 'vec__ngram_range': (2, 2...</td>\n",
       "      <td>0.747889</td>\n",
       "      <td>0.760597</td>\n",
       "      <td>0.774116</td>\n",
       "      <td>0.753500</td>\n",
       "      <td>0.758661</td>\n",
       "      <td>0.743902</td>\n",
       "      <td>0.769772</td>\n",
       "      <td>0.761847</td>\n",
       "      <td>0.757869</td>\n",
       "      <td>0.737656</td>\n",
       "      <td>0.735383</td>\n",
       "      <td>0.764777</td>\n",
       "      <td>0.750754</td>\n",
       "      <td>0.735383</td>\n",
       "      <td>0.750306</td>\n",
       "      <td>0.753496</td>\n",
       "      <td>0.011637</td>\n",
       "      <td>20</td>\n",
       "      <td>0.760726</td>\n",
       "      <td>0.762351</td>\n",
       "      <td>0.760912</td>\n",
       "      <td>0.762780</td>\n",
       "      <td>0.760733</td>\n",
       "      <td>0.762015</td>\n",
       "      <td>0.759861</td>\n",
       "      <td>0.760617</td>\n",
       "      <td>0.761144</td>\n",
       "      <td>0.759998</td>\n",
       "      <td>0.762416</td>\n",
       "      <td>0.760642</td>\n",
       "      <td>0.762583</td>\n",
       "      <td>0.762543</td>\n",
       "      <td>0.760027</td>\n",
       "      <td>0.761290</td>\n",
       "      <td>0.001012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "11      27.338482      0.264187         1.195799        0.035670   \n",
       "0      256.710281      2.887174        17.397306        0.407629   \n",
       "5       33.380934      0.351356         1.609731        0.032869   \n",
       "17     265.768027      9.286911       768.698651     1913.983053   \n",
       "1       33.303984      0.316284         1.598839        0.052621   \n",
       "15     262.922652      1.083385        17.752333        0.420279   \n",
       "14     264.902432      0.850924        17.942699        0.342192   \n",
       "13      31.445339      0.172708         1.521602        0.033771   \n",
       "7      267.644572      6.375577        18.188342        0.242856   \n",
       "4      259.376821      2.669786        17.647668        0.405302   \n",
       "19    1777.653417   2500.229758        17.303805        4.820780   \n",
       "2      255.647084      1.375877        17.445716        0.374399   \n",
       "9      256.636996      2.613164        17.504603        0.388934   \n",
       "3      260.098398      0.965629        17.717461        0.371146   \n",
       "18      31.663341      0.489359         1.423488        0.086009   \n",
       "10      25.231685      0.139000         1.013610        0.025277   \n",
       "8      262.184039      0.948326        17.942864        0.326410   \n",
       "6      261.939638      0.887518        17.866828        0.298943   \n",
       "16     256.526303      1.066164        17.208825        0.401150   \n",
       "12      24.837714      0.167324         0.963338        0.028127   \n",
       "\n",
       "   param_vec__stem param_vec__ngram_range param_vec__min_df param_vec__max_df  \\\n",
       "11           False                 (2, 2)                 1                 1   \n",
       "0             True                 (2, 2)                 1                 1   \n",
       "5            False                 (1, 2)                 1              4000   \n",
       "17            True                 (2, 2)                 3                 1   \n",
       "1            False                 (1, 2)                 1             40000   \n",
       "15            True                 (1, 2)                 3                 1   \n",
       "14            True                 (1, 2)                 1             40000   \n",
       "13           False                 (1, 2)                 3                 1   \n",
       "7             True                 (1, 2)                 3             40000   \n",
       "4             True                 (1, 2)                50              4000   \n",
       "19            True                 (2, 2)                50              4000   \n",
       "2             True                 (2, 2)                50             40000   \n",
       "9             True                 (2, 2)                50             40000   \n",
       "3             True                 (1, 2)                50             40000   \n",
       "18           False                 (1, 2)               100             40000   \n",
       "10           False                 (2, 2)               100              4000   \n",
       "8             True                 (1, 2)               400             40000   \n",
       "6             True                 (1, 2)               400                 1   \n",
       "16            True                 (2, 2)               400                 1   \n",
       "12           False                 (2, 2)               400                 1   \n",
       "\n",
       "   param_clf__alpha                                             params  \\\n",
       "11                2  {'vec__stem': False, 'vec__ngram_range': (2, 2...   \n",
       "0                 2  {'vec__stem': True, 'vec__ngram_range': (2, 2)...   \n",
       "5                 3  {'vec__stem': False, 'vec__ngram_range': (1, 2...   \n",
       "17              0.1  {'vec__stem': True, 'vec__ngram_range': (2, 2)...   \n",
       "1               0.5  {'vec__stem': False, 'vec__ngram_range': (1, 2...   \n",
       "15              0.5  {'vec__stem': True, 'vec__ngram_range': (1, 2)...   \n",
       "14                3  {'vec__stem': True, 'vec__ngram_range': (1, 2)...   \n",
       "13                2  {'vec__stem': False, 'vec__ngram_range': (1, 2...   \n",
       "7                 2  {'vec__stem': True, 'vec__ngram_range': (1, 2)...   \n",
       "4               0.5  {'vec__stem': True, 'vec__ngram_range': (1, 2)...   \n",
       "19                2  {'vec__stem': True, 'vec__ngram_range': (2, 2)...   \n",
       "2               0.5  {'vec__stem': True, 'vec__ngram_range': (2, 2)...   \n",
       "9                 2  {'vec__stem': True, 'vec__ngram_range': (2, 2)...   \n",
       "3                 3  {'vec__stem': True, 'vec__ngram_range': (1, 2)...   \n",
       "18              0.5  {'vec__stem': False, 'vec__ngram_range': (1, 2...   \n",
       "10              0.1  {'vec__stem': False, 'vec__ngram_range': (2, 2...   \n",
       "8               0.5  {'vec__stem': True, 'vec__ngram_range': (1, 2)...   \n",
       "6               0.5  {'vec__stem': True, 'vec__ngram_range': (1, 2)...   \n",
       "16                3  {'vec__stem': True, 'vec__ngram_range': (2, 2)...   \n",
       "12              0.5  {'vec__stem': False, 'vec__ngram_range': (2, 2...   \n",
       "\n",
       "    split0_test_score  split1_test_score  split2_test_score  \\\n",
       "11           0.879511           0.900480           0.899094   \n",
       "0            0.879708           0.894387           0.901391   \n",
       "5            0.877724           0.897619           0.898082   \n",
       "17           0.879081           0.896716           0.890877   \n",
       "1            0.875000           0.893617           0.894833   \n",
       "15           0.870416           0.887009           0.889697   \n",
       "14           0.867764           0.881785           0.886473   \n",
       "13           0.864167           0.885385           0.881995   \n",
       "7            0.865031           0.880923           0.886199   \n",
       "4            0.861613           0.875224           0.868531   \n",
       "19           0.848594           0.867969           0.866430   \n",
       "2            0.847761           0.866469           0.863744   \n",
       "9            0.847761           0.865282           0.862722   \n",
       "3            0.847826           0.868990           0.867384   \n",
       "18           0.845969           0.866547           0.859206   \n",
       "10           0.830054           0.854810           0.847017   \n",
       "8            0.816986           0.841727           0.837349   \n",
       "6            0.816986           0.841727           0.837349   \n",
       "16           0.766926           0.780546           0.786437   \n",
       "12           0.747889           0.760597           0.774116   \n",
       "\n",
       "    split3_test_score  split4_test_score  split5_test_score  \\\n",
       "11           0.883273           0.889831           0.888485   \n",
       "0            0.879663           0.886623           0.888214   \n",
       "5            0.883582           0.886499           0.882424   \n",
       "17           0.877551           0.881618           0.884988   \n",
       "1            0.879854           0.885662           0.877839   \n",
       "15           0.871981           0.880435           0.875921   \n",
       "14           0.869148           0.879518           0.873394   \n",
       "13           0.873341           0.873716           0.874539   \n",
       "7            0.866989           0.872044           0.869458   \n",
       "4            0.868687           0.865316           0.863719   \n",
       "19           0.851742           0.850764           0.863905   \n",
       "2            0.851567           0.851089           0.859504   \n",
       "9            0.851240           0.851089           0.859338   \n",
       "3            0.855271           0.855443           0.851582   \n",
       "18           0.846655           0.850088           0.846013   \n",
       "10           0.836152           0.835165           0.852246   \n",
       "8            0.820482           0.815696           0.816351   \n",
       "6            0.820482           0.815696           0.816351   \n",
       "16           0.773574           0.767868           0.763062   \n",
       "12           0.753500           0.758661           0.743902   \n",
       "\n",
       "    split6_test_score  split7_test_score  split8_test_score  \\\n",
       "11           0.894231           0.893513           0.874156   \n",
       "0            0.894103           0.886978           0.878287   \n",
       "5            0.894358           0.892683           0.872881   \n",
       "17           0.882775           0.887668           0.882675   \n",
       "1            0.888082           0.885609           0.868389   \n",
       "15           0.880823           0.880835           0.858716   \n",
       "14           0.883023           0.877689           0.861314   \n",
       "13           0.885784           0.877601           0.861709   \n",
       "7            0.881643           0.875616           0.858716   \n",
       "4            0.875075           0.855596           0.861925   \n",
       "19           0.866785           0.856971           0.850450   \n",
       "2            0.864066           0.862627           0.849760   \n",
       "9            0.863394           0.862110           0.849249   \n",
       "3            0.859880           0.847498           0.838554   \n",
       "18           0.858164           0.842797           0.842676   \n",
       "10           0.846244           0.836710           0.846933   \n",
       "8            0.827545           0.810188           0.807483   \n",
       "6            0.827545           0.810188           0.807483   \n",
       "16           0.786885           0.773881           0.769784   \n",
       "12           0.769772           0.761847           0.757869   \n",
       "\n",
       "    split9_test_score  split10_test_score  split11_test_score  \\\n",
       "11           0.879569            0.871299            0.889851   \n",
       "0            0.871703            0.869565            0.892175   \n",
       "5            0.870968            0.870813            0.883436   \n",
       "17           0.866469            0.868516            0.888889   \n",
       "1            0.863388            0.862674            0.877085   \n",
       "15           0.856799            0.863005            0.877388   \n",
       "14           0.856802            0.859892            0.875616   \n",
       "13           0.856451            0.855407            0.868159   \n",
       "7            0.854732            0.856796            0.872908   \n",
       "4            0.855450            0.848810            0.876364   \n",
       "19           0.844626            0.839812            0.873728   \n",
       "2            0.845481            0.837758            0.871856   \n",
       "9            0.844133            0.835979            0.872010   \n",
       "3            0.843044            0.841916            0.867764   \n",
       "18           0.833136            0.835113            0.854015   \n",
       "10           0.828341            0.834600            0.860548   \n",
       "8            0.816181            0.817527            0.828818   \n",
       "6            0.816181            0.817527            0.828818   \n",
       "16           0.764497            0.760143            0.783234   \n",
       "12           0.737656            0.735383            0.764777   \n",
       "\n",
       "    split12_test_score  split13_test_score  split14_test_score  \\\n",
       "11            0.894417            0.889831            0.885487   \n",
       "0             0.895181            0.885801            0.886598   \n",
       "5             0.893848            0.880866            0.890909   \n",
       "17            0.897129            0.885009            0.878460   \n",
       "1             0.891743            0.876144            0.877538   \n",
       "15            0.886336            0.874543            0.876144   \n",
       "14            0.882850            0.876812            0.870146   \n",
       "13            0.883037            0.869671            0.877689   \n",
       "7             0.883636            0.871420            0.872705   \n",
       "4             0.881336            0.858525            0.872771   \n",
       "19            0.872856            0.853919            0.864800   \n",
       "2             0.872706            0.854093            0.861061   \n",
       "9             0.871674            0.852575            0.860382   \n",
       "3             0.863145            0.847682            0.855429   \n",
       "18            0.858164            0.836451            0.841471   \n",
       "10            0.842290            0.833931            0.841855   \n",
       "8             0.821192            0.819594            0.813415   \n",
       "6             0.821192            0.819594            0.813415   \n",
       "16            0.766908            0.756853            0.770708   \n",
       "12            0.750754            0.735383            0.750306   \n",
       "\n",
       "    mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
       "11         0.887536        0.008322                1            0.994407   \n",
       "0          0.886026        0.008624                2            0.990813   \n",
       "5          0.885114        0.009012                3            0.979375   \n",
       "17         0.883229        0.008423                4            0.982700   \n",
       "1          0.879833        0.009804                5            0.995441   \n",
       "15         0.875338        0.009532                6            0.953661   \n",
       "14         0.873483        0.008793                7            0.967574   \n",
       "13         0.872578        0.009540                8            0.940639   \n",
       "7          0.871255        0.009316                9            0.936261   \n",
       "4          0.865930        0.008875               10            0.882878   \n",
       "19         0.858223        0.010199               11            0.881586   \n",
       "2          0.857302        0.009726               12            0.878961   \n",
       "9          0.856595        0.009878               13            0.878069   \n",
       "3          0.854096        0.009479               14            0.868471   \n",
       "18         0.847767        0.009467               15            0.857619   \n",
       "10         0.841793        0.009085               16            0.856001   \n",
       "8          0.820704        0.009167               17            0.825331   \n",
       "6          0.820704        0.009167               17            0.825331   \n",
       "16         0.771422        0.008987               19            0.779268   \n",
       "12         0.753496        0.011637               20            0.760726   \n",
       "\n",
       "    split1_train_score  split2_train_score  split3_train_score  \\\n",
       "11            0.994103            0.994192            0.993845   \n",
       "0             0.990985            0.991289            0.990727   \n",
       "5             0.979666            0.979627            0.979459   \n",
       "17            0.982733            0.982449            0.982185   \n",
       "1             0.995743            0.995527            0.995570   \n",
       "15            0.953838            0.953813            0.952546   \n",
       "14            0.968639            0.968081            0.967355   \n",
       "13            0.941156            0.942409            0.940951   \n",
       "7             0.936900            0.937296            0.936840   \n",
       "4             0.883608            0.883612            0.883175   \n",
       "19            0.881221            0.881912            0.881735   \n",
       "2             0.878810            0.879573            0.879013   \n",
       "9             0.878127            0.878444            0.878460   \n",
       "3             0.869632            0.869185            0.869311   \n",
       "18            0.858559            0.858155            0.858427   \n",
       "10            0.857179            0.857215            0.857396   \n",
       "8             0.824990            0.824625            0.825749   \n",
       "6             0.824990            0.824625            0.825749   \n",
       "16            0.779612            0.778029            0.779569   \n",
       "12            0.762351            0.760912            0.762780   \n",
       "\n",
       "    split4_train_score  split5_train_score  split6_train_score  \\\n",
       "11            0.994320            0.993931            0.993497   \n",
       "0             0.990725            0.990857            0.990682   \n",
       "5             0.979678            0.979325            0.979219   \n",
       "17            0.982060            0.982231            0.982394   \n",
       "1             0.995571            0.995311            0.995440   \n",
       "15            0.953447            0.953566            0.953534   \n",
       "14            0.968305            0.967187            0.967574   \n",
       "13            0.941713            0.940915            0.941279   \n",
       "7             0.936677            0.936514            0.936953   \n",
       "4             0.883993            0.883582            0.882718   \n",
       "19            0.881665            0.881824            0.880872   \n",
       "2             0.879925            0.878088            0.877208   \n",
       "9             0.878938            0.877467            0.876655   \n",
       "3             0.870304            0.868733            0.868210   \n",
       "18            0.859537            0.857558            0.857938   \n",
       "10            0.856938            0.855661            0.857299   \n",
       "8             0.826585            0.823605            0.824127   \n",
       "6             0.826585            0.823605            0.824127   \n",
       "16            0.778621            0.778655            0.777265   \n",
       "12            0.760733            0.762015            0.759861   \n",
       "\n",
       "    split7_train_score  split8_train_score  split9_train_score  \\\n",
       "11            0.994017            0.993844            0.993931   \n",
       "0             0.990724            0.990813            0.990594   \n",
       "5             0.979449            0.979458            0.979235   \n",
       "17            0.981748            0.982399            0.981300   \n",
       "1             0.995527            0.995441            0.995225   \n",
       "15            0.952996            0.953617            0.953166   \n",
       "14            0.967661            0.968036            0.967862   \n",
       "13            0.940281            0.941356            0.939967   \n",
       "7             0.936281            0.936524            0.934988   \n",
       "4             0.884472            0.882768            0.882451   \n",
       "19            0.881770            0.881498            0.880557   \n",
       "2             0.880007            0.878382            0.877570   \n",
       "9             0.878671            0.877586            0.876497   \n",
       "3             0.870181            0.868628            0.867605   \n",
       "18            0.858645            0.858109            0.857694   \n",
       "10            0.856975            0.856542            0.855541   \n",
       "8             0.826365            0.825705            0.823408   \n",
       "6             0.826365            0.825705            0.823408   \n",
       "16            0.777778            0.778342            0.777408   \n",
       "12            0.760617            0.761144            0.759998   \n",
       "\n",
       "    split10_train_score  split11_train_score  split12_train_score  \\\n",
       "11             0.993844             0.993628             0.993585   \n",
       "0              0.990724             0.990901             0.990725   \n",
       "5              0.979654             0.979410             0.979263   \n",
       "17             0.982789             0.982533             0.981712   \n",
       "1              0.995829             0.995484             0.995440   \n",
       "15             0.953954             0.953371             0.952840   \n",
       "14             0.967585             0.968347             0.967001   \n",
       "13             0.941141             0.941320             0.940695   \n",
       "7              0.936725             0.936161             0.935628   \n",
       "4              0.884230             0.883335             0.882906   \n",
       "19             0.882345             0.880820             0.880427   \n",
       "2              0.880662             0.878460             0.878051   \n",
       "9              0.879891             0.877606             0.877339   \n",
       "3              0.870316             0.868274             0.869050   \n",
       "18             0.859245             0.858657             0.859183   \n",
       "10             0.857372             0.855642             0.856409   \n",
       "8              0.825129             0.825766             0.825245   \n",
       "6              0.825129             0.825766             0.825245   \n",
       "16             0.779105             0.778692             0.779528   \n",
       "12             0.762416             0.760642             0.762583   \n",
       "\n",
       "    split13_train_score  split14_train_score  mean_train_score  \\\n",
       "11             0.994105             0.993973          0.993948   \n",
       "0              0.990987             0.990684          0.990815   \n",
       "5              0.979719             0.978902          0.979429   \n",
       "17             0.982362             0.982228          0.982255   \n",
       "1              0.995528             0.995398          0.995498   \n",
       "15             0.954670             0.953479          0.953500   \n",
       "14             0.968249             0.967347          0.967787   \n",
       "13             0.943111             0.940475          0.941161   \n",
       "7              0.938130             0.936057          0.936529   \n",
       "4              0.884752             0.882114          0.883373   \n",
       "19             0.882864             0.881635          0.881515   \n",
       "2              0.879640             0.878113          0.878831   \n",
       "9              0.878690             0.877856          0.878020   \n",
       "3              0.871126             0.867996          0.869135   \n",
       "18             0.859758             0.858097          0.858479   \n",
       "10             0.857323             0.856686          0.856678   \n",
       "8              0.825683             0.824197          0.825101   \n",
       "6              0.825683             0.824197          0.825101   \n",
       "16             0.780332             0.778171          0.778692   \n",
       "12             0.762543             0.760027          0.761290   \n",
       "\n",
       "    std_train_score  \n",
       "11         0.000249  \n",
       "0          0.000167  \n",
       "5          0.000215  \n",
       "17         0.000399  \n",
       "1          0.000145  \n",
       "15         0.000487  \n",
       "14         0.000460  \n",
       "13         0.000775  \n",
       "7          0.000698  \n",
       "4          0.000736  \n",
       "19         0.000633  \n",
       "2          0.000946  \n",
       "9          0.000854  \n",
       "3          0.000973  \n",
       "18         0.000673  \n",
       "10         0.000655  \n",
       "8          0.000915  \n",
       "6          0.000915  \n",
       "16         0.000851  \n",
       "12         0.001012  "
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(rs_mnv_bow.cv_results_).sort_values(by=['mean_test_score'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering, binary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(485.6470880508423, {'clf__smooth': 0.07653511447789829}, 0.8528520285195769)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_bnv = {\n",
    "    'clf__smooth': uniform(0,2)\n",
    "}\n",
    "\n",
    "bnv = vect = LemmaCountVectorizer(strip_accents='unicode', stop_words=None, binary=True, \n",
    "                                stem=True, ngram_range=(1, 2), min_df=250, max_df=4000, preprocessing=True)\n",
    "X_train_bnv = bnv.fit_transform(X_train)\n",
    "\n",
    "pipeline_bnv = Pipeline([\n",
    "    ('clf', BernoulliNaiveBayes())\n",
    "])\n",
    "\n",
    "gs_bnv = RandomizedSearchCV(pipeline_bnv, params_bnv, \n",
    "                                   cv=10, scoring=score, verbose=0, return_train_score=True)\n",
    "start = time.time()\n",
    "gs_bnv.fit(X_train_bnv.toarray(), y)\n",
    "\n",
    "# scores = cross_val_score(bnv_clf, X_train_vect.toarray(), y, cv=20, scoring='f1', n_jobs=-1)\n",
    "# time.time() - start, scores, scores.mean(), scores.std()\n",
    "time.time() - start, gs_bnv.best_params_, gs_bnv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_clf__smooth</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>split5_train_score</th>\n",
       "      <th>split6_train_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>split8_train_score</th>\n",
       "      <th>split9_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.232343</td>\n",
       "      <td>0.059794</td>\n",
       "      <td>0.100694</td>\n",
       "      <td>0.004363</td>\n",
       "      <td>0.0765351</td>\n",
       "      <td>{'clf__smooth': 0.07653511447789829}</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>0.859628</td>\n",
       "      <td>0.853375</td>\n",
       "      <td>0.850119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.860644</td>\n",
       "      <td>0.861239</td>\n",
       "      <td>0.861396</td>\n",
       "      <td>0.859468</td>\n",
       "      <td>0.861672</td>\n",
       "      <td>0.861179</td>\n",
       "      <td>0.860748</td>\n",
       "      <td>0.861650</td>\n",
       "      <td>0.860863</td>\n",
       "      <td>0.000670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.128640</td>\n",
       "      <td>0.026545</td>\n",
       "      <td>0.095614</td>\n",
       "      <td>0.000912</td>\n",
       "      <td>0.33695</td>\n",
       "      <td>{'clf__smooth': 0.3369503010062169}</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>0.859628</td>\n",
       "      <td>0.853375</td>\n",
       "      <td>0.850119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.860594</td>\n",
       "      <td>0.861152</td>\n",
       "      <td>0.861309</td>\n",
       "      <td>0.859506</td>\n",
       "      <td>0.861672</td>\n",
       "      <td>0.861141</td>\n",
       "      <td>0.860848</td>\n",
       "      <td>0.861699</td>\n",
       "      <td>0.860846</td>\n",
       "      <td>0.000662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.109517</td>\n",
       "      <td>0.025445</td>\n",
       "      <td>0.097078</td>\n",
       "      <td>0.002044</td>\n",
       "      <td>0.343728</td>\n",
       "      <td>{'clf__smooth': 0.34372788670152166}</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>0.859628</td>\n",
       "      <td>0.853375</td>\n",
       "      <td>0.850119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.860594</td>\n",
       "      <td>0.861152</td>\n",
       "      <td>0.861309</td>\n",
       "      <td>0.859506</td>\n",
       "      <td>0.861672</td>\n",
       "      <td>0.861141</td>\n",
       "      <td>0.860848</td>\n",
       "      <td>0.861699</td>\n",
       "      <td>0.860846</td>\n",
       "      <td>0.000662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.130950</td>\n",
       "      <td>0.035579</td>\n",
       "      <td>0.097852</td>\n",
       "      <td>0.002570</td>\n",
       "      <td>0.205979</td>\n",
       "      <td>{'clf__smooth': 0.20597933983692807}</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>0.859628</td>\n",
       "      <td>0.853375</td>\n",
       "      <td>0.850119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.860644</td>\n",
       "      <td>0.861202</td>\n",
       "      <td>0.861309</td>\n",
       "      <td>0.859468</td>\n",
       "      <td>0.861672</td>\n",
       "      <td>0.861179</td>\n",
       "      <td>0.860848</td>\n",
       "      <td>0.861650</td>\n",
       "      <td>0.860851</td>\n",
       "      <td>0.000666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.165793</td>\n",
       "      <td>0.083789</td>\n",
       "      <td>0.097643</td>\n",
       "      <td>0.002877</td>\n",
       "      <td>0.542786</td>\n",
       "      <td>{'clf__smooth': 0.5427860801725255}</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>0.859628</td>\n",
       "      <td>0.853375</td>\n",
       "      <td>0.850119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.860556</td>\n",
       "      <td>0.861339</td>\n",
       "      <td>0.861284</td>\n",
       "      <td>0.859555</td>\n",
       "      <td>0.861646</td>\n",
       "      <td>0.861141</td>\n",
       "      <td>0.860860</td>\n",
       "      <td>0.861699</td>\n",
       "      <td>0.860858</td>\n",
       "      <td>0.000668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.130206</td>\n",
       "      <td>0.028252</td>\n",
       "      <td>0.096907</td>\n",
       "      <td>0.001243</td>\n",
       "      <td>0.513698</td>\n",
       "      <td>{'clf__smooth': 0.5136977116067787}</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>0.859628</td>\n",
       "      <td>0.853375</td>\n",
       "      <td>0.850119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.860556</td>\n",
       "      <td>0.861252</td>\n",
       "      <td>0.861284</td>\n",
       "      <td>0.859555</td>\n",
       "      <td>0.861646</td>\n",
       "      <td>0.861141</td>\n",
       "      <td>0.860860</td>\n",
       "      <td>0.861699</td>\n",
       "      <td>0.860844</td>\n",
       "      <td>0.000665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.123743</td>\n",
       "      <td>0.030240</td>\n",
       "      <td>0.097057</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>0.99687</td>\n",
       "      <td>{'clf__smooth': 0.9968695443754341}</td>\n",
       "      <td>0.861827</td>\n",
       "      <td>0.859628</td>\n",
       "      <td>0.853375</td>\n",
       "      <td>0.850119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.860606</td>\n",
       "      <td>0.861276</td>\n",
       "      <td>0.861334</td>\n",
       "      <td>0.859617</td>\n",
       "      <td>0.861696</td>\n",
       "      <td>0.861153</td>\n",
       "      <td>0.860785</td>\n",
       "      <td>0.861624</td>\n",
       "      <td>0.860858</td>\n",
       "      <td>0.000654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.130850</td>\n",
       "      <td>0.031603</td>\n",
       "      <td>0.097292</td>\n",
       "      <td>0.001526</td>\n",
       "      <td>0.975939</td>\n",
       "      <td>{'clf__smooth': 0.9759385602089541}</td>\n",
       "      <td>0.861827</td>\n",
       "      <td>0.859628</td>\n",
       "      <td>0.853375</td>\n",
       "      <td>0.850119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.860606</td>\n",
       "      <td>0.861313</td>\n",
       "      <td>0.861334</td>\n",
       "      <td>0.859617</td>\n",
       "      <td>0.861696</td>\n",
       "      <td>0.861153</td>\n",
       "      <td>0.860785</td>\n",
       "      <td>0.861624</td>\n",
       "      <td>0.860861</td>\n",
       "      <td>0.000657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.226222</td>\n",
       "      <td>0.097866</td>\n",
       "      <td>0.103614</td>\n",
       "      <td>0.018131</td>\n",
       "      <td>1.24639</td>\n",
       "      <td>{'clf__smooth': 1.2463857325411893}</td>\n",
       "      <td>0.861827</td>\n",
       "      <td>0.859628</td>\n",
       "      <td>0.853044</td>\n",
       "      <td>0.849663</td>\n",
       "      <td>...</td>\n",
       "      <td>0.860531</td>\n",
       "      <td>0.861288</td>\n",
       "      <td>0.861383</td>\n",
       "      <td>0.859629</td>\n",
       "      <td>0.861646</td>\n",
       "      <td>0.861078</td>\n",
       "      <td>0.860759</td>\n",
       "      <td>0.861624</td>\n",
       "      <td>0.860838</td>\n",
       "      <td>0.000657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.118246</td>\n",
       "      <td>0.038577</td>\n",
       "      <td>0.098309</td>\n",
       "      <td>0.002291</td>\n",
       "      <td>1.24479</td>\n",
       "      <td>{'clf__smooth': 1.2447870787653956}</td>\n",
       "      <td>0.861827</td>\n",
       "      <td>0.859628</td>\n",
       "      <td>0.853044</td>\n",
       "      <td>0.849663</td>\n",
       "      <td>...</td>\n",
       "      <td>0.860531</td>\n",
       "      <td>0.861288</td>\n",
       "      <td>0.861383</td>\n",
       "      <td>0.859629</td>\n",
       "      <td>0.861646</td>\n",
       "      <td>0.861078</td>\n",
       "      <td>0.860759</td>\n",
       "      <td>0.861624</td>\n",
       "      <td>0.860838</td>\n",
       "      <td>0.000657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       3.232343      0.059794         0.100694        0.004363   \n",
       "4       3.128640      0.026545         0.095614        0.000912   \n",
       "5       3.109517      0.025445         0.097078        0.002044   \n",
       "6       3.130950      0.035579         0.097852        0.002570   \n",
       "1       3.165793      0.083789         0.097643        0.002877   \n",
       "9       3.130206      0.028252         0.096907        0.001243   \n",
       "7       3.123743      0.030240         0.097057        0.001695   \n",
       "8       3.130850      0.031603         0.097292        0.001526   \n",
       "2       3.226222      0.097866         0.103614        0.018131   \n",
       "3       3.118246      0.038577         0.098309        0.002291   \n",
       "\n",
       "  param_clf__smooth                                params  split0_test_score  \\\n",
       "0         0.0765351  {'clf__smooth': 0.07653511447789829}           0.862500   \n",
       "4           0.33695   {'clf__smooth': 0.3369503010062169}           0.862500   \n",
       "5          0.343728  {'clf__smooth': 0.34372788670152166}           0.862500   \n",
       "6          0.205979  {'clf__smooth': 0.20597933983692807}           0.862500   \n",
       "1          0.542786   {'clf__smooth': 0.5427860801725255}           0.862500   \n",
       "9          0.513698   {'clf__smooth': 0.5136977116067787}           0.862500   \n",
       "7           0.99687   {'clf__smooth': 0.9968695443754341}           0.861827   \n",
       "8          0.975939   {'clf__smooth': 0.9759385602089541}           0.861827   \n",
       "2           1.24639   {'clf__smooth': 1.2463857325411893}           0.861827   \n",
       "3           1.24479   {'clf__smooth': 1.2447870787653956}           0.861827   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  ...  \\\n",
       "0           0.859628           0.853375           0.850119  ...   \n",
       "4           0.859628           0.853375           0.850119  ...   \n",
       "5           0.859628           0.853375           0.850119  ...   \n",
       "6           0.859628           0.853375           0.850119  ...   \n",
       "1           0.859628           0.853375           0.850119  ...   \n",
       "9           0.859628           0.853375           0.850119  ...   \n",
       "7           0.859628           0.853375           0.850119  ...   \n",
       "8           0.859628           0.853375           0.850119  ...   \n",
       "2           0.859628           0.853044           0.849663  ...   \n",
       "3           0.859628           0.853044           0.849663  ...   \n",
       "\n",
       "   split2_train_score  split3_train_score  split4_train_score  \\\n",
       "0            0.860644            0.861239            0.861396   \n",
       "4            0.860594            0.861152            0.861309   \n",
       "5            0.860594            0.861152            0.861309   \n",
       "6            0.860644            0.861202            0.861309   \n",
       "1            0.860556            0.861339            0.861284   \n",
       "9            0.860556            0.861252            0.861284   \n",
       "7            0.860606            0.861276            0.861334   \n",
       "8            0.860606            0.861313            0.861334   \n",
       "2            0.860531            0.861288            0.861383   \n",
       "3            0.860531            0.861288            0.861383   \n",
       "\n",
       "   split5_train_score  split6_train_score  split7_train_score  \\\n",
       "0            0.859468            0.861672            0.861179   \n",
       "4            0.859506            0.861672            0.861141   \n",
       "5            0.859506            0.861672            0.861141   \n",
       "6            0.859468            0.861672            0.861179   \n",
       "1            0.859555            0.861646            0.861141   \n",
       "9            0.859555            0.861646            0.861141   \n",
       "7            0.859617            0.861696            0.861153   \n",
       "8            0.859617            0.861696            0.861153   \n",
       "2            0.859629            0.861646            0.861078   \n",
       "3            0.859629            0.861646            0.861078   \n",
       "\n",
       "   split8_train_score  split9_train_score  mean_train_score  std_train_score  \n",
       "0            0.860748            0.861650          0.860863         0.000670  \n",
       "4            0.860848            0.861699          0.860846         0.000662  \n",
       "5            0.860848            0.861699          0.860846         0.000662  \n",
       "6            0.860848            0.861650          0.860851         0.000666  \n",
       "1            0.860860            0.861699          0.860858         0.000668  \n",
       "9            0.860860            0.861699          0.860844         0.000665  \n",
       "7            0.860785            0.861624          0.860858         0.000654  \n",
       "8            0.860785            0.861624          0.860861         0.000657  \n",
       "2            0.860759            0.861624          0.860838         0.000657  \n",
       "3            0.860759            0.861624          0.860838         0.000657  \n",
       "\n",
       "[10 rows x 31 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(gs_bnv.cv_results_).sort_values(by=['mean_test_score'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1134.332978963852,\n",
       " {'vec__stem': True,\n",
       "  'vec__ngram_range': (1, 2),\n",
       "  'vec__min_df': 50,\n",
       "  'vec__max_df': 4000,\n",
       "  'clf__min_samples_split': 2,\n",
       "  'clf__min_samples_leaf': 1,\n",
       "  'clf__max_depth': 300},\n",
       " 0.7072313256744643)"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_tree = {\n",
    "    'vec__min_df': [3, 50, 100, 400],\n",
    "    'vec__max_df': [4000, 40000],\n",
    "    'vec__stem': [True],\n",
    "    'vec__ngram_range':[(1, 2), (2, 2)],\n",
    "    'clf__max_depth': [300, 500, 700],\n",
    "    'clf__min_samples_split': [1, 2, 5],\n",
    "    'clf__min_samples_leaf': [1, 2, 5]\n",
    "}\n",
    "\n",
    "pipeline_tree = Pipeline([\n",
    "    ('vec', LemmaCountVectorizer(strip_accents='unicode', stop_words=None, binary=False)),\n",
    "    ('clf', DecisionTreeClassifier())\n",
    "])\n",
    "                  \n",
    "rs_tree = RandomizedSearchCV(pipeline_tree, parameters_tree, \n",
    "                                   cv=10, scoring=score, n_jobs=-1, verbose=0, random_state=62, n_iter=1\n",
    "                               , return_train_score=True)\n",
    "start = time.time()\n",
    "rs_tree.fit(X_train, y)\n",
    "\n",
    "time.time() - start, rs_tree.best_params_, rs_tree.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2105709,
     "status": "ok",
     "timestamp": 1549504031186,
     "user": {
      "displayName": "FRANCISCO XAVIER SUMBA TORAL",
      "photoUrl": "",
      "userId": "11128736706802637809"
     },
     "user_tz": 300
    },
    "id": "FyIB_vuE3ZtA",
    "outputId": "e5e5a82e-7192-45ab-bbda-d548a03689b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "Pickling array (shape=(25000,), dtype=object).\n",
      "Pickling array (shape=(25000,), dtype=int64).\n",
      "Pickling array (shape=(12500,), dtype=int64).\n",
      "Pickling array (shape=(12500,), dtype=int64).\n",
      "Pickling array (shape=(25000,), dtype=object).\n",
      "Pickling array (shape=(25000,), dtype=int64).\n",
      "Pickling array (shape=(12500,), dtype=int64).\n",
      "Pickling array (shape=(12500,), dtype=int64).\n",
      "Pickling array (shape=(25000,), dtype=object).\n",
      "Pickling array (shape=(25000,), dtype=int64).\n",
      "Pickling array (shape=(12500,), dtype=int64).\n",
      "Pickling array (shape=(12500,), dtype=int64).\n",
      "Pickling array (shape=(25000,), dtype=object).\n",
      "Pickling array (shape=(25000,), dtype=int64).\n",
      "Pickling array (shape=(12500,), dtype=int64).\n",
      "Pickling array (shape=(12500,), dtype=int64).\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed: 15.2min\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   4 | elapsed: 15.2min remaining: 15.2min\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed: 15.2min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed: 15.2min finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cuent/anaconda/envs/comp551-proj2/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1747.760658979416, {'clf__degree': 3, 'clf__C': 2.5}, 0.8262288670868729)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_svm = {\n",
    "    'clf__degree': [2, 3, 4], \n",
    "    'clf__C': [.5, 1.0, 2.5, 3], \n",
    "}\n",
    "\n",
    "pipeline_svm = Pipeline([\n",
    "    ('vec', LemmaCountVectorizer(strip_accents='unicode', stop_words=None, ngram_range=(1,2), \n",
    "                                 min_df=3, max_df=400, stem=True, binary=False)),\n",
    "    ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True,)),\n",
    "    ('clf', SVC())\n",
    "])\n",
    "                  \n",
    "rs_svm = RandomizedSearchCV(pipeline_svm, parameters_svm, cv=2, scoring=score, n_jobs=-1, \n",
    "                            verbose=1000, random_state=62, n_iter=2, return_train_score=True)\n",
    "start = time.time()\n",
    "rs_svm.fit(X_train, y)\n",
    "time.time() - start, rs_svm.best_params_, rs_svm.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "Pickling array (shape=(25000,), dtype=object).\n",
      "Pickling array (shape=(25000,), dtype=int64).\n",
      "Pickling array (shape=(12500,), dtype=int64).\n",
      "Pickling array (shape=(12500,), dtype=int64).\n",
      "Pickling array (shape=(25000,), dtype=object).\n",
      "Pickling array (shape=(25000,), dtype=int64).\n",
      "Pickling array (shape=(12500,), dtype=int64).\n",
      "Pickling array (shape=(12500,), dtype=int64).\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  3.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  3.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(310.9432089328766, {}, 0.8958614175826981)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_linearsvm = {\n",
    "#     'clf__degree': [2, 3, 4], \n",
    "#     'clf__C': [.5, 1.0, 2.5, 3], \n",
    "}\n",
    "\n",
    "pipeline_linearsvm = Pipeline([\n",
    "    ('vec', LemmaCountVectorizer(strip_accents='unicode', stop_words=None, ngram_range=(1,2), \n",
    "                                 min_df=5, max_df=.8, stem=True, binary=False, preprocessing=True)),\n",
    "    ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=True, use_idf=True)),\n",
    "    ('clf', LinearSVC())\n",
    "])\n",
    "                  \n",
    "rs_linearsvm = GridSearchCV(pipeline_linearsvm, parameters_linearsvm, \n",
    "                                   cv=2, scoring=score, n_jobs=-1, verbose=1000, \n",
    "                                         return_train_score=True)\n",
    "start = time.time()\n",
    "rs_linearsvm.fit(X_train, y)\n",
    "time.time() - start, rs_linearsvm.best_params_, rs_linearsvm.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NBSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed: 55.4min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed: 97.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5935.145317077637,\n",
       " {'clf__C': 63.31041288467941,\n",
       "  'clf__alpha': 0.6492837008604102,\n",
       "  'clf__beta': 0.2736639871258223,\n",
       "  'vect__max_df': 1.0,\n",
       "  'vect__min_df': 2,\n",
       "  'vect__ngram_range': (1, 3)},\n",
       " 0.9164024781200871)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import uniform\n",
    "\n",
    "parameters_nbsvm = {\n",
    "    'vect__ngram_range': [(1,2), (1,3)],\n",
    "    'vect__min_df': [2],\n",
    "    'vect__max_df': [1.0],\n",
    "    'clf__beta': uniform(0,1),\n",
    "    'clf__alpha': uniform(.5,1),\n",
    "    'clf__C': uniform(30, 40)\n",
    "}\n",
    "pipeline_nbsvm = Pipeline([\n",
    "    ('vect', LemmaCountVectorizer(strip_accents='unicode', stop_words=None,stem=False, preprocessing=True)),\n",
    "    ('clf', NBSVM())\n",
    "])\n",
    "\n",
    "gs_nbsvm = RandomizedSearchCV(pipeline_nbsvm, parameters_nbsvm, \n",
    "                                   cv=10, scoring=score, n_jobs=-1, verbose=3,\n",
    "                                 return_train_score=True, n_iter=20, random_state=62)\n",
    "start = time.time()\n",
    "gs_nbsvm.fit(X_train, y)\n",
    "time.time()-start, gs_nbsvm.best_params_, gs_nbsvm.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_clf__C</th>\n",
       "      <th>param_clf__alpha</th>\n",
       "      <th>param_clf__beta</th>\n",
       "      <th>param_vect__max_df</th>\n",
       "      <th>param_vect__min_df</th>\n",
       "      <th>param_vect__ngram_range</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>split5_train_score</th>\n",
       "      <th>split6_train_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>split8_train_score</th>\n",
       "      <th>split9_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>108.174109</td>\n",
       "      <td>12.945563</td>\n",
       "      <td>5.019913</td>\n",
       "      <td>0.303700</td>\n",
       "      <td>63.3104</td>\n",
       "      <td>0.649284</td>\n",
       "      <td>0.273664</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>{'clf__C': 63.31041288467941, 'clf__alpha': 0....</td>\n",
       "      <td>0.920143</td>\n",
       "      <td>0.918424</td>\n",
       "      <td>0.913541</td>\n",
       "      <td>0.915970</td>\n",
       "      <td>0.919284</td>\n",
       "      <td>0.915470</td>\n",
       "      <td>0.898024</td>\n",
       "      <td>0.921304</td>\n",
       "      <td>0.921053</td>\n",
       "      <td>0.920812</td>\n",
       "      <td>0.916402</td>\n",
       "      <td>0.006622</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999956</td>\n",
       "      <td>0.999956</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>238.186817</td>\n",
       "      <td>55.525326</td>\n",
       "      <td>5.649907</td>\n",
       "      <td>0.250644</td>\n",
       "      <td>58.365</td>\n",
       "      <td>1.38641</td>\n",
       "      <td>0.282825</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>{'clf__C': 58.36497474652987, 'clf__alpha': 1....</td>\n",
       "      <td>0.920382</td>\n",
       "      <td>0.918660</td>\n",
       "      <td>0.916996</td>\n",
       "      <td>0.913511</td>\n",
       "      <td>0.920080</td>\n",
       "      <td>0.911706</td>\n",
       "      <td>0.896361</td>\n",
       "      <td>0.919952</td>\n",
       "      <td>0.922524</td>\n",
       "      <td>0.922280</td>\n",
       "      <td>0.916245</td>\n",
       "      <td>0.007425</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999822</td>\n",
       "      <td>0.999822</td>\n",
       "      <td>0.999956</td>\n",
       "      <td>0.999911</td>\n",
       "      <td>0.999911</td>\n",
       "      <td>0.999911</td>\n",
       "      <td>0.999822</td>\n",
       "      <td>0.999956</td>\n",
       "      <td>0.999867</td>\n",
       "      <td>0.999911</td>\n",
       "      <td>0.999889</td>\n",
       "      <td>0.000050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>180.406100</td>\n",
       "      <td>52.394044</td>\n",
       "      <td>5.466442</td>\n",
       "      <td>0.374667</td>\n",
       "      <td>41.295</td>\n",
       "      <td>0.791467</td>\n",
       "      <td>0.51763</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>{'clf__C': 41.29497015311559, 'clf__alpha': 0....</td>\n",
       "      <td>0.918940</td>\n",
       "      <td>0.916899</td>\n",
       "      <td>0.912667</td>\n",
       "      <td>0.918359</td>\n",
       "      <td>0.915079</td>\n",
       "      <td>0.909163</td>\n",
       "      <td>0.905138</td>\n",
       "      <td>0.918489</td>\n",
       "      <td>0.919952</td>\n",
       "      <td>0.921856</td>\n",
       "      <td>0.915654</td>\n",
       "      <td>0.004966</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>109.667099</td>\n",
       "      <td>12.395941</td>\n",
       "      <td>5.154001</td>\n",
       "      <td>0.241083</td>\n",
       "      <td>58.8777</td>\n",
       "      <td>0.910497</td>\n",
       "      <td>0.20378</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>{'clf__C': 58.877698434633004, 'clf__alpha': 0...</td>\n",
       "      <td>0.919091</td>\n",
       "      <td>0.917131</td>\n",
       "      <td>0.911024</td>\n",
       "      <td>0.912714</td>\n",
       "      <td>0.920572</td>\n",
       "      <td>0.914698</td>\n",
       "      <td>0.898734</td>\n",
       "      <td>0.919824</td>\n",
       "      <td>0.921116</td>\n",
       "      <td>0.920749</td>\n",
       "      <td>0.915565</td>\n",
       "      <td>0.006546</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999778</td>\n",
       "      <td>0.999778</td>\n",
       "      <td>0.999867</td>\n",
       "      <td>0.999778</td>\n",
       "      <td>0.999822</td>\n",
       "      <td>0.999822</td>\n",
       "      <td>0.999778</td>\n",
       "      <td>0.999822</td>\n",
       "      <td>0.999778</td>\n",
       "      <td>0.999778</td>\n",
       "      <td>0.999800</td>\n",
       "      <td>0.000030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>168.066260</td>\n",
       "      <td>40.028676</td>\n",
       "      <td>3.945833</td>\n",
       "      <td>0.183873</td>\n",
       "      <td>63.9887</td>\n",
       "      <td>0.863862</td>\n",
       "      <td>0.335656</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'clf__C': 63.988721188429295, 'clf__alpha': 0...</td>\n",
       "      <td>0.919284</td>\n",
       "      <td>0.918229</td>\n",
       "      <td>0.913718</td>\n",
       "      <td>0.919760</td>\n",
       "      <td>0.917263</td>\n",
       "      <td>0.914902</td>\n",
       "      <td>0.899046</td>\n",
       "      <td>0.910621</td>\n",
       "      <td>0.923016</td>\n",
       "      <td>0.918465</td>\n",
       "      <td>0.915431</td>\n",
       "      <td>0.006374</td>\n",
       "      <td>5</td>\n",
       "      <td>0.999778</td>\n",
       "      <td>0.999733</td>\n",
       "      <td>0.999822</td>\n",
       "      <td>0.999689</td>\n",
       "      <td>0.999778</td>\n",
       "      <td>0.999733</td>\n",
       "      <td>0.999733</td>\n",
       "      <td>0.999778</td>\n",
       "      <td>0.999778</td>\n",
       "      <td>0.999733</td>\n",
       "      <td>0.999756</td>\n",
       "      <td>0.000036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>230.771548</td>\n",
       "      <td>49.179394</td>\n",
       "      <td>4.138723</td>\n",
       "      <td>0.116819</td>\n",
       "      <td>33.5631</td>\n",
       "      <td>0.636795</td>\n",
       "      <td>0.297257</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'clf__C': 33.563094208219184, 'clf__alpha': 0...</td>\n",
       "      <td>0.917994</td>\n",
       "      <td>0.917431</td>\n",
       "      <td>0.913199</td>\n",
       "      <td>0.917497</td>\n",
       "      <td>0.915903</td>\n",
       "      <td>0.916733</td>\n",
       "      <td>0.898769</td>\n",
       "      <td>0.912435</td>\n",
       "      <td>0.920495</td>\n",
       "      <td>0.917666</td>\n",
       "      <td>0.914812</td>\n",
       "      <td>0.005789</td>\n",
       "      <td>6</td>\n",
       "      <td>0.999644</td>\n",
       "      <td>0.999689</td>\n",
       "      <td>0.999822</td>\n",
       "      <td>0.999689</td>\n",
       "      <td>0.999733</td>\n",
       "      <td>0.999689</td>\n",
       "      <td>0.999733</td>\n",
       "      <td>0.999644</td>\n",
       "      <td>0.999689</td>\n",
       "      <td>0.999689</td>\n",
       "      <td>0.999702</td>\n",
       "      <td>0.000049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>209.410699</td>\n",
       "      <td>64.977677</td>\n",
       "      <td>5.086894</td>\n",
       "      <td>2.194887</td>\n",
       "      <td>37.1676</td>\n",
       "      <td>0.837202</td>\n",
       "      <td>0.718023</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>{'clf__C': 37.1675929931015, 'clf__alpha': 0.8...</td>\n",
       "      <td>0.916338</td>\n",
       "      <td>0.915173</td>\n",
       "      <td>0.911742</td>\n",
       "      <td>0.917395</td>\n",
       "      <td>0.915012</td>\n",
       "      <td>0.906709</td>\n",
       "      <td>0.904988</td>\n",
       "      <td>0.917395</td>\n",
       "      <td>0.919841</td>\n",
       "      <td>0.919048</td>\n",
       "      <td>0.914364</td>\n",
       "      <td>0.004782</td>\n",
       "      <td>7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>224.179084</td>\n",
       "      <td>34.102164</td>\n",
       "      <td>5.724685</td>\n",
       "      <td>0.949904</td>\n",
       "      <td>49.541</td>\n",
       "      <td>1.29761</td>\n",
       "      <td>0.627411</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>{'clf__C': 49.54097085416407, 'clf__alpha': 1....</td>\n",
       "      <td>0.916996</td>\n",
       "      <td>0.916468</td>\n",
       "      <td>0.911811</td>\n",
       "      <td>0.915241</td>\n",
       "      <td>0.915903</td>\n",
       "      <td>0.907069</td>\n",
       "      <td>0.904196</td>\n",
       "      <td>0.915605</td>\n",
       "      <td>0.919586</td>\n",
       "      <td>0.919841</td>\n",
       "      <td>0.914272</td>\n",
       "      <td>0.004861</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>206.606502</td>\n",
       "      <td>44.275841</td>\n",
       "      <td>5.701531</td>\n",
       "      <td>0.628311</td>\n",
       "      <td>65.5653</td>\n",
       "      <td>1.46278</td>\n",
       "      <td>0.611175</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>{'clf__C': 65.56528254738022, 'clf__alpha': 1....</td>\n",
       "      <td>0.917852</td>\n",
       "      <td>0.916468</td>\n",
       "      <td>0.912598</td>\n",
       "      <td>0.915173</td>\n",
       "      <td>0.916633</td>\n",
       "      <td>0.905990</td>\n",
       "      <td>0.904120</td>\n",
       "      <td>0.914877</td>\n",
       "      <td>0.919778</td>\n",
       "      <td>0.919048</td>\n",
       "      <td>0.914254</td>\n",
       "      <td>0.005020</td>\n",
       "      <td>9</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>104.990756</td>\n",
       "      <td>13.371900</td>\n",
       "      <td>5.066596</td>\n",
       "      <td>0.270805</td>\n",
       "      <td>57.1893</td>\n",
       "      <td>0.517834</td>\n",
       "      <td>0.800053</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>{'clf__C': 57.1893234573495, 'clf__alpha': 0.5...</td>\n",
       "      <td>0.917489</td>\n",
       "      <td>0.915241</td>\n",
       "      <td>0.913249</td>\n",
       "      <td>0.916766</td>\n",
       "      <td>0.913286</td>\n",
       "      <td>0.905121</td>\n",
       "      <td>0.902487</td>\n",
       "      <td>0.916799</td>\n",
       "      <td>0.916436</td>\n",
       "      <td>0.919841</td>\n",
       "      <td>0.913671</td>\n",
       "      <td>0.005297</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>201.708653</td>\n",
       "      <td>37.987615</td>\n",
       "      <td>3.964477</td>\n",
       "      <td>0.236035</td>\n",
       "      <td>37.51</td>\n",
       "      <td>1.26019</td>\n",
       "      <td>0.542029</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'clf__C': 37.509985933375745, 'clf__alpha': 1...</td>\n",
       "      <td>0.915777</td>\n",
       "      <td>0.913855</td>\n",
       "      <td>0.914877</td>\n",
       "      <td>0.918124</td>\n",
       "      <td>0.913355</td>\n",
       "      <td>0.905901</td>\n",
       "      <td>0.900158</td>\n",
       "      <td>0.908946</td>\n",
       "      <td>0.923810</td>\n",
       "      <td>0.915403</td>\n",
       "      <td>0.913020</td>\n",
       "      <td>0.006267</td>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>156.659829</td>\n",
       "      <td>42.496307</td>\n",
       "      <td>5.371664</td>\n",
       "      <td>0.295641</td>\n",
       "      <td>35.2347</td>\n",
       "      <td>0.61201</td>\n",
       "      <td>0.928836</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>{'clf__C': 35.234657004628126, 'clf__alpha': 0...</td>\n",
       "      <td>0.916043</td>\n",
       "      <td>0.915903</td>\n",
       "      <td>0.912101</td>\n",
       "      <td>0.915012</td>\n",
       "      <td>0.909670</td>\n",
       "      <td>0.902816</td>\n",
       "      <td>0.902844</td>\n",
       "      <td>0.915510</td>\n",
       "      <td>0.917228</td>\n",
       "      <td>0.917031</td>\n",
       "      <td>0.912416</td>\n",
       "      <td>0.005268</td>\n",
       "      <td>12</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>191.723289</td>\n",
       "      <td>52.963525</td>\n",
       "      <td>4.064317</td>\n",
       "      <td>0.187262</td>\n",
       "      <td>68.7629</td>\n",
       "      <td>0.861227</td>\n",
       "      <td>0.706063</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'clf__C': 68.76292058793166, 'clf__alpha': 0....</td>\n",
       "      <td>0.916338</td>\n",
       "      <td>0.913561</td>\n",
       "      <td>0.907869</td>\n",
       "      <td>0.915375</td>\n",
       "      <td>0.910608</td>\n",
       "      <td>0.905255</td>\n",
       "      <td>0.899802</td>\n",
       "      <td>0.904230</td>\n",
       "      <td>0.923504</td>\n",
       "      <td>0.915375</td>\n",
       "      <td>0.911192</td>\n",
       "      <td>0.006660</td>\n",
       "      <td>13</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>205.502866</td>\n",
       "      <td>52.780908</td>\n",
       "      <td>4.075435</td>\n",
       "      <td>0.184276</td>\n",
       "      <td>52.0242</td>\n",
       "      <td>0.885184</td>\n",
       "      <td>0.198672</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'clf__C': 52.024180061957544, 'clf__alpha': 0...</td>\n",
       "      <td>0.916867</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.911543</td>\n",
       "      <td>0.912224</td>\n",
       "      <td>0.913235</td>\n",
       "      <td>0.908578</td>\n",
       "      <td>0.896879</td>\n",
       "      <td>0.906827</td>\n",
       "      <td>0.919329</td>\n",
       "      <td>0.910256</td>\n",
       "      <td>0.911174</td>\n",
       "      <td>0.005988</td>\n",
       "      <td>14</td>\n",
       "      <td>0.998132</td>\n",
       "      <td>0.997953</td>\n",
       "      <td>0.998221</td>\n",
       "      <td>0.998265</td>\n",
       "      <td>0.998131</td>\n",
       "      <td>0.998042</td>\n",
       "      <td>0.998221</td>\n",
       "      <td>0.998309</td>\n",
       "      <td>0.998087</td>\n",
       "      <td>0.997909</td>\n",
       "      <td>0.998127</td>\n",
       "      <td>0.000125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>198.758272</td>\n",
       "      <td>38.372587</td>\n",
       "      <td>3.934628</td>\n",
       "      <td>0.102642</td>\n",
       "      <td>44.8862</td>\n",
       "      <td>0.930903</td>\n",
       "      <td>0.713671</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'clf__C': 44.886169812356464, 'clf__alpha': 0...</td>\n",
       "      <td>0.916272</td>\n",
       "      <td>0.914218</td>\n",
       "      <td>0.908660</td>\n",
       "      <td>0.916237</td>\n",
       "      <td>0.909380</td>\n",
       "      <td>0.904819</td>\n",
       "      <td>0.899446</td>\n",
       "      <td>0.904153</td>\n",
       "      <td>0.923870</td>\n",
       "      <td>0.914013</td>\n",
       "      <td>0.911107</td>\n",
       "      <td>0.006833</td>\n",
       "      <td>15</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>243.590881</td>\n",
       "      <td>30.618701</td>\n",
       "      <td>4.251482</td>\n",
       "      <td>0.492905</td>\n",
       "      <td>31.3502</td>\n",
       "      <td>0.989108</td>\n",
       "      <td>0.846085</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'clf__C': 31.350188416928333, 'clf__alpha': 0...</td>\n",
       "      <td>0.910813</td>\n",
       "      <td>0.913199</td>\n",
       "      <td>0.907583</td>\n",
       "      <td>0.914716</td>\n",
       "      <td>0.907143</td>\n",
       "      <td>0.903021</td>\n",
       "      <td>0.898185</td>\n",
       "      <td>0.903355</td>\n",
       "      <td>0.920509</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.909029</td>\n",
       "      <td>0.006195</td>\n",
       "      <td>16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>185.836852</td>\n",
       "      <td>37.198955</td>\n",
       "      <td>3.931449</td>\n",
       "      <td>0.202012</td>\n",
       "      <td>66.9558</td>\n",
       "      <td>1.07016</td>\n",
       "      <td>0.167467</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'clf__C': 66.95583224828755, 'clf__alpha': 1....</td>\n",
       "      <td>0.913759</td>\n",
       "      <td>0.916367</td>\n",
       "      <td>0.908946</td>\n",
       "      <td>0.909823</td>\n",
       "      <td>0.913235</td>\n",
       "      <td>0.904474</td>\n",
       "      <td>0.894695</td>\n",
       "      <td>0.903382</td>\n",
       "      <td>0.917233</td>\n",
       "      <td>0.907779</td>\n",
       "      <td>0.908969</td>\n",
       "      <td>0.006492</td>\n",
       "      <td>17</td>\n",
       "      <td>0.996573</td>\n",
       "      <td>0.996395</td>\n",
       "      <td>0.996619</td>\n",
       "      <td>0.996440</td>\n",
       "      <td>0.996170</td>\n",
       "      <td>0.996306</td>\n",
       "      <td>0.996574</td>\n",
       "      <td>0.996527</td>\n",
       "      <td>0.996173</td>\n",
       "      <td>0.996261</td>\n",
       "      <td>0.996404</td>\n",
       "      <td>0.000161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>173.778865</td>\n",
       "      <td>34.042796</td>\n",
       "      <td>3.982314</td>\n",
       "      <td>0.143543</td>\n",
       "      <td>57.7414</td>\n",
       "      <td>0.877328</td>\n",
       "      <td>0.915871</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'clf__C': 57.74135825337959, 'clf__alpha': 0....</td>\n",
       "      <td>0.910454</td>\n",
       "      <td>0.914422</td>\n",
       "      <td>0.906435</td>\n",
       "      <td>0.914354</td>\n",
       "      <td>0.905990</td>\n",
       "      <td>0.900715</td>\n",
       "      <td>0.894945</td>\n",
       "      <td>0.902917</td>\n",
       "      <td>0.920812</td>\n",
       "      <td>0.911332</td>\n",
       "      <td>0.908237</td>\n",
       "      <td>0.007198</td>\n",
       "      <td>18</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>219.516779</td>\n",
       "      <td>49.811927</td>\n",
       "      <td>4.205957</td>\n",
       "      <td>0.122927</td>\n",
       "      <td>39.1273</td>\n",
       "      <td>0.740276</td>\n",
       "      <td>0.953458</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'clf__C': 39.127255196132154, 'clf__alpha': 0...</td>\n",
       "      <td>0.910095</td>\n",
       "      <td>0.913923</td>\n",
       "      <td>0.904499</td>\n",
       "      <td>0.913561</td>\n",
       "      <td>0.905121</td>\n",
       "      <td>0.901352</td>\n",
       "      <td>0.895028</td>\n",
       "      <td>0.903509</td>\n",
       "      <td>0.919778</td>\n",
       "      <td>0.913061</td>\n",
       "      <td>0.907993</td>\n",
       "      <td>0.006985</td>\n",
       "      <td>19</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>153.988118</td>\n",
       "      <td>29.812537</td>\n",
       "      <td>5.388496</td>\n",
       "      <td>0.341807</td>\n",
       "      <td>45.3809</td>\n",
       "      <td>0.793904</td>\n",
       "      <td>0.0175767</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>{'clf__C': 45.380924964462686, 'clf__alpha': 0...</td>\n",
       "      <td>0.894631</td>\n",
       "      <td>0.900160</td>\n",
       "      <td>0.892843</td>\n",
       "      <td>0.891653</td>\n",
       "      <td>0.898353</td>\n",
       "      <td>0.886025</td>\n",
       "      <td>0.880159</td>\n",
       "      <td>0.900202</td>\n",
       "      <td>0.904153</td>\n",
       "      <td>0.894779</td>\n",
       "      <td>0.894296</td>\n",
       "      <td>0.006787</td>\n",
       "      <td>20</td>\n",
       "      <td>0.990824</td>\n",
       "      <td>0.990373</td>\n",
       "      <td>0.990512</td>\n",
       "      <td>0.990330</td>\n",
       "      <td>0.990280</td>\n",
       "      <td>0.990508</td>\n",
       "      <td>0.990053</td>\n",
       "      <td>0.990596</td>\n",
       "      <td>0.990416</td>\n",
       "      <td>0.989571</td>\n",
       "      <td>0.990346</td>\n",
       "      <td>0.000323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_clf__C  \\\n",
       "7      108.174109     12.945563         5.019913        0.303700      63.3104   \n",
       "1      238.186817     55.525326         5.649907        0.250644       58.365   \n",
       "12     180.406100     52.394044         5.466442        0.374667       41.295   \n",
       "4      109.667099     12.395941         5.154001        0.241083      58.8777   \n",
       "14     168.066260     40.028676         3.945833        0.183873      63.9887   \n",
       "13     230.771548     49.179394         4.138723        0.116819      33.5631   \n",
       "19     209.410699     64.977677         5.086894        2.194887      37.1676   \n",
       "15     224.179084     34.102164         5.724685        0.949904       49.541   \n",
       "6      206.606502     44.275841         5.701531        0.628311      65.5653   \n",
       "16     104.990756     13.371900         5.066596        0.270805      57.1893   \n",
       "18     201.708653     37.987615         3.964477        0.236035        37.51   \n",
       "10     156.659829     42.496307         5.371664        0.295641      35.2347   \n",
       "17     191.723289     52.963525         4.064317        0.187262      68.7629   \n",
       "5      205.502866     52.780908         4.075435        0.184276      52.0242   \n",
       "11     198.758272     38.372587         3.934628        0.102642      44.8862   \n",
       "0      243.590881     30.618701         4.251482        0.492905      31.3502   \n",
       "3      185.836852     37.198955         3.931449        0.202012      66.9558   \n",
       "9      173.778865     34.042796         3.982314        0.143543      57.7414   \n",
       "2      219.516779     49.811927         4.205957        0.122927      39.1273   \n",
       "8      153.988118     29.812537         5.388496        0.341807      45.3809   \n",
       "\n",
       "   param_clf__alpha param_clf__beta param_vect__max_df param_vect__min_df  \\\n",
       "7          0.649284        0.273664                  1                  2   \n",
       "1           1.38641        0.282825                  1                  2   \n",
       "12         0.791467         0.51763                  1                  2   \n",
       "4          0.910497         0.20378                  1                  2   \n",
       "14         0.863862        0.335656                  1                  2   \n",
       "13         0.636795        0.297257                  1                  2   \n",
       "19         0.837202        0.718023                  1                  2   \n",
       "15          1.29761        0.627411                  1                  2   \n",
       "6           1.46278        0.611175                  1                  2   \n",
       "16         0.517834        0.800053                  1                  2   \n",
       "18          1.26019        0.542029                  1                  2   \n",
       "10          0.61201        0.928836                  1                  2   \n",
       "17         0.861227        0.706063                  1                  2   \n",
       "5          0.885184        0.198672                  1                  2   \n",
       "11         0.930903        0.713671                  1                  2   \n",
       "0          0.989108        0.846085                  1                  2   \n",
       "3           1.07016        0.167467                  1                  2   \n",
       "9          0.877328        0.915871                  1                  2   \n",
       "2          0.740276        0.953458                  1                  2   \n",
       "8          0.793904       0.0175767                  1                  2   \n",
       "\n",
       "   param_vect__ngram_range                                             params  \\\n",
       "7                   (1, 3)  {'clf__C': 63.31041288467941, 'clf__alpha': 0....   \n",
       "1                   (1, 3)  {'clf__C': 58.36497474652987, 'clf__alpha': 1....   \n",
       "12                  (1, 3)  {'clf__C': 41.29497015311559, 'clf__alpha': 0....   \n",
       "4                   (1, 3)  {'clf__C': 58.877698434633004, 'clf__alpha': 0...   \n",
       "14                  (1, 2)  {'clf__C': 63.988721188429295, 'clf__alpha': 0...   \n",
       "13                  (1, 2)  {'clf__C': 33.563094208219184, 'clf__alpha': 0...   \n",
       "19                  (1, 3)  {'clf__C': 37.1675929931015, 'clf__alpha': 0.8...   \n",
       "15                  (1, 3)  {'clf__C': 49.54097085416407, 'clf__alpha': 1....   \n",
       "6                   (1, 3)  {'clf__C': 65.56528254738022, 'clf__alpha': 1....   \n",
       "16                  (1, 3)  {'clf__C': 57.1893234573495, 'clf__alpha': 0.5...   \n",
       "18                  (1, 2)  {'clf__C': 37.509985933375745, 'clf__alpha': 1...   \n",
       "10                  (1, 3)  {'clf__C': 35.234657004628126, 'clf__alpha': 0...   \n",
       "17                  (1, 2)  {'clf__C': 68.76292058793166, 'clf__alpha': 0....   \n",
       "5                   (1, 2)  {'clf__C': 52.024180061957544, 'clf__alpha': 0...   \n",
       "11                  (1, 2)  {'clf__C': 44.886169812356464, 'clf__alpha': 0...   \n",
       "0                   (1, 2)  {'clf__C': 31.350188416928333, 'clf__alpha': 0...   \n",
       "3                   (1, 2)  {'clf__C': 66.95583224828755, 'clf__alpha': 1....   \n",
       "9                   (1, 2)  {'clf__C': 57.74135825337959, 'clf__alpha': 0....   \n",
       "2                   (1, 2)  {'clf__C': 39.127255196132154, 'clf__alpha': 0...   \n",
       "8                   (1, 3)  {'clf__C': 45.380924964462686, 'clf__alpha': 0...   \n",
       "\n",
       "    split0_test_score  split1_test_score  split2_test_score  \\\n",
       "7            0.920143           0.918424           0.913541   \n",
       "1            0.920382           0.918660           0.916996   \n",
       "12           0.918940           0.916899           0.912667   \n",
       "4            0.919091           0.917131           0.911024   \n",
       "14           0.919284           0.918229           0.913718   \n",
       "13           0.917994           0.917431           0.913199   \n",
       "19           0.916338           0.915173           0.911742   \n",
       "15           0.916996           0.916468           0.911811   \n",
       "6            0.917852           0.916468           0.912598   \n",
       "16           0.917489           0.915241           0.913249   \n",
       "18           0.915777           0.913855           0.914877   \n",
       "10           0.916043           0.915903           0.912101   \n",
       "17           0.916338           0.913561           0.907869   \n",
       "5            0.916867           0.916000           0.911543   \n",
       "11           0.916272           0.914218           0.908660   \n",
       "0            0.910813           0.913199           0.907583   \n",
       "3            0.913759           0.916367           0.908946   \n",
       "9            0.910454           0.914422           0.906435   \n",
       "2            0.910095           0.913923           0.904499   \n",
       "8            0.894631           0.900160           0.892843   \n",
       "\n",
       "    split3_test_score  split4_test_score  split5_test_score  \\\n",
       "7            0.915970           0.919284           0.915470   \n",
       "1            0.913511           0.920080           0.911706   \n",
       "12           0.918359           0.915079           0.909163   \n",
       "4            0.912714           0.920572           0.914698   \n",
       "14           0.919760           0.917263           0.914902   \n",
       "13           0.917497           0.915903           0.916733   \n",
       "19           0.917395           0.915012           0.906709   \n",
       "15           0.915241           0.915903           0.907069   \n",
       "6            0.915173           0.916633           0.905990   \n",
       "16           0.916766           0.913286           0.905121   \n",
       "18           0.918124           0.913355           0.905901   \n",
       "10           0.915012           0.909670           0.902816   \n",
       "17           0.915375           0.910608           0.905255   \n",
       "5            0.912224           0.913235           0.908578   \n",
       "11           0.916237           0.909380           0.904819   \n",
       "0            0.914716           0.907143           0.903021   \n",
       "3            0.909823           0.913235           0.904474   \n",
       "9            0.914354           0.905990           0.900715   \n",
       "2            0.913561           0.905121           0.901352   \n",
       "8            0.891653           0.898353           0.886025   \n",
       "\n",
       "    split6_test_score  split7_test_score  split8_test_score  \\\n",
       "7            0.898024           0.921304           0.921053   \n",
       "1            0.896361           0.919952           0.922524   \n",
       "12           0.905138           0.918489           0.919952   \n",
       "4            0.898734           0.919824           0.921116   \n",
       "14           0.899046           0.910621           0.923016   \n",
       "13           0.898769           0.912435           0.920495   \n",
       "19           0.904988           0.917395           0.919841   \n",
       "15           0.904196           0.915605           0.919586   \n",
       "6            0.904120           0.914877           0.919778   \n",
       "16           0.902487           0.916799           0.916436   \n",
       "18           0.900158           0.908946           0.923810   \n",
       "10           0.902844           0.915510           0.917228   \n",
       "17           0.899802           0.904230           0.923504   \n",
       "5            0.896879           0.906827           0.919329   \n",
       "11           0.899446           0.904153           0.923870   \n",
       "0            0.898185           0.903355           0.920509   \n",
       "3            0.894695           0.903382           0.917233   \n",
       "9            0.894945           0.902917           0.920812   \n",
       "2            0.895028           0.903509           0.919778   \n",
       "8            0.880159           0.900202           0.904153   \n",
       "\n",
       "    split9_test_score  mean_test_score  std_test_score  rank_test_score  \\\n",
       "7            0.920812         0.916402        0.006622                1   \n",
       "1            0.922280         0.916245        0.007425                2   \n",
       "12           0.921856         0.915654        0.004966                3   \n",
       "4            0.920749         0.915565        0.006546                4   \n",
       "14           0.918465         0.915431        0.006374                5   \n",
       "13           0.917666         0.914812        0.005789                6   \n",
       "19           0.919048         0.914364        0.004782                7   \n",
       "15           0.919841         0.914272        0.004861                8   \n",
       "6            0.919048         0.914254        0.005020                9   \n",
       "16           0.919841         0.913671        0.005297               10   \n",
       "18           0.915403         0.913020        0.006267               11   \n",
       "10           0.917031         0.912416        0.005268               12   \n",
       "17           0.915375         0.911192        0.006660               13   \n",
       "5            0.910256         0.911174        0.005988               14   \n",
       "11           0.914013         0.911107        0.006833               15   \n",
       "0            0.911765         0.909029        0.006195               16   \n",
       "3            0.907779         0.908969        0.006492               17   \n",
       "9            0.911332         0.908237        0.007198               18   \n",
       "2            0.913061         0.907993        0.006985               19   \n",
       "8            0.894779         0.894296        0.006787               20   \n",
       "\n",
       "    split0_train_score  split1_train_score  split2_train_score  \\\n",
       "7             1.000000            1.000000            1.000000   \n",
       "1             0.999822            0.999822            0.999956   \n",
       "12            1.000000            1.000000            1.000000   \n",
       "4             0.999778            0.999778            0.999867   \n",
       "14            0.999778            0.999733            0.999822   \n",
       "13            0.999644            0.999689            0.999822   \n",
       "19            1.000000            1.000000            1.000000   \n",
       "15            1.000000            1.000000            1.000000   \n",
       "6             1.000000            1.000000            1.000000   \n",
       "16            1.000000            1.000000            1.000000   \n",
       "18            1.000000            1.000000            1.000000   \n",
       "10            1.000000            1.000000            1.000000   \n",
       "17            1.000000            1.000000            1.000000   \n",
       "5             0.998132            0.997953            0.998221   \n",
       "11            1.000000            1.000000            1.000000   \n",
       "0             1.000000            1.000000            1.000000   \n",
       "3             0.996573            0.996395            0.996619   \n",
       "9             1.000000            1.000000            1.000000   \n",
       "2             1.000000            1.000000            1.000000   \n",
       "8             0.990824            0.990373            0.990512   \n",
       "\n",
       "    split3_train_score  split4_train_score  split5_train_score  \\\n",
       "7             1.000000            1.000000            1.000000   \n",
       "1             0.999911            0.999911            0.999911   \n",
       "12            1.000000            1.000000            1.000000   \n",
       "4             0.999778            0.999822            0.999822   \n",
       "14            0.999689            0.999778            0.999733   \n",
       "13            0.999689            0.999733            0.999689   \n",
       "19            1.000000            1.000000            1.000000   \n",
       "15            1.000000            1.000000            1.000000   \n",
       "6             1.000000            1.000000            1.000000   \n",
       "16            1.000000            1.000000            1.000000   \n",
       "18            1.000000            1.000000            1.000000   \n",
       "10            1.000000            1.000000            1.000000   \n",
       "17            1.000000            1.000000            1.000000   \n",
       "5             0.998265            0.998131            0.998042   \n",
       "11            1.000000            1.000000            1.000000   \n",
       "0             1.000000            1.000000            1.000000   \n",
       "3             0.996440            0.996170            0.996306   \n",
       "9             1.000000            1.000000            1.000000   \n",
       "2             1.000000            1.000000            1.000000   \n",
       "8             0.990330            0.990280            0.990508   \n",
       "\n",
       "    split6_train_score  split7_train_score  split8_train_score  \\\n",
       "7             0.999956            0.999956            1.000000   \n",
       "1             0.999822            0.999956            0.999867   \n",
       "12            1.000000            1.000000            1.000000   \n",
       "4             0.999778            0.999822            0.999778   \n",
       "14            0.999733            0.999778            0.999778   \n",
       "13            0.999733            0.999644            0.999689   \n",
       "19            1.000000            1.000000            1.000000   \n",
       "15            1.000000            1.000000            1.000000   \n",
       "6             1.000000            1.000000            1.000000   \n",
       "16            1.000000            1.000000            1.000000   \n",
       "18            1.000000            1.000000            1.000000   \n",
       "10            1.000000            1.000000            1.000000   \n",
       "17            1.000000            1.000000            1.000000   \n",
       "5             0.998221            0.998309            0.998087   \n",
       "11            1.000000            1.000000            1.000000   \n",
       "0             1.000000            1.000000            1.000000   \n",
       "3             0.996574            0.996527            0.996173   \n",
       "9             1.000000            1.000000            1.000000   \n",
       "2             1.000000            1.000000            1.000000   \n",
       "8             0.990053            0.990596            0.990416   \n",
       "\n",
       "    split9_train_score  mean_train_score  std_train_score  \n",
       "7             1.000000          0.999991         0.000018  \n",
       "1             0.999911          0.999889         0.000050  \n",
       "12            1.000000          1.000000         0.000000  \n",
       "4             0.999778          0.999800         0.000030  \n",
       "14            0.999733          0.999756         0.000036  \n",
       "13            0.999689          0.999702         0.000049  \n",
       "19            1.000000          1.000000         0.000000  \n",
       "15            1.000000          1.000000         0.000000  \n",
       "6             1.000000          1.000000         0.000000  \n",
       "16            1.000000          1.000000         0.000000  \n",
       "18            1.000000          1.000000         0.000000  \n",
       "10            1.000000          1.000000         0.000000  \n",
       "17            1.000000          1.000000         0.000000  \n",
       "5             0.997909          0.998127         0.000125  \n",
       "11            1.000000          1.000000         0.000000  \n",
       "0             1.000000          1.000000         0.000000  \n",
       "3             0.996261          0.996404         0.000161  \n",
       "9             1.000000          1.000000         0.000000  \n",
       "2             1.000000          1.000000         0.000000  \n",
       "8             0.989571          0.990346         0.000323  "
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(gs_nbsvm.cv_results_).sort_values(by=['mean_test_score'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression \n",
    "Search max_df, percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129.82043886184692,\n",
       " {'clf__C': 35, 'vec__max_df': 0.9, 'vec__min_df': 2},\n",
       " 0.9084344418346936)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_best_lr = {\n",
    "    'vec__min_df': [2],\n",
    "    'vec__max_df': [.9],\n",
    "    'clf__C': [35], \n",
    "}\n",
    "\n",
    "pipeline_best_lr = Pipeline([\n",
    "    ('vec', LemmaCountVectorizer(strip_accents='unicode', stop_words=None, min_df=3, \n",
    "                                 stem=False, ngram_range=(1, 2), preprocessing=True, )),\n",
    "        ('tfidf', TfidfTransformer(use_idf=1,smooth_idf=1,sublinear_tf=1)),\n",
    "    ('clf', LogisticRegression(solver='saga', penalty='l2', max_iter=40000, tol=0.0001, fit_intercept=True))\n",
    "])\n",
    "                  \n",
    "rs_lr_max_per = GridSearchCV(pipeline_best_lr, parameters_best_lr, \n",
    "                                   cv=5, scoring=score, n_jobs=-1, verbose=0,\n",
    "                                      return_train_score=True)\n",
    "\n",
    "start = time.time()\n",
    "rs_lr_max_per.fit(X_train, y)\n",
    "time.time() - start, rs_lr_max_per.best_params_, rs_lr_max_per.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression \n",
    "Search max_df, fixed value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2582.644148349762,\n",
       " {'clf__C': 35, 'vec__max_df': 4400, 'vec__min_df': 2},\n",
       " 0.9136939635528595)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_best_lr = {\n",
    "    'vec__min_df': [2],\n",
    "    'vec__max_df': [4400],\n",
    "    'clf__C': [35], \n",
    "}\n",
    "\n",
    "pipeline_best_lr = Pipeline([\n",
    "    ('vec', LemmaCountVectorizer(strip_accents='unicode', stop_words=None, \n",
    "                                 stem=False, ngram_range=(1, 2), preprocessing=True, )),\n",
    "        ('tfidf', TfidfTransformer(use_idf=1,smooth_idf=1,sublinear_tf=1)),\n",
    "    ('clf', LogisticRegression(solver='saga', penalty='l2', max_iter=40000, tol=0.0001, fit_intercept=True))\n",
    "])\n",
    "                  \n",
    "rs_best_lr = GridSearchCV(pipeline_best_lr, parameters_best_lr, \n",
    "                                   cv=80, scoring=['f1', 'precision', 'recall'], n_jobs=-1, verbose=0,\n",
    "                                      return_train_score=True, refit='f1')\n",
    "\n",
    "start = time.time()\n",
    "rs_best_lr.fit(X_train, y)\n",
    "time.time() - start, rs_best_lr.best_params_, rs_best_lr.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1395.5862810611725,\n",
       " {'clf__C': 68.35390244026648, 'vec__max_df': 4400, 'vec__min_df': 2},\n",
       " 0.9095024019634056)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_best_lr = {\n",
    "    'vec__min_df': [2],\n",
    "    'vec__max_df': [4400],\n",
    "    'clf__C': uniform(30,40), \n",
    "}\n",
    "\n",
    "pipeline_best_lr = Pipeline([\n",
    "    ('vec', LemmaCountVectorizer(strip_accents='unicode', stop_words=None, \n",
    "                                 stem=False, ngram_range=(1, 2), preprocessing=True, )),\n",
    "        ('tfidf', TfidfTransformer(use_idf=1,smooth_idf=1,sublinear_tf=1)),\n",
    "    ('clf', LogisticRegression(solver='saga', penalty='l2', max_iter=40000, tol=0.0001, fit_intercept=True))\n",
    "])\n",
    "                  \n",
    "rs_best_lr3 = RandomizedSearchCV(pipeline_best_lr, parameters_best_lr, \n",
    "                                   cv=5, scoring=['f1', 'precision', 'recall'], n_jobs=-1, verbose=0,\n",
    "                                      return_train_score=True, refit='f1')\n",
    "\n",
    "start = time.time()\n",
    "rs_best_lr3.fit(X_train, y)\n",
    "time.time() - start, rs_best_lr3.best_params_, rs_best_lr3.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "Try normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(242.97897291183472,\n",
       " {'clf__C': 1, 'vec__max_df': 44000, 'vec__min_df': 2},\n",
       " 0.8529732208900852)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "parameters_best_lr = {\n",
    "    'vec__min_df': [2],\n",
    "    'vec__max_df': [44000],\n",
    "    'clf__C': [1], \n",
    "}\n",
    "\n",
    "pipeline_best_lr = Pipeline([\n",
    "    ('vec', LemmaCountVectorizer(strip_accents='unicode', stop_words=None, \n",
    "                                 stem=False, ngram_range=(2, 2), preprocessing=True, )),\n",
    "        ('tfidf', TfidfTransformer(use_idf=1,smooth_idf=1,sublinear_tf=1)),\n",
    "    ('scale', RobustScaler(with_centering=False)),\n",
    "    ('clf', LogisticRegression(solver='saga', penalty='l2', max_iter=40000, tol=0.0001, fit_intercept=True))\n",
    "])\n",
    "                  \n",
    "rs_lr_scaler = GridSearchCV(pipeline_best_lr, parameters_best_lr, \n",
    "                                   cv=2, scoring=['f1', 'precision', 'recall'], n_jobs=-1, verbose=0,\n",
    "                                      return_train_score=True, refit='f1')\n",
    "\n",
    "start = time.time()\n",
    "rs_lr_scaler.fit(X_train, y)\n",
    "time.time() - start, rs_lr_scaler.best_params_, rs_lr_scaler.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "Only counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1160.9041471481323,\n",
       " {'clf__C': 35, 'vec__max_df': 0.9, 'vec__min_df': 2},\n",
       " 0.8967622744752337)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_best_lr2 = {\n",
    "    'vec__min_df': [2],\n",
    "    'vec__max_df': [4400],\n",
    "    'clf__C': [35], \n",
    "}\n",
    "\n",
    "pipeline_best_lr = Pipeline([\n",
    "    ('vec', LemmaCountVectorizer(strip_accents='unicode', stop_words=None, \n",
    "                                 stem=False, ngram_range=(1, 2), preprocessing=True, )),\n",
    "    ('clf', LogisticRegression(solver='saga', penalty='l2', max_iter=40000, tol=0.0001, fit_intercept=True))\n",
    "])\n",
    "                  \n",
    "rs_lr_notf = GridSearchCV(pipeline_best_lr, parameters_best_lr, \n",
    "                                   cv=5, scoring=['f1', 'precision', 'recall'], n_jobs=-1, verbose=0,\n",
    "                                      return_train_score=True, refit='f1')\n",
    "\n",
    "start = time.time()\n",
    "rs_lr_notf.fit(X_train, y)\n",
    "time.time() - start, rs_lr_notf.best_params_, rs_lr_notf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNB \n",
    "Try with tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109.62222814559937,\n",
       " {'clf__alpha': 0.01, 'vec__max_df': 4400, 'vec__min_df': 2},\n",
       " 0.8719168762788287)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_best_lr = {\n",
    "    'vec__min_df': [2],\n",
    "    'vec__max_df': [4400],\n",
    "    'clf__alpha': [0.01], \n",
    "}\n",
    "\n",
    "pipeline_best_lr = Pipeline([\n",
    "    ('vec', LemmaCountVectorizer(strip_accents='unicode', stop_words='english', \n",
    "                                 stem=False, ngram_range=(1, 2), preprocessing=True, )),\n",
    "        ('tfidf', TfidfTransformer(use_idf=1,smooth_idf=1,sublinear_tf=1)),\n",
    "     ('clf', MultinomialNB())\n",
    "])\n",
    "                  \n",
    "rs_mnb_fix = GridSearchCV(pipeline_best_lr, parameters_best_lr, \n",
    "                                   cv=5, scoring=['f1', 'precision', 'recall'], n_jobs=-1, verbose=0,\n",
    "                                      return_train_score=True, refit='f1')\n",
    "\n",
    "start = time.time()\n",
    "rs_mnb_fix.fit(X_train, y)\n",
    "time.time() - start, rs_mnb_fix.best_params_, rs_mnb_fix.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MI and best model\n",
    "#### build vocabulary\n",
    "\n",
    "no stop words, 50%, no steamming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = LemmaCountVectorizer(strip_accents='unicode', stop_words=None,stem=False, ngram_range=(1, 2), \n",
    "                                  preprocessing=True, max_df=1.0, min_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing...\n",
      "Getting MI...\n",
      "Buiding vocabulary...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "percentage = 50\n",
    "\n",
    "vect = counter\n",
    "# LemmaCountVectorizer(strip_accents='unicode', stop_words=None, binary=False, max_df=1.0, min_df=1)\n",
    "print(\"Vectorizing...\")\n",
    "counts = vect.fit_transform(X_train)\n",
    "print(\"Getting best tMI indexes...\")\n",
    "mutual_info_idx = np.argsort(-mutual_info_classif(counts, y))\n",
    "\n",
    "print(\"Buiding vocabulary...\")\n",
    "top = int((mutual_info_idx.shape[0] * percentage) / 100)\n",
    "# top = 10 \n",
    "vocab_all = vect.get_feature_names()\n",
    "top_idx = mutual_info_idx[:top]\n",
    "vocabulary = np.take(vocab_all, top_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LemmaCountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 2), preprocessing=True, preprocessor=None,\n",
       "           stem=False, stop_words=None, strip_accents='unicode',\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None,\n",
       "           vocabulary=array(['bad', 'worst', ..., 'splash certainly', 'splash but'],\n",
       "      dtype='<U88'))"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.set_params(vocabulary=vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no stop words, 40%, steamming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing...\n",
      "Getting best tMI indexes...\n",
      "Buiding vocabulary...\n"
     ]
    }
   ],
   "source": [
    "counter1 = LemmaCountVectorizer(strip_accents='unicode', stop_words=None,stem=True, ngram_range=(1, 2), \n",
    "                                  preprocessing=True, max_df=1.0, min_df=1)\n",
    "\n",
    "percentage1 = 40\n",
    "\n",
    "vect1 = counter1\n",
    "# LemmaCountVectorizer(strip_accents='unicode', stop_words=None, binary=False, max_df=1.0, min_df=1)\n",
    "print(\"Vectorizing...\")\n",
    "counts1 = vect1.fit_transform(X_train)\n",
    "print(\"Getting best tMI indexes...\")\n",
    "mutual_info_idx1 = np.argsort(-mutual_info_classif(counts1, y))\n",
    "\n",
    "print(\"Buiding vocabulary...\")\n",
    "top1 = int((mutual_info_idx1.shape[0] * percentage1) / 100)\n",
    "# top = 10 \n",
    "vocab_all1 = vect1.get_feature_names()\n",
    "top_idx1 = mutual_info_idx1[:top1]\n",
    "vocabulary1 = np.take(vocab_all1, top_idx1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LemmaCountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 2), preprocessing=True, preprocessor=None,\n",
       "           stem=True, stop_words=None, strip_accents='unicode',\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None,\n",
       "           vocabulary=array(['bad', 'worst', ..., 'wall will', 'wall william'], dtype='<U88'))"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter1.set_params(vocabulary=vocabulary1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no stop words, 80%, steamming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing...\n",
      "Getting best tMI indexes...\n",
      "Buiding vocabulary...\n"
     ]
    }
   ],
   "source": [
    "counter80 = LemmaCountVectorizer(strip_accents='unicode', stop_words=None,stem=False, ngram_range=(1, 2), \n",
    "                                  preprocessing=True, max_df=1.0, min_df=1)\n",
    "\n",
    "percentage80 = 80\n",
    "\n",
    "vect80 = counter80\n",
    "# LemmaCountVectorizer(strip_accents='unicode', stop_words=None, binary=False, max_df=1.0, min_df=1)\n",
    "print(\"Vectorizing...\")\n",
    "counts80 = vect80.fit_transform(X_train)\n",
    "print(\"Getting best tMI indexes...\")\n",
    "mutual_info_idx80 = np.argsort(-mutual_info_classif(counts80, y))\n",
    "\n",
    "print(\"Buiding vocabulary...\")\n",
    "top80 = int((mutual_info_idx80.shape[0] * percentage80) / 100)\n",
    "# top = 10 \n",
    "vocab_all80 = vect80.get_feature_names()\n",
    "top_idx80 = mutual_info_idx80[:top80]\n",
    "vocabulary80 = np.take(vocab_all80, top_idx80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LemmaCountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 2), preprocessing=True, preprocessor=None,\n",
       "           stem=False, stop_words=None, strip_accents='unicode',\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None,\n",
       "           vocabulary=array(['bad', 'worst', ..., 'grandfatherly skill', 'andrei tarkovky'],\n",
       "      dtype='<U88'))"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter80.set_params(vocabulary=vocabulary80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### best model with counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cuent/anaconda/envs/comp551-proj2/lib/python3.6/site-packages/sklearn/model_selection/_search.py:271: UserWarning: The total space of parameters 1 is smaller than n_iter=20. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  2.3min remaining:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  2.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(246.3034553527832, {}, 0.9228923536177926)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_nbsvm = {\n",
    "}\n",
    "pipeline_nbsvm = Pipeline([\n",
    "    ('vect', counter),\n",
    "    ('clf', NBSVM(C=41.29497015311559, alpha=0.7914671389502778, beta=0.5176297089108057))\n",
    "])\n",
    "\n",
    "gs_nbsvm_best_count = RandomizedSearchCV(pipeline_nbsvm, parameters_nbsvm, \n",
    "                                   cv=5, scoring=score, n_jobs=-1, verbose=3,\n",
    "                                 return_train_score=True, n_iter=20, random_state=62)\n",
    "start = time.time()\n",
    "gs_nbsvm_best_count.fit(X_train, y)\n",
    "time.time()-start, gs_nbsvm_best_count.best_params_, gs_nbsvm_best_count.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cuent/anaconda/envs/comp551-proj2/lib/python3.6/site-packages/sklearn/model_selection/_search.py:271: UserWarning: The total space of parameters 1 is smaller than n_iter=20. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  7.3min remaining: 10.9min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  7.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(662.469446182251, {}, 0.920845066179548)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_nbsvm = {\n",
    "}\n",
    "pipeline_nbsvm1 = Pipeline([\n",
    "    ('vect', counter1),\n",
    "    ('clf', NBSVM(C=41.29497015311559, alpha=0.7914671389502778, beta=0.5176297089108057))\n",
    "])\n",
    "\n",
    "gs_nbsvm_count_40 = RandomizedSearchCV(pipeline_nbsvm1, parameters_nbsvm, \n",
    "                                   cv=5, scoring=score, n_jobs=-1, verbose=3,\n",
    "                                 return_train_score=True, n_iter=20, random_state=62)\n",
    "start = time.time()\n",
    "gs_nbsvm_count_40.fit(X_train, y)\n",
    "time.time()-start, gs_nbsvm_count_40.best_params_, gs_nbsvm_count_40.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### best model with tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cuent/anaconda/envs/comp551-proj2/lib/python3.6/site-packages/sklearn/model_selection/_search.py:271: UserWarning: The total space of parameters 1 is smaller than n_iter=20. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   49.0s remaining:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   51.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(73.76267528533936, {}, 0.9171623398613418)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_nbsvm = {\n",
    "}\n",
    "pipeline_nbsvm = Pipeline([\n",
    "    ('vect', counter),\n",
    "    ('tfidf', TfidfTransformer(use_idf=1,smooth_idf=1,sublinear_tf=1)),\n",
    "    ('clf', NBSVM(C=41.29497015311559, alpha=0.7914671389502778, beta=0.5176297089108057))\n",
    "])\n",
    "\n",
    "gs_nbsvm_tf_vocab = RandomizedSearchCV(pipeline_nbsvm, parameters_nbsvm, \n",
    "                                   cv=5, scoring=score, n_jobs=-1, verbose=3,\n",
    "                                 return_train_score=True, n_iter=20, random_state=62)\n",
    "start = time.time()\n",
    "gs_nbsvm_tf_vocab.fit(X_train, y)\n",
    "time.time()-start, gs_nbsvm_tf_vocab.best_params_, gs_nbsvm_tf_vocab.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  best logistic regression with vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LemmaCountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 2), preprocessing=True, preprocessor=None,\n",
       "           stem=False, stop_words=None, strip_accents='unicode',\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None,\n",
       "           vocabulary=array(['bad', 'worst', ..., 'splash certainly', 'splash but'],\n",
       "      dtype='<U88'))"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102.08423566818237, {}, 0.9036950733860422)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_best_lr = Pipeline([\n",
    "    ('vec', counter),\n",
    "    ('tfidf', TfidfTransformer(use_idf=1, smooth_idf=1, sublinear_tf=1)),\n",
    "    ('clf', LogisticRegression(solver='saga', penalty='l2', max_iter=40000, tol=0.0001, fit_intercept=True)\n",
    ")])\n",
    "                  \n",
    "gs_lr1_vocab = GridSearchCV(pipeline_best_lr, {}, \n",
    "                                   cv=5, scoring=score, n_jobs=-1, verbose=0,\n",
    "                                      return_train_score=True)\n",
    "\n",
    "start = time.time()\n",
    "gs_lr1_vocab.fit(X_train, y)\n",
    "time.time() - start, gs_lr1_vocab.best_params_, gs_lr1_vocab.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing...\n",
      "Getting best tMI indexes...\n",
      "Buiding vocabulary...\n"
     ]
    }
   ],
   "source": [
    "counter1 = LemmaCountVectorizer(strip_accents='unicode', stop_words=None,stem=True, ngram_range=(1, 2), \n",
    "                                  preprocessing=True, max_df=1.0, min_df=1)\n",
    "\n",
    "percentage1 = 40\n",
    "\n",
    "vect1 = counter1\n",
    "# LemmaCountVectorizer(strip_accents='unicode', stop_words=None, binary=False, max_df=1.0, min_df=1)\n",
    "print(\"Vectorizing...\")\n",
    "counts1 = vect1.fit_transform(X_train)\n",
    "print(\"Getting best tMI indexes...\")\n",
    "mutual_info_idx1 = np.argsort(-mutual_info_classif(counts1, y))\n",
    "\n",
    "print(\"Buiding vocabulary...\")\n",
    "top1 = int((mutual_info_idx1.shape[0] * percentage1) / 100)\n",
    "# top = 10 \n",
    "vocab_all1 = vect1.get_feature_names()\n",
    "top_idx1 = mutual_info_idx1[:top1]\n",
    "vocabulary1 = np.take(vocab_all1, top_idx1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### best nbsvm top 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cuent/anaconda/envs/comp551-proj2/lib/python3.6/site-packages/sklearn/model_selection/_search.py:271: UserWarning: The total space of parameters 1 is smaller than n_iter=20. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  2.6min remaining:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  3.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(328.94378900527954, {}, 0.9208116514084702)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_nbsvm = Pipeline([\n",
    "    ('vect', counter80),\n",
    "    ('clf', NBSVM(C=41.29497015311559, alpha=0.7914671389502778, beta=0.5176297089108057))\n",
    "])\n",
    "\n",
    "gs_nbsvm_count_80 = RandomizedSearchCV(pipeline_nbsvm, {}, \n",
    "                                   cv=5, scoring=score, n_jobs=-1, verbose=3,\n",
    "                                 return_train_score=True, n_iter=20, random_state=62)\n",
    "start = time.time()\n",
    "gs_nbsvm_count_tfidf_80.fit(X_train, y)\n",
    "time.time()-start, gs_nbsvm_count_80.best_params_, gs_nbsvm_count_80.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### best nbsvm top 80 tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cuent/anaconda/envs/comp551-proj2/lib/python3.6/site-packages/sklearn/model_selection/_search.py:271: UserWarning: The total space of parameters 1 is smaller than n_iter=20. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  1.2min remaining:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100.67411518096924, {}, 0.9161478412195226)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_nbsvm = Pipeline([\n",
    "    ('vect', counter80),\n",
    "    ('tfidf', TfidfTransformer(use_idf=1, smooth_idf=1, sublinear_tf=1)),\n",
    "    ('clf', NBSVM(C=41.29497015311559, alpha=0.7914671389502778, beta=0.5176297089108057))\n",
    "])\n",
    "\n",
    "gs_nbsvm_count_tfidf_80 = RandomizedSearchCV(pipeline_nbsvm, {}, \n",
    "                                   cv=5, scoring=score, n_jobs=-1, verbose=3,\n",
    "                                 return_train_score=True, n_iter=20, random_state=62)\n",
    "start = time.time()\n",
    "gs_nbsvm_count_tfidf_80.fit(X_train, y)\n",
    "time.time()-start, gs_nbsvm_count_tfidf_80.best_params_, gs_nbsvm_count_tfidf_80.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analyze incorrect predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do not expect a depiction of the \"truth\". However, the accounts of these veterans of the Iraqi & Afghanistan wars demand thoughtful consideration. <br /><br />The major strength of the film is that it vividly portrays the words and war wounds of these vets and their post-war struggles to reconstruct some degree of normalcy and functionality to their lives. <br /><br />My major criticism of the film is twofold: it is one-sided and it advocates anti-war activism but nothing more to correct the serious shortcomings of the military's and Veterans Affairs' programs for helping those who've suffered and still suffer the traumas of war. These are NOT fatal flaws of the film.<br /><br />As a veteran myself, I know that the horrible aftermath of war is real, and these young men and women articulate it very well. These vets vividly describe the physical and mental pain and torment that most veterans experience and that ordinary people need to understand because the horrors of ALL wars are so traumatic and disturbing.\n",
      "(1, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['account',\n",
       " 'activ',\n",
       " 'advoc',\n",
       " 'affair',\n",
       " 'afghanistan',\n",
       " 'aftermath',\n",
       " 'anti',\n",
       " 'articul',\n",
       " 'becaus',\n",
       " 'consider',\n",
       " 'correct',\n",
       " 'critic',\n",
       " 'degre',\n",
       " 'demand',\n",
       " 'depict',\n",
       " 'describ',\n",
       " 'disturb',\n",
       " 'expect',\n",
       " 'experi',\n",
       " 'fatal',\n",
       " 'film',\n",
       " 'flaw',\n",
       " 'function',\n",
       " 'help',\n",
       " 'horribl',\n",
       " 'horror',\n",
       " 'howev',\n",
       " 'iraqi',\n",
       " 'know',\n",
       " 'live',\n",
       " 'major',\n",
       " 'men',\n",
       " 'mental',\n",
       " 'militari',\n",
       " 'need',\n",
       " 'normalci',\n",
       " 'noth',\n",
       " 'ordinari',\n",
       " 'pain',\n",
       " 'peopl',\n",
       " 'physic',\n",
       " 'portray',\n",
       " 'post',\n",
       " 'program',\n",
       " 'real',\n",
       " 'reconstruct',\n",
       " 'seriou',\n",
       " 'shortcom',\n",
       " 'strength',\n",
       " 'struggl',\n",
       " 'suffer',\n",
       " 'thought',\n",
       " 'torment',\n",
       " 'trauma',\n",
       " 'traumat',\n",
       " 'truth',\n",
       " 'twofold',\n",
       " 'understand',\n",
       " 've',\n",
       " 'veri',\n",
       " 'vet',\n",
       " 'veteran',\n",
       " 'vividli',\n",
       " 'war',\n",
       " 'women',\n",
       " 'word',\n",
       " 'wound',\n",
       " 'young']"
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong = np.argwhere(y_pred!=y_test)\n",
    "check = X_test[wrong]\n",
    "print(check[0,0])\n",
    "print(check.shape)\n",
    "vec = nlp.LemmaCountVectorizer(strip_accents='unicode', stop_words='english', min_df=1, \n",
    "                                 stem=True, ngram_range=(1, 1), preprocessing=True, )\n",
    "vec.fit_transform(check[0])\n",
    "vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8953462918411281,\n",
       " 0.9040914014287277,\n",
       " 0.8958046812134007,\n",
       " 0.8875362305239204,\n",
       " 0.8262288670868729,\n",
       " 0.8958614175826981,\n",
       " 0.9164024781200871,\n",
       " 0.9084344418346936,\n",
       " 0.9136939635528595,\n",
       " 0.9095024019634056,\n",
       " 0.8529732208900852,\n",
       " 0.8967622744752337,\n",
       " 0.8719168762788287,\n",
       " 0.9228923536177926,\n",
       " 0.920845066179548,\n",
       " 0.9171623398613418,\n",
       " 0.9036950733860422,\n",
       " 0.9161478412195226,\n",
       " 0.9208116514084702]"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [rs_regression_tfidf,\n",
    "rs_regression_bow,\n",
    "rs_regression_bin,\n",
    "rs_mnv_bow,\n",
    "# gs_bnv, #error\n",
    "rs_svm,\n",
    "rs_linearsvm,\n",
    "gs_nbsvm,\n",
    "rs_lr_max_per, #*\n",
    "rs_best_lr,\n",
    "rs_best_lr3, #*\n",
    "rs_lr_scaler,#*\n",
    "rs_lr_notf,\n",
    "rs_mnb_fix,#*\n",
    "gs_nbsvm_best_count,\n",
    "gs_nbsvm_count_40,\n",
    "gs_nbsvm_tf_vocab,\n",
    "gs_lr1_vocab,\n",
    "gs_nbsvm_count_tfidf_80,\n",
    "gs_nbsvm_count_80]\n",
    "\n",
    "scores = [m.best_score_ for m in models]\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9040914 , 0.91640248, 0.90843444, 0.91369396, 0.9095024 ,\n",
       "       0.92289235, 0.92084507, 0.91716234, 0.90369507, 0.91614784,\n",
       "       0.92081165])"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = np.array(scores)\n",
    "idx = np.where( s> 0.9)[0]\n",
    "s[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_f1_final = np.array([s[1] for s in f1_scoress])\n",
    "idx_sort = np.argsort(-scores_f1_final)\n",
    "scores_f1_final[idx_sort]\n",
    "estimators = np.array(models)[idx]\n",
    "e = estimators[idx_sort][0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('1', Pipeline(memory=None,\n",
       "     steps=[('vec', LemmaCountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=0.9, max_features=None, min_df=2,\n",
       "           ngram_range...alty='l2', random_state=None, solver='saga',\n",
       "          tol=0.0001, verbose=0, warm_start=False))]))],\n",
       "         flatten_transform=None, n_jobs=-1, voting='soft',\n",
       "         weights=[1, 0.95])"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "v = VotingClassifier([(str(i),e_.best_estimator_) for i,e_ in enumerate(e)][1:], voting='soft', weights=[1,.95],\n",
    "                    n_jobs=-1)\n",
    "v.fit(X_train, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=LogisticRegression(C=35, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=40000, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='saga',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None)"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_pip = Pipeline([('vec',\n",
    "  LemmaCountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
    "             encoding='utf-8', input='content',\n",
    "             lowercase=True, max_df=0.9, max_features=None, min_df=2,\n",
    "             ngram_range=(1, 2), preprocessing=True, preprocessor=None,\n",
    "             stem=False, stop_words=None, strip_accents='unicode',\n",
    "             token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None,\n",
    "             vocabulary=None)),\n",
    " ('tfidf',\n",
    "  TfidfTransformer(norm='l2', smooth_idf=1, sublinear_tf=1, use_idf=1))\n",
    "                   ])\n",
    "\n",
    "dddd = ada_pip.fit_transform(X_train)\n",
    "\n",
    "log = LogisticRegression(C=35, class_weight=None, dual=False, fit_intercept=True,\n",
    "            intercept_scaling=1, max_iter=40000, multi_class='warn',\n",
    "            n_jobs=None, penalty='l2', random_state=None, solver='saga',\n",
    "            tol=0.0001, verbose=0, warm_start=False)\n",
    "ada = AdaBoostClassifier(base_estimator=log)\n",
    "ada.fit(dddd, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model in leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cuent/anaconda/envs/comp551-proj2/lib/python3.6/site-packages/sklearn/model_selection/_search.py:271: UserWarning: The total space of parameters 1 is smaller than n_iter=20. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(197.97414898872375, {}, 0.9041663438626208)"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "token = nltk.word_tokenize\n",
    "\n",
    "pipeline_fine_tunning = Pipeline([('vect',\n",
    "  LemmaCountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
    "            encoding='utf-8', input='content',\n",
    "             lowercase=True, max_df=6000, max_features=None, min_df=2,\n",
    "             ngram_range=(1, 3), preprocessing=True, preprocessor=None,\n",
    "             strip_accents='unicode',\n",
    "             token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=token,\n",
    "             vocabulary=None, stem=False)),\n",
    " ('clf',\n",
    "#   NBSVM(beta=0.31925992753471094, alpha=0.6585988241122311, C=0.40531603281740625, fit_intercept=False))])\n",
    "  NBSVM(beta=0.31925992753471094, alpha=1, C=0.40531603281740625, fit_intercept=False))])\n",
    "rs_est_fine_tun = RandomizedSearchCV(pipeline_fine_tunning, params_best, \n",
    "                                   cv=2, scoring=score, n_jobs=-1, verbose=3,\n",
    "                                 return_train_score=True, n_iter=20, random_state=62)\n",
    "start = time.time()\n",
    "rs_est_fine_tun.fit(X_train, y)\n",
    "time.time()-start, rs_est_fine_tun.best_params_, rs_est_fine_tun.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', LemmaCountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=6000, max_features=None, min_df=2,\n",
       "           ngram_range=(1, 3), preprocessing=True, preprocessor=N...), ('clf', NBSVM(C=0.40531603281740625, alpha=1, beta=0.31925992753471094,\n",
       "   fit_intercept=False))])"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train.iloc[:,1].values\n",
    "X_test = test.iloc[:,1].values\n",
    "y = train.iloc[:,2].values.astype(int)\n",
    "model = rs_est_fine_tun.best_estimator_\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "R2qgOsTYBJbD"
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ve3cstNGDKIv"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "test['Category'] = y_pred\n",
    "\n",
    "# submission results\n",
    "submission = test.drop(columns='Text')\n",
    "submission.to_csv('../data/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "CtDlCzOhQ4Xo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "comp551_proj2.ipynb",
   "provenance": [
    {
     "file_id": "1PiolizwBetCCjzPZQudwPMiUv9EnnM0U",
     "timestamp": 1549490375052
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
